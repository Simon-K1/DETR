{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.tensor(11)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=torch.ones(16,768,16,16)\n",
    "weight*torch.tensor(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "Wq=torch.randint(0,255,(64,32,3,3))\n",
    "# with open ('TxT//ConvWeight.txt','a') as ff:\n",
    "#     #Wq的维度：[OC,IC,K,K]\n",
    "#     for OC in range(Wq.shape[0]):#遍历全部输出通道\n",
    "#         for K_Row in range(Wq.shape[2]):#遍历行\n",
    "#             for K_Col in range(Wq.shape[3]):#遍历列\n",
    "#                 for IC in range(0,Wq.shape[1],8):#遍历通道\n",
    "#                     ff.write('%02x'%(int(Wq[OC,IC:IC+7,K_Row,K_Col].item())&0xff))\n",
    "#                     ff.write(\"\\n\")\n",
    "#     ff.close()\n",
    "for OC in range(Wq.shape[0]):#遍历全部输出通道\n",
    "    for K_Row in range(Wq.shape[2]):#遍历行\n",
    "        for K_Col in range(Wq.shape[3]):#遍历列\n",
    "            for IC in range(0,Wq.shape[1],8):#遍历通道\n",
    "                # print(list(Wq[OC,IC:IC+2,K_Row,K_Col]))\n",
    "                Hex0=Wq[OC,IC,K_Row,K_Col].item()&0xff\n",
    "                Hex1=Wq[OC,IC+1,K_Row,K_Col].item()&0xff\n",
    "                Hex2=Wq[OC,IC+2,K_Row,K_Col].item()&0xff\n",
    "                Hex3=Wq[OC,IC+3,K_Row,K_Col].item()&0xff\n",
    "                Hex4=Wq[OC,IC+4,K_Row,K_Col].item()&0xff\n",
    "                Hex5=Wq[OC,IC+5,K_Row,K_Col].item()&0xff\n",
    "                Hex6=Wq[OC,IC+6,K_Row,K_Col].item()&0xff\n",
    "                Hex7=Wq[OC,IC+7,K_Row,K_Col].item()&0xff\n",
    "                print('%02x%02x%02x%02x%02x%02x%02x%02x'%(Hex7,Hex6,Hex5,Hex4,Hex3,Hex2,Hex1,Hex0))\n",
    "                # ff.write(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = r'E:\\Transformer\\Transformer_Arithmatic\\Transformer_Main\\TxT\\Weight.txt'\n",
    "\n",
    "# 判断文件是否存在不存在则创建一个\n",
    "if not os.path.isfile(filename):\n",
    "     fd = open(filename, mode=\"w\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "picture_in=torch.rand(1,3,224,224)\n",
    "\n",
    "nn.conv(picture_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,32):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "Wq=torch.randint(0,10,(64,32,3,3),dtype=torch.float)\n",
    "Xq=torch.randint(0,10,(1,32,322,322),dtype=torch.float)\n",
    "# Xq=torch.rand(1,32,322,322)\n",
    "ConvLayer=nn.Conv2d(32,64,3,bias=False,stride=2,padding=0)\n",
    "ConvLayer.weight.data=Wq\n",
    "Output=ConvLayer(Xq)\n",
    "print(Output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  6,  4,  5,  6,  9,  5, 11,  9,  4], dtype=uint32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "def gen_B(S1, S2, S3):  # 第一组  β即N_REAL  +++++求shitf++++\n",
    "    M = (S1 * S2) / S3\n",
    "    M = M.numpy()\n",
    "\n",
    "    daxiao = S2.shape[0]  # 第一层权重的shape[0]是32 shape[0]表示行数 是一维大小位32的列向量\n",
    "    SCALE = np.zeros(daxiao, dtype=np.uint32, order='C')  # 相当于32个输出通道 每个对应一组shift\n",
    "    N_REAL = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    for i, ii in enumerate(M):  # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "\n",
    "        while not (ii >= 0.5 and ii <= 1.0):  # 左移到（0.5，1） 左移一次相当于*2\n",
    "            ii *= 2\n",
    "        pass\n",
    "        mmmm = ii * (2 ** 32)  # 乘2^32\n",
    "\n",
    "        SCALE[i] = mmmm.astype(np.int32)#更新Scale放大后的值\n",
    "\n",
    "    for i, ii in enumerate(M):#存在一个问题，如果shift为0怎么办？\n",
    "        N_REAL[i] = round(math.log(SCALE[i] / ii, 2)) - 32 - 1  # fpga加了1这里要减1,  β值也是m维 相当于存的mmmm\n",
    "\n",
    "    return N_REAL\n",
    "S1=torch.rand(1)\n",
    "S2=torch.rand(10)\n",
    "S3=torch.rand(1)\n",
    "gen_B(S1,S2,S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_M_N(S1, S2, S3):  # 第二组求M'即M  ++++++返回scale和shift+++++\n",
    "    daxiao = S2.shape[0]\n",
    "    M = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    N_REAL = gen_B(S1, S2, S3)\n",
    "    M = np.zeros(S2.shape[0])\n",
    "    for i, ii in enumerate(M):\n",
    "        M[i] = (torch.round((S1 * S2[i]) / S3 * (2 ** (32 + N_REAL[i] + 1)))).numpy()  # s1s2/s3 *2^(32+β+1)\n",
    "    M = M.astype(np.uint32)\n",
    "    # exit()\n",
    "    return M, N_REAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 8, 3, 6, 9, 3, 2, 8, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "M=torch.randint(1,10,[1,10])\n",
    "for i,ii in enumerate(M):\n",
    "    print(ii)\n",
    "# for i, ii in enumerate(M):  # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "#     while not (ii >= 0.5 and ii <= 1.0):  # 左移到（0.5，1） 左移一次相当于*2\n",
    "#         ii *= 2\n",
    "#     pass\n",
    "#     mmmm = ii * (2 ** 32)  # 乘2^32\n",
    "#     SCALE[i] = mmmm.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.round(torch.tensor(2.5)))\n",
    "torch.round(torch.tensor(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷积操作在GPU上的执行时间为:4.78804248046875 毫秒\n"
     ]
    }
   ],
   "source": [
    "#统计计算时间\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "torch.cuda.is_available()\n",
    "\n",
    "InChannel=8\n",
    "OutChannel=768\n",
    "FeatureSize=224\n",
    "Device='cuda'\n",
    "times=1000\n",
    "net=nn.Conv2d(InChannel,OutChannel,16,bias=False,stride=16,padding=0)\n",
    "input=torch.rand([1,InChannel,FeatureSize,FeatureSize])\n",
    "net = net.to(Device)  # 网络模型\n",
    "input = input.to(Device)  # 输入\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "# 记录开始事件\n",
    "start_event.record()\n",
    "# 执行卷积操作\n",
    "\n",
    "for i in range(times):\n",
    "    output = net(input)\n",
    "# 记录结束事件并同步\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# 计算执行时间（单位：毫秒）\n",
    "elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "print(f\"卷积操作在GPU上的执行时间为:{elapsed_time/times} 毫秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7195680141449\n"
     ]
    }
   ],
   "source": [
    "#接着上面\n",
    "Device='cpu'\n",
    "times=10000\n",
    "net=nn.Conv2d(InChannel,OutChannel,16,bias=False,stride=16,padding=0)\n",
    "input=torch.rand([1,InChannel,FeatureSize,FeatureSize])\n",
    "net = net.to(Device)  # 网络模型\n",
    "input = input.to(Device)  # 输入\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    output = net(input)\n",
    "time_end = time.time()\n",
    "time_sum = time_end - time_start\n",
    "print(time_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-21.5111)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-21.5111)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=torch.rand(197,768)\n",
    "x2=torch.rand(197,768)\n",
    "error=x1-x2\n",
    "print(error.sum())\n",
    "(x1-x2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "余弦距离:%d%d -0.0008845366537570953 -0.0008845366537570953\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 创建两个输入张量\n",
    "tensor1 = torch.randn(1, 197, 786)\n",
    "tensor2 = torch.randn(1, 197, 786)\n",
    "\n",
    "# 将张量移动到GPU上（如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensor1 = tensor1.to(device)\n",
    "tensor2 = tensor2.to(device)\n",
    "\n",
    "# 将张量展平为二维矩阵\n",
    "tensor1_flat = tensor1.view(1, -1)  # shape: [1, 155142]\n",
    "tensor2_flat = tensor2.view(1, -1)  # shape: [1, 155142]\n",
    "\n",
    "# 计算余弦相似度\n",
    "similarity = F.cosine_similarity(tensor1_flat, tensor2_flat)\n",
    "\n",
    "print(\"余弦距离:\", similarity.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
