{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14., 12., 12.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=[13.5,12.5,11.5]\n",
    "a=torch.tensor(a)\n",
    "a.round()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -2., -0.,  0.,  2.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([-2.5000 , -1.5000,   -0.5000,    0.5000 ,   1.5000  ,  2.5000]).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.tensor([[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]])\n",
    "Y=torch.tensor([1])\n",
    "\n",
    "X*Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x =torch.tensor([[[1,2,3,4]]])\n",
    "x.shape\n",
    "x.expand(1,4,4)\n",
    "for i in x[0,0,:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out_scale\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(\u001b[39m0.234\u001b[39m)\n\u001b[0;32m      2\u001b[0m in_scale\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m384\u001b[39m)\n\u001b[0;32m      3\u001b[0m channel_nums\u001b[39m=\u001b[39m\u001b[39m384\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "out_scale=torch.tensor(0.234)\n",
    "in_scale=torch.rand(1,1,384)\n",
    "channel_nums=384\n",
    "SCALE=torch.rand(1,1,384)\n",
    "x_q=torch.rand(16,197,384)\n",
    "x_q_sum=x_q.sum(dim=-1)\n",
    "std_x_q =  torch.sqrt(#std标准差也就是加入alpha后每一行的标准差，\n",
    "                channel_nums * (x_q**2).sum(dim=-1) - x_q.sum(dim=-1)**2)\n",
    "print(x_q_sum.shape)\n",
    "for i in range(x_q.shape[2]):#x_q:[1,197,384]\n",
    "    x_q[:,:,i]=(x_q[:,:,i]*channel_nums-x_q_sum)/std_x_q\n",
    "    \n",
    "for i in range(x_q.shape[1]):\n",
    "    x_q[:,i,:]=x_q[:,i,:]*in_scale\n",
    "\n",
    "beta=torch.rand(384)\n",
    "\n",
    "\n",
    "\n",
    "X_q=torch.rand(1,197,384)\n",
    "print(X_q.shape)\n",
    "X_Mean=X_q.sum(dim=-1).unsqueeze(-1)*torch.ones(384)\n",
    "print(X_Mean.shape,X_Mean)\n",
    "\n",
    "\n",
    "Bias=torch.rand(384)\n",
    "Bias=Bias.reshape(1,-1,1)\n",
    "print(Bias.shape)\n",
    "Bias=Bias*torch.ones(197)\n",
    "print(Bias.transpose(-1,-2).shape,Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8284, 0.8615, 0.9001,  ..., 0.4303, 0.8895, 0.7683],\n",
      "        [0.1498, 0.9806, 0.2665,  ..., 0.1643, 0.7077, 0.8185],\n",
      "        [0.8216, 0.4132, 0.8983,  ..., 0.3648, 0.8573, 0.3017],\n",
      "        ...,\n",
      "        [0.5421, 0.3475, 0.2392,  ..., 0.4069, 0.5456, 0.9214],\n",
      "        [0.9650, 0.2893, 0.0048,  ..., 0.8635, 0.5091, 0.3144],\n",
      "        [0.7948, 0.3334, 0.4086,  ..., 0.1863, 0.5976, 0.7749]])\n",
      "torch.Size([16, 197])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 1, 1])\n",
      "(1,)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch=16\n",
    "x_q=torch.rand(batch,197,384)#构造输入\n",
    "in_scale=torch.rand(1,1,384)\n",
    "out_scale=torch.rand(1)\n",
    "channel_nums=x_q.shape[-1]#获取通道数\n",
    "print(x_q[0,:,:])\n",
    "\n",
    "x_q=(x_q/in_scale).round()\n",
    "M1=x_q.sum(dim=-1)\n",
    "print(M1.shape)\n",
    "print(M1[:,1].shape)\n",
    "print(M1[:,1].reshape(16,-1,1).shape)\n",
    "#然后让每一行的点减去对应行的sum\n",
    "std_x_q = torch.sqrt(#std标准差也就是加入alpha后每一行的标准差，\n",
    "                channel_nums * (x_q**2).sum(dim=-1) - x_q.sum(dim=-1)**2)#[1,197]\n",
    "for i in range(M1.shape[-1]):#对于每一行来说，都是[16,384]维，对于M1的每一行来说，都是[16]维\n",
    "    x_q[:,i,:]=(x_q[:,i,:]-M1[:,i].reshape(batch,-1))/std_x_q[:,i].reshape(batch,-1)\n",
    "\n",
    "\n",
    "Gama=torch.rand(1,384)\n",
    "Beta=torch.rand(1,384)\n",
    "for i in range(x_q.shape[1]):#对于每一行来说\n",
    "    x_q[:,i,:]=(x_q[:,i,:]*Gama+Beta).round()\n",
    "# x=torch.tensor([[[1,2,3],[4,5,6]]])\n",
    "# M1=x.sum(dim=-1)\n",
    "# print(x)\n",
    "# print(M1)\n",
    "# print(x[:,1,:]-M1[:,1])\n",
    "a=torch.pow(2,torch.tensor(32))\n",
    "topk=(1,)\n",
    "print(topk)\n",
    "\n",
    "\n",
    "print(Gama.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#研究acuracy是如何计算的\n",
    "output=torch.rand(16,1000)#16个batch，1000个类的概率\n",
    "topk=(1,5)\n",
    "maxk=max(topk)\n",
    "_, pred = output.topk(maxk, 1, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "out_scale=torch.tensor(0.0234)\n",
    "Gama=((torch.rand(1,1,384)/out_scale*torch.pow(2, torch.tensor(32))).round()/torch.pow(2, torch.tensor(32))).round()\n",
    "Gama\n",
    "Beta=((torch.rand(1,1,384)/out_scale*torch.pow(2, torch.tensor(32))).round()/torch.pow(2, torch.tensor(32))).round()\n",
    "with open ('Scale_Bias.txt','a') as ff:\n",
    "    for i in range(Gama.shape[-1]):\n",
    "        ff.write('%02x%02x'%(int(Gama[0,0,i].item())&0xff,int(Beta[0,0,i].item())&0xff))\n",
    "        ff.write(\"\\n\")\n",
    "ff.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(\u001b[39m100.1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m      2\u001b[0m \u001b[39mint\u001b[39m(a)\n\u001b[1;32m----> 3\u001b[0m a\u001b[39m.\u001b[39;49mtype()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(100.1).item()\n",
    "int(a)\n",
    "a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a-c\n"
     ]
    }
   ],
   "source": [
    "num=10\n",
    "b=-12\n",
    "print('%02x%02x'%(num,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec:-4 hex:FC\n",
      "dec:-3 hex:FD\n",
      "dec:-2 hex:FE\n",
      "dec:-1 hex:FF\n",
      "dec:0 hex:00\n",
      "dec:1 hex:01\n",
      "dec:2 hex:02\n",
      "dec:3 hex:03\n",
      "dec:4 hex:04\n"
     ]
    }
   ],
   "source": [
    "for x in range(-4,5):\n",
    "    print (\"dec:%d hex:%02X\" % (x, x & 0xff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.5648e+04, -7.5647e+04, -7.5646e+04,  ..., -7.5267e+04,\n",
      "          -7.5266e+04, -7.5265e+04],\n",
      "         [-7.5264e+04, -7.5263e+04, -7.5262e+04,  ..., -7.4883e+04,\n",
      "          -7.4882e+04, -7.4881e+04],\n",
      "         [-7.4880e+04, -7.4879e+04, -7.4878e+04,  ..., -7.4499e+04,\n",
      "          -7.4498e+04, -7.4497e+04],\n",
      "         ...,\n",
      "         [-1.1510e+03, -1.1500e+03, -1.1490e+03,  ..., -7.7001e+02,\n",
      "          -7.6901e+02, -7.6801e+02],\n",
      "         [-7.6701e+02, -7.6601e+02, -7.6501e+02,  ..., -3.8601e+02,\n",
      "          -3.8501e+02, -3.8401e+02],\n",
      "         [-3.8301e+02, -3.8201e+02, -3.8101e+02,  ..., -2.0000e+00,\n",
      "          -1.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_q=(torch.linspace(-197*384,0,197*384)).reshape(1,197,384)\n",
    "print(x_q)\n",
    "with open ('Xq_LayerNorm.txt','a') as ff:\n",
    "    for i in range(x_q.shape[-2]):\n",
    "        for j in range(x_q.shape[-1]):\n",
    "            ff.write('%02x'%(int(x_q[0,i,j].item())&0xff))\n",
    "            ff.write(\"\\n\")\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30., device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "fuck=torch.tensor([[[1,2,3],[4,5,6]]])\n",
    "fuck_sum=fuck.sum(dim=-1)\n",
    "print(fuck_sum)\n",
    "\n",
    "fuck=torch.tensor([  -2.,    0.,    2.,  -76.,  -46.,  -62.,   20.,  -64.,    2.,  -96.,\n",
    "           0.,    2.,    0.,  -62., -108.,   46.,  -32.,   12.,   48.,    2.,\n",
    "         -14.,   14.,    0.,   -2.,  -20.,  -20.,  -12.,  146.,   34.,   -2.,\n",
    "           0.,    0.,    4.,   46.,    0.,  -92.,    0.,   -2.,   12.,   28.,\n",
    "          -2.,   14.,    0.,  124.,   16.,   16.,  -30.,   30.,   12.,   50.,\n",
    "          12.,   -4.,   80.,   48.,    4.,    4.,    2.,   32.,   32.,   30.,\n",
    "         -20.,    0.,  -12.,   32.,    4.,  -50.,    4.,  -14.,  -12.,    2.,\n",
    "          52.,  -16.,  -76.,  -80.,  -36.,   76.,   96.,    0.,  -12.,    0.,\n",
    "          66.,    2.,   16.,    0.,  -66.,   14.,    0.,   32.,   14.,    2.,\n",
    "           4.,   12.,    0.,   16.,    0.,   -4.,   22.,   12.,   16.,   20.,\n",
    "           4.,    0.,   64.,  -48.,  -64.,   12.,   -2.,   14.,  -14.,    0.,\n",
    "          36.,  -48.,  -14.,   14.,  -12.,    0.,   12.,    0.,   92.,   20.,\n",
    "         -50., -126.,    2.,   48.,  -12.,  -24.,  -32.,   22.,  -12.,   34.,\n",
    "        -126.,   14.,  -12.,    0.,  -76.,    0.,   66.,    2.,  -20.,    2.,\n",
    "          -2.,   12.,   82.,  -16.,   -2.,   12.,    4.,    4.,   32.,  -30.,\n",
    "          32.,   16.,    0.,    2.,   -2.,  -14.,  -12.,   16.,  -20.,  124.,\n",
    "          -2.,    2.,   66.,   20.,  -12.,    0.,   62.,   80.,   44.,  -46.,\n",
    "          44.,   -4.,  -14.,  -30.,    2.,   32.,   20.,  -16.,   -2.,    0.,\n",
    "         -66.,   -4.,  -64.,  -12.,   20.,    0.,    4.,  -12.,  -82.,  -48.,\n",
    "         -14.,  -14.,   -4., -156.,  -48.,  -10.,   66.,  -76.,    4.,  -14.,\n",
    "          62.,   12.,    0.,  -64.,   -2.,  -12.,   -2.,    4.,  -34.,   -2.,\n",
    "          16.,   -2.,   14.,    2.,    0.,  -16.,   12.,   14.,    0.,   50.,\n",
    "          -4.,   16.,   32.,   -2.,  -14.,  -12.,  -92.,  -48.,    2.,  -52.,\n",
    "          -2.,   -4.,   80.,   16.,  -28.,  -30.,   -4.,   32.,   -2.,   -4.,\n",
    "          16.,  -16.,   -4.,  -32.,   46.,  -16.,    2.,    4.,  -50.,  100.,\n",
    "          20.,    2.,  -50.,  -98.,   -2.,  -12.,  238.,   12.,   -4.,    2.,\n",
    "          -2.,   44.,   46.,   30.,   20.,   14.,  -20.,   12.,   -2.,  -62.,\n",
    "          -4.,    0.,    0.,  -32.,  -30.,   12.,    2.,    0.,   78.,    4.,\n",
    "           2.,   20.,    0.,    4.,    0.,   -4.,  -30.,    0.,   -2.,  -64.,\n",
    "          30.,    0.,   14.,   -4.,    4.,  -12.,  -16.,   34.,  -12.,   -2.,\n",
    "          58.,   46.,    4.,    0.,  -14.,    0.,   12.,   78.,   80.,  -20.,\n",
    "          34.,  -12.,    0.,   30.,    2.,   -2.,    0.,    0.,   28.,    4.,\n",
    "          32.,    0.,   -2.,   78.,    6.,   48.,   -4.,  -14.,    0.,   34.,\n",
    "         -34., -784.,  -12.,   -2.,   16.,   68.,    2.,   66.,    4.,  -30.,\n",
    "          12.,    2.,   62.,   12.,   30.,   28.,  -14.,    4.,   -4.,  -14.,\n",
    "          96.,    4.,   32.,  -62.,   32.,  -80.,  -14.,  -64.,    2.,    2.,\n",
    "           0.,   16.,    4.,  -12.,    0.,  -66.,   24.,  -20.,  -14.,  -24.,\n",
    "           2.,    0.,   20.,    0.,   36.,   12.,    4.,   -4.,    2.,   -4.,\n",
    "           0.,    2.,   -4.,   16.], device='cuda:0')\n",
    "fuck.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x1.ec7df40000000p+6\n",
      "0x1.fc00000000000p+6\n"
     ]
    }
   ],
   "source": [
    "#pytorch按位操作尝试\n",
    "import torch\n",
    "a=torch.tensor(123.123)\n",
    "a.dtype\n",
    "print(float.hex(a.item()))\n",
    "print(float.hex(127.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45afb9dd\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5623.23291015625,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import struct\n",
    "s=5623.23291015625\n",
    "print(struct.pack('>f' ,float(s)).hex())  #`45afb9dd`\n",
    "#字符串处理\n",
    "a=struct.pack('>f' ,float(s)).hex()\n",
    "print(a[0])\n",
    "#十六进制转回浮点\n",
    "struct.unpack('!f',bytes.fromhex(a))\n",
    "#https://www.toutiao.com/article/6951557507735175683/?channel=&source=search_tab&wid=1670397209573\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0492e-01, -6.2791e-01, -4.2683e-01, -8.4953e-01, -3.9024e-01,\n",
      "         -3.6699e-01, -9.3318e-01, -3.0160e-01, -4.3609e-01, -4.6078e-01,\n",
      "         -5.2103e-02, -9.0410e-01, -5.1404e-02, -2.2124e-01, -5.4400e-01,\n",
      "         -3.8249e-01, -9.4395e-01, -4.0693e-01, -2.5390e-01, -1.1809e-01,\n",
      "         -1.9701e-01, -8.2507e-01, -2.8496e-01, -5.3652e-01, -6.9537e-01,\n",
      "         -9.9382e-01, -8.3490e-02, -5.5523e-01, -5.8305e-02, -1.6997e-01,\n",
      "         -5.0587e-01, -1.9212e-01, -4.8381e-01, -6.2029e-01, -2.1932e-01,\n",
      "         -5.3865e-01, -1.6629e-01, -6.6195e-01, -9.8675e-02, -2.1505e-01,\n",
      "         -6.4151e-02, -9.6111e-01, -9.6006e-01, -8.5085e-01, -1.4400e-01,\n",
      "         -2.6372e-01, -7.8700e-01, -5.9504e-01, -3.4097e-01, -3.8721e-02,\n",
      "         -5.3464e-01, -2.7193e-01, -9.9421e-01, -6.3418e-02, -8.9056e-01,\n",
      "         -8.3086e-01, -5.6086e-01, -1.9388e-01, -3.3273e-01, -1.7529e-01,\n",
      "         -4.6956e-01, -8.1601e-01, -2.7330e-01, -4.8496e-01, -8.4140e-02,\n",
      "         -2.3053e-01, -8.0498e-01, -2.4364e-01, -8.2936e-01, -8.7514e-01,\n",
      "         -8.8252e-01, -1.9427e-01, -7.5245e-01, -4.2622e-02, -1.6059e-01,\n",
      "         -1.4057e-01, -5.1612e-01, -8.9444e-01, -9.6135e-01, -3.4676e-02,\n",
      "         -4.4431e-01, -4.9660e-01, -6.8123e-01, -1.3034e-01, -9.6809e-01,\n",
      "         -1.1867e-04, -9.4763e-01, -6.4380e-01, -4.9094e-01, -1.6003e-02,\n",
      "         -3.0612e-01, -7.7434e-01, -3.8006e-01, -3.2475e-01, -4.9847e-01,\n",
      "         -3.8795e-01, -1.4841e-01, -5.9754e-02, -1.7128e-02, -5.5046e-01,\n",
      "         -4.3784e-01, -4.5625e-01, -6.7973e-01, -2.4595e-01, -2.1863e-01,\n",
      "         -6.9887e-01, -7.3520e-01, -8.9268e-02, -1.1937e-01, -9.3232e-01,\n",
      "         -1.3289e-01, -5.4832e-01, -1.3346e-01, -8.1107e-01, -2.0905e-01,\n",
      "         -7.7510e-01, -5.5296e-02, -7.8318e-01, -2.3340e-02, -8.1656e-01,\n",
      "         -2.7031e-01, -7.6009e-01, -8.0593e-01, -1.4091e-01, -1.7016e-01,\n",
      "         -3.7214e-01, -6.8796e-01, -7.4812e-01, -3.7316e-01, -1.3417e-01,\n",
      "         -4.7607e-01, -8.7455e-01, -6.5108e-01, -7.5219e-01, -5.8195e-01,\n",
      "         -4.4716e-02, -7.6014e-01, -2.1793e-01, -1.4313e-01, -9.0463e-01,\n",
      "         -6.7017e-01, -1.9646e-01, -5.0188e-01, -5.4883e-01, -8.6399e-01,\n",
      "         -7.8158e-01, -6.2031e-02, -9.2803e-02, -8.7345e-01, -2.5952e-02,\n",
      "         -7.1637e-01, -1.4012e-01, -4.7209e-01, -1.9819e-02, -8.0357e-01,\n",
      "         -7.8483e-01, -8.2403e-01, -7.6895e-01, -8.7682e-02, -3.3806e-01,\n",
      "         -6.3035e-02, -6.8637e-01, -3.6960e-01, -9.1642e-01, -4.7623e-01,\n",
      "         -5.5403e-01, -2.8308e-01, -6.4377e-01, -4.6720e-01, -7.6143e-01,\n",
      "         -4.3539e-01, -8.7809e-01, -1.4978e-01, -4.4043e-01, -1.3687e-01,\n",
      "         -1.4331e-01, -7.3126e-01, -7.4257e-01, -7.8308e-01, -3.5117e-01,\n",
      "         -4.2293e-01, -4.8215e-01, -7.1280e-01, -2.0478e-01, -8.5154e-01,\n",
      "         -2.9006e-01, -6.9382e-01, -9.9772e-01, -6.3179e-01, -6.1283e-01,\n",
      "         -5.5376e-01, -7.0966e-01, -8.3604e-01, -7.6453e-02, -5.6846e-01,\n",
      "         -4.8297e-01, -6.1056e-01, -1.5549e-01, -6.4740e-01, -7.9587e-01,\n",
      "         -3.8961e-01, -2.8704e-02, -1.3438e-01, -5.2629e-01, -7.7663e-01,\n",
      "         -7.8500e-01, -7.6845e-01, -6.1119e-01, -4.8284e-02, -9.1955e-01,\n",
      "         -2.7739e-01, -7.9857e-02, -8.4965e-01, -6.3072e-01, -6.9012e-02,\n",
      "         -4.1254e-01, -4.2509e-01, -8.4164e-01, -1.1097e-02, -1.8508e-01,\n",
      "         -6.9475e-01, -6.6311e-02, -1.2705e-01, -2.5058e-01, -6.0014e-01,\n",
      "         -1.7296e-01, -9.6386e-01, -5.4731e-01, -7.6536e-01, -1.1002e-01,\n",
      "         -1.9397e-01, -4.0457e-01, -8.1820e-01, -1.7549e-01, -3.7954e-01,\n",
      "         -2.2255e-01, -2.8073e-01, -3.8136e-01, -6.1705e-01, -4.7472e-01,\n",
      "         -6.8191e-01, -2.0016e-01, -6.6363e-02, -4.3842e-01, -2.1581e-01,\n",
      "         -8.8741e-01, -6.3471e-01, -5.1652e-01, -6.9663e-01, -7.7860e-01,\n",
      "         -8.0866e-01, -4.2791e-02, -4.9741e-01, -7.8911e-01, -1.0299e-01,\n",
      "         -7.6820e-02, -7.7858e-01, -4.7044e-01, -2.0105e-01, -1.8873e-01,\n",
      "         -8.1780e-01, -4.1254e-01, -4.8223e-01, -3.4518e-02, -1.8195e-01,\n",
      "         -3.9132e-01, -2.3002e-01, -9.6003e-01, -3.1591e-01, -7.8610e-01,\n",
      "         -9.8285e-01, -1.1418e-01, -8.2648e-02, -3.5035e-01, -8.9685e-01,\n",
      "         -1.8661e-01, -3.5089e-01, -8.3327e-01, -2.5039e-01, -2.5259e-01,\n",
      "         -3.4071e-01, -7.5095e-01, -8.4513e-01, -6.7266e-01, -2.5006e-01,\n",
      "         -2.7326e-01, -4.0640e-01, -1.4888e-01, -8.1998e-01, -7.8670e-01,\n",
      "         -2.3809e-01, -6.4025e-02, -9.5480e-02, -9.2236e-02, -6.9245e-01,\n",
      "         -5.0872e-01, -9.9821e-01, -7.1123e-01, -2.7855e-01, -8.4255e-01,\n",
      "         -8.3589e-01, -7.3238e-01, -4.3587e-02, -6.4410e-01, -9.4009e-01,\n",
      "         -8.7311e-01, -6.9587e-01, -6.8423e-01, -8.6370e-01, -5.7472e-01,\n",
      "         -5.8447e-02, -1.4344e-01, -9.6431e-01, -2.5406e-01, -3.3992e-02,\n",
      "         -3.5839e-01, -5.7720e-01, -4.3401e-01, -4.7142e-01, -5.1929e-01,\n",
      "         -2.0383e-01, -9.6674e-01, -5.1330e-01, -2.8216e-01, -7.7011e-01,\n",
      "         -1.5737e-01, -7.9943e-01, -8.8714e-01, -2.2732e-01, -8.5184e-01,\n",
      "         -1.9236e-01, -9.6604e-01, -8.0007e-01, -6.3221e-02, -4.2177e-02,\n",
      "         -4.8410e-01, -2.2460e-01, -6.1330e-01, -7.3216e-01, -9.8193e-01,\n",
      "         -4.4763e-01, -9.4095e-01, -2.3443e-01, -3.1661e-01, -8.8954e-01,\n",
      "         -4.7872e-01, -1.7326e-01, -6.3379e-01, -9.7848e-01, -3.4965e-01,\n",
      "         -1.6509e-01, -2.7036e-01, -6.7770e-01, -8.0855e-02, -4.2107e-01,\n",
      "         -5.2536e-01, -1.2841e-01, -8.4438e-01, -7.2758e-01, -5.6323e-01,\n",
      "         -7.3332e-01, -4.8343e-01, -9.7753e-01, -4.7425e-01, -5.5238e-01,\n",
      "         -6.9057e-01, -7.5802e-01, -7.3780e-01, -7.5439e-02, -1.8968e-02,\n",
      "         -9.2086e-01, -3.2339e-01, -3.7281e-01, -6.8428e-01, -3.4137e-01,\n",
      "         -9.2476e-01, -1.0446e-01, -5.9858e-01, -2.8625e-01, -8.5622e-02,\n",
      "         -7.6974e-01, -2.5114e-01, -4.8435e-01, -9.7527e-01]])\n",
      "torch.float32\n",
      "<class 'list'> 1\n",
      "['1010001110101101010010', '0100000101111101101110', '1011010100010010111011', '1011001011110101100100', '1000111110011100000100', '0111011111001011001000', '1101110111001001001110', '0011010011010101011011', '1011111010001111010000', '1101011111010110110100', '1010101011010100010000', '1100111011100110001001', '1010010100011001111000', '1100010100010111010110', '0001011010000111101101', '1000011110101011010111', '1110001101001101000010', '1010000010110010110000', '0000001111111101100111', '1110001110110000011100', '1001001101111011111110', '1010011001101111110111', '0010001111001011100001', '0001001010110010100100', '0110010000000111100001', '1111110011010110010011', '0101010111111001011100', '0001110001000111000001', '1101110110100011001000', '0101110000010111110110', '0000001100000001100110', '1000100101110110110110', '1110111101101100111111', '0011110110010111001011', '1100000100101100011000', '0001001111001001101001', '0101010010001110011010', '0101001011101011101100', '1001010000101100010100', '1011100001101101011000', '0000011011000010110000', '1110110000010111001011', '1110101110001101100100', '1011001110100011000111', '0010011011101011001100', '0000111000001101111001', '1001001011110001100111', '0011000010101001000100', '0101110100100111010111', '0011110100110100010000', '0001000110111100100100', '0001011001110101111111', '1111110100001000100110', '0000001111000010010100', '1100011111110111110100', '1010100101100110000101', '0001111100101001101011', '1000110100010001101110', '0101010010111000000111', '0110011011111111111100', '1110000011010100011010', '1010000111001100010111', '0001011111011011010001', '1111000010011010010111', '0101100010100010101100', '1101100000100001110110', '1001110000100110011011', '1111001011111011110110', '1010100010100001101010', '1100000000010001110111', '1100001111011001110001', '1000110111011100101010', '1000000101000001000101', '0101110100101000100000', '0100100011100101101000', '0001111111100001101000', '0000100001000001100001', '1100100111110011111101', '1110110000110110010110', '0001110000010000001000', '1100011011110111111100', '1111110010000101000100', '0101110011001010001101', '0000101011110000101000', '1110111110101001111010', '1111000111000000000000', '1110010100101111111110', '0100100110011111111111', '1111011010110111110000', '0000011000110011100000', '0011100101111000001100', '1000110001110101101101', '1000010100101111011111', '0100110010001011011101', '1111111001101111011001', '1000110101000010100110', '0010111111101111011110', '1110100110000010000000', '0001100010011111000000', '0001100111010101101110', '1100000001011010001000', '1101001100110011101010', '0101110000000110000100', '1111011110110011001010', '1011111111000010111000', '0110010111010010001000', '0111100001101011100110', '0110110110100100100000', '1110100011101110000000', '1101110101011000111110', '0001000000100110110100', '0001100010111101110001', '0001000101010001100100', '1001111101000100000111', '1010110000100100100010', '1000110011011010011110', '1100010011111011101000', '1001000011111100011000', '0111111001100110000000', '1010001000010100001000', '0001010011001011001011', '1000010100101010100011', '1001110010100011010111', '0010000010010101010110', '0101110001111100001100', '0111110100010001101011', '0110000000111100001001', '0111111100001001110001', '0111111000011101100001', '0001001011000111101000', '1110011101111110111111', '1011111111000100111100', '0100110101011010000000', '1000000100011111100110', '0010100111110100111100', '0110111001001111111000', '1000010100110000101001', '1011111001010011010100', '0010010100100000100010', '1100111100101011001100', '0101011100100000100111', '1001001001011011001110', '0000000011110110111110', '0001100100000000100001', '1011101001011101010011', '1001000000101011000101', '1111110000101000000000', '0111110000011111010100', '1011111100110100110011', '1010100100110100000000', '0110111011001000011011', '0001111011110111100110', '1110001101101011110010', '0100010010110101100000', '1001101101101100111001', '1001000111010101000111', '1010010111100111011010', '1000100110110100000010', '0110011100100101001000', '0101101000101100000011', '0000001000110001100000', '0101111101101011101111', '0111101001111001100101', '1101010100110100110000', '1110011110101001111110', '0001101110101010011100', '0010000111011111001011', '0100100110011011101010', '1101111001101010101001', '1000010111011010101010', '1011110111010110101101', '1100000110010100011011', '0011001010111111001110', '1100001011111111000011', '0001100001001110010100', '0010010101111111111100', '0111011001100111101110', '0111110000110001101011', '1001000011101111011011', '0110011110011010001110', '1011000100010011111011', '1110110110111001000111', '0110110011110100011110', '1010001101100011100110', '1011001111111100111100', '0010100100000100100101', '0110001100111100010101', '1111111011010101000000', '0100001101111010001011', '0011100111000101010100', '0001101110000101111110', '0110101101011000110010', '1010110000001101110000', '0011100100100110110000', '0010001100001101100100', '1110111010001111011000', '0011100010011011010111', '0011111001110100001010', '0100101101110111100011', '1001011101111011110101', '1000111011110101110000', '1101011001001001000000', '0001001100110011101000', '0000110101110101010011', '1000110110100001111110', '1001000111101011011111', '1000100101110010110010', '0011100011101101010010', '1000101110001011101000', '1101011011001111110011', '0001110000001100101001', '0100011100010111101000', '1011001100000100110111', '0100001011101110010101', '0001101010101100011000', '1010011001110000000100', '1011001101001010010110', '1010111011101011101100', '0110101110100001100000', '0111101100001101010010', '0110001110110110000011', '0000111110011011101100', '0000010000110000001100', '0000000010010110111000', '0011001101000110001001', '0110001000110110111010', '1110110101111110011101', '0001100000111001101001', '1000011111011100111110', '1100001010100111101000', '1000110101000001110000', '1001111001001000000110', '1010001011101011001010', '0110011101101001110110', '1000010010100110011010', '1100011111000110001000', '0001111101110111100010', '1000011010000100011111', '0011101111101110011110', '1110011000011011111111', '0101110100100011110100', '1001100111101101100000', '0000111111010010100100', '1100000011110001100110', '1011100111111100001010', '1100011001011010110100', '0100010011111000101111', '0000100001110100110111', '0110010010101100111100', '1000111010100101010011', '1001111000001000101010', '0101111010001100001000', '1111110101011000011011', '1001010000000110101101', '1010010111011000110000', '0011101010100111100100', '1000111010100010000010', '1110000110111010110001', '1001101110111110011000', '1000001010000101111100', '1010001010110110001110', '1010011001110000010111', '1110110111001100011010', '0001101011000101100000', '0111010010100000011010', '1001000010110101100001', '1101011100010100011100', '1110101110001001010100', '0100001101111110101101', '1001001001111011110001', '1111011100111000100111', '1101001110101100110000', '0101001010000110100000', '0110011011000010101100', '1100101100101111111100', '0111111000101011100110', '0110011101001110100011', '1010101010100010000100', '0000000001100111010101', '0000001010100110111010', '0101110011100011000111', '1000000001111100101111', '1011000010110101011011', '0101100001100111100000', '0000000000001111111001', '0001011111010000011111', '1010000000100110011011', '0011000011101001001110', '1010001111010100110101', '1001001011001010000100', '1110011110011100110110', '0000011000111111001100', '1000011100010101110000', '0111100111001100001100', '0110001010001001011101', '0000010001110110010110', '1111111100010101011101', '0110110000100110001011', '0001110100111100001111', '1010111101100011000110', '1010101111111001001000', '0111011011111010000111', '0110010100010000000000', '0100100111000111100010', '1110000101010011001001', '1011111100001000100001', '0110010001001000110010', '0101111001010011011101', '1011101000110110111111', '0010011001000010001101', '1101111011001100011000', '0010010111000001010010', '1110110110111001101110', '0000010000101001000001', '0001011001110101111000', '0110111011111110111101', '0010011110000110110001', '1011110001101110000110', '1110001010111011100001', '0000100111100000000111', '1010000101110010111010', '1110111011111001001010', '0000011011001110110011', '0010000011101110100000', '1000101001001011110110', '0100001001001100011100', '1001100101001110101110', '1100011000110111110011', '1101000110001111001000', '1011010000100011110010', '1000100111110011011000', '1110111010011100011111', '1001100110100011010011', '0000001011110100001000', '0101100110000100100000', '1110111110111000101100', '1100101111111100101010', '0011101000000001111111', '0111011011011101110010', '1111011010111111110001', '1100101001100000101010', '1110000111000100011000', '1110000000011011010110', '0100010000110101100001', '1100011101110001001101', '1110101000110100010010', '0110001011010011110010', '0100010010000000001001', '1111010011111011100110', '0110011000001011100010', '0101001000011010101110', '0001010011010111111111', '0101101011111011010010', '0100101100101110010000', '1010111100101101100010', '0000110011111011111001', '0000011011111101001110', '1011000001010010011001', '0111010010000101011111', '0010000001100000000101', '0111011101110101110001', '1110111100000111110001', '1111010001111110110000', '1110010110100010000110', '0001101011010001100010', '0110000110010010011100', '1000010000011011101000', '0111100111000000101111', '0011010011111111110000', '0011011011000110010000', '1101011101111011001011', '0100101100100111010110', '0111110111000010111010', '0101111001011001101001', '0101110110001111111000', '1101100101111010011011', '1010101111011101111000', '0011001001111001101100', '0010010100011101100110', '0101111010110100101100', '1000101000011011101100', '0000000100101010011111', '1110111111111010000110', '1111001101010110101010']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "fuck=torch.rand(1,384)*-1\n",
    "print(fuck)\n",
    "print(fuck.dtype)\n",
    "fuck_float=fuck.numpy().tolist()  \n",
    "print(type(fuck_float),len(fuck_float))\n",
    "f=[]\n",
    "for a in fuck_float[0]:\n",
    "    hex_str=struct.pack('>f' ,float(a)).hex()\n",
    "    bin_str1=bin(struct.unpack('>i',bytes.fromhex(hex_str))[0])#可能存在不足32位的情况，并且不能处理负数为补码，这种方法笨死了\n",
    "    bin_str2=\"{0:b}\".format(int(struct.unpack('>i',bytes.fromhex(hex_str))[0])&0xffffffff)#'%032b'%(struct.unpack('>i',bytes.fromhex(hex_str))[0]&0xffffffff)\n",
    "    bin_frac_part=bin_str2[8:-1]#'%032b'%(struct.unpack('>i',bytes.fromhex(hex_str))[0]&0xffffffff)\n",
    "    f.append(bin_frac_part)\n",
    "print(f)\n",
    "#0b111111000101101011010001111101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "flops: 4133742592.0\n",
      "params: 25557032.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    " \n",
    "from thop import profile\n",
    " \n",
    "from   torchvision.models import resnet50\n",
    "model = resnet50()\n",
    " \n",
    "input = torch.randn(1, 3, 224, 224)\n",
    " \n",
    "flops,   params = profile(model, inputs=(input, ))\n",
    "print('flops:', flops)\n",
    "print('params:', params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1a2\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "def print_binary(number_string):\n",
    "    bin = \"\"   \n",
    "    if \".\" in number_string:\n",
    "        number_string = float(number_string)\n",
    "        bin += bin(struct.unpack('!i',struct.pack('!f', number_string)))\n",
    "    else:\n",
    "        num = int(number_string)  \n",
    "        bin += \"{0:x}\".format(num)#格式化，0号参数填num，为二进制表示\n",
    "    print(bin)\n",
    "print_binary('57762')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +a\n",
      "0005\n",
      " 2.300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unsupported format character 'b' (0x62) at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [116], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%04d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%6.3f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39m2.3\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;31mValueError\u001b[0m: unsupported format character 'b' (0x62) at index 1"
     ]
    }
   ],
   "source": [
    "print(\"%+10x\" % 10)\n",
    "\n",
    "print(\"%04d\" % 5)\n",
    "\n",
    "print(\"%6.3f\" % 2.3)\n",
    "print(\"%b\"%10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "495274a28bdb076e4e5d468612a890f31d5e619c1dd6c8cb530d5f7b822d194c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
