{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch#源于master版本\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "sys.path.append(\"../utils/\")\n",
    "# sys.path.append('MyTransformer/utils')\n",
    "from Mymodels import Vit\n",
    "from utils import build_transform\n",
    "\n",
    "from utils import validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_Channels=3\n",
    "Embed_Dim=384\n",
    "Picture_Size=224\n",
    "Patch_Size=16\n",
    "Num_Class=3\n",
    "Num_Heads=6\n",
    "Encoder_Layers=6\n",
    "Onnx_Export_Path='Exported_Fx_Onnx.onnx'\n",
    "model_fp=Vit(In_Channels=In_Channels,Out_Channels=Embed_Dim,Picture_Size=Picture_Size,Patch_Size=Patch_Size\n",
    ",Num_Class=Num_Class,Num_Heads=Num_Heads,Encoder_Layers=Encoder_Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(val_loader,model, device):\n",
    "\n",
    "    # lr = 0.0001\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # val_start_time = end = time.time()\n",
    "    # running_loss=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(val_loader):#一次验证一个batch，每个target就有batch的维度\n",
    "            data = data.to(device)\n",
    "            print(\"calibrate times:\",i)\n",
    "            # with torch.no_grad():\n",
    "            output = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torchvision\\transforms\\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "crop_pct = 0.9\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_dataset = datasets.ImageFolder('E:/Transformer/DataSets/imagenet/Mini_Train/val',train_transform)\n",
    "train_dataset = datasets.ImageFolder('E:/Transformer/DataSets/imagenet/Mini_Train/train',train_transform)\n",
    "model_save_path='Saved.pth'\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,#args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=8,#args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,#args.val_batchsize,\n",
    "    shuffle=True,\n",
    "    num_workers=8,#args.num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibrate times: 0\n",
      "calibrate times: 1\n",
      "calibrate times: 2\n",
      "calibrate times: 3\n",
      "calibrate times: 4\n",
      "calibrate times: 5\n",
      "calibrate times: 6\n",
      "calibrate times: 7\n",
      "calibrate times: 8\n",
      "calibrate times: 9\n",
      "calibrate times: 10\n",
      "calibrate times: 11\n",
      "calibrate times: 12\n",
      "calibrate times: 13\n",
      "calibrate times: 14\n",
      "calibrate times: 15\n",
      "calibrate times: 16\n",
      "calibrate times: 17\n",
      "calibrate times: 18\n",
      "calibrate times: 19\n",
      "calibrate times: 20\n",
      "calibrate times: 21\n",
      "calibrate times: 22\n",
      "calibrate times: 23\n",
      "calibrate times: 24\n",
      "calibrate times: 25\n",
      "calibrate times: 26\n",
      "calibrate times: 27\n",
      "calibrate times: 28\n",
      "calibrate times: 29\n",
      "calibrate times: 30\n",
      "calibrate times: 31\n",
      "calibrate times: 32\n",
      "calibrate times: 33\n",
      "calibrate times: 34\n",
      "calibrate times: 35\n",
      "calibrate times: 36\n",
      "calibrate times: 37\n",
      "calibrate times: 38\n",
      "calibrate times: 39\n",
      "calibrate times: 40\n",
      "calibrate times: 41\n",
      "calibrate times: 42\n",
      "calibrate times: 43\n",
      "calibrate times: 44\n",
      "calibrate times: 45\n",
      "calibrate times: 46\n",
      "calibrate times: 47\n",
      "calibrate times: 48\n",
      "calibrate times: 49\n",
      "calibrate times: 50\n",
      "calibrate times: 51\n",
      "calibrate times: 52\n",
      "calibrate times: 53\n",
      "calibrate times: 54\n",
      "calibrate times: 55\n",
      "calibrate times: 56\n",
      "calibrate times: 57\n",
      "calibrate times: 58\n",
      "calibrate times: 59\n",
      "calibrate times: 60\n",
      "calibrate times: 61\n",
      "calibrate times: 62\n",
      "calibrate times: 63\n",
      "calibrate times: 64\n",
      "calibrate times: 65\n",
      "calibrate times: 66\n",
      "calibrate times: 67\n",
      "calibrate times: 68\n",
      "calibrate times: 69\n",
      "calibrate times: 70\n",
      "calibrate times: 71\n",
      "calibrate times: 72\n",
      "calibrate times: 73\n",
      "calibrate times: 74\n",
      "calibrate times: 75\n",
      "calibrate times: 76\n",
      "calibrate times: 77\n",
      "calibrate times: 78\n",
      "calibrate times: 79\n",
      "calibrate times: 80\n",
      "calibrate times: 81\n",
      "calibrate times: 82\n",
      "calibrate times: 83\n",
      "calibrate times: 84\n",
      "calibrate times: 85\n",
      "calibrate times: 86\n",
      "calibrate times: 87\n",
      "calibrate times: 88\n",
      "calibrate times: 89\n",
      "calibrate times: 90\n",
      "calibrate times: 91\n",
      "calibrate times: 92\n",
      "calibrate times: 93\n",
      "calibrate times: 94\n",
      "calibrate times: 95\n",
      "calibrate times: 96\n",
      "calibrate times: 97\n",
      "calibrate times: 98\n",
      "calibrate times: 99\n",
      "calibrate times: 100\n",
      "calibrate times: 101\n",
      "calibrate times: 102\n",
      "calibrate times: 103\n",
      "calibrate times: 104\n",
      "calibrate times: 105\n",
      "calibrate times: 106\n",
      "calibrate times: 107\n",
      "calibrate times: 108\n",
      "calibrate times: 109\n",
      "calibrate times: 110\n",
      "calibrate times: 111\n",
      "calibrate times: 112\n",
      "calibrate times: 113\n",
      "calibrate times: 114\n",
      "calibrate times: 115\n",
      "calibrate times: 116\n",
      "calibrate times: 117\n",
      "calibrate times: 118\n",
      "calibrate times: 119\n",
      "calibrate times: 120\n",
      "calibrate times: 121\n",
      "calibrate times: 122\n",
      "calibrate times: 123\n",
      "calibrate times: 124\n",
      "calibrate times: 125\n",
      "calibrate times: 126\n",
      "calibrate times: 127\n",
      "calibrate times: 128\n",
      "calibrate times: 129\n",
      "calibrate times: 130\n",
      "calibrate times: 131\n",
      "calibrate times: 132\n",
      "calibrate times: 133\n",
      "calibrate times: 134\n",
      "calibrate times: 135\n",
      "calibrate times: 136\n",
      "calibrate times: 137\n",
      "calibrate times: 138\n",
      "calibrate times: 139\n",
      "calibrate times: 140\n",
      "calibrate times: 141\n",
      "calibrate times: 142\n",
      "calibrate times: 143\n",
      "calibrate times: 144\n",
      "calibrate times: 145\n",
      "calibrate times: 146\n",
      "calibrate times: 147\n",
      "calibrate times: 148\n",
      "calibrate times: 149\n"
     ]
    }
   ],
   "source": [
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "model_to_quantize.eval()\n",
    "# prepare\n",
    "example_inputs=torch.rand(1,3,224,224)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# calibrate\n",
    "model_prepared.to('cuda')\n",
    "calibrate(val_loader,model_prepared,'cuda')\n",
    "# quantize\n",
    "\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "getCudnnDataTypeFromScalarType() not supported for QUInt8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_quantized\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m validate(\u001b[39m1\u001b[39;49m,val_loader,model_quantized,criterion,\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32me:\\Transformer\\Transformer_Main\\MyTransformer\\examples\\../utils\\utils.py:123\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(print_freq, val_loader, model, criterion, device)\u001b[0m\n\u001b[0;32m    120\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 123\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m    124\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m    126\u001b[0m \u001b[39m# measure accuracy and record loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\fx\\graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped_call(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\fx\\graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\fx\\graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls, obj)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m<eval_with_key>.6:8\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m patch_embed_proj_input_zero_point_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embed_proj_input_zero_point_0\n\u001b[0;32m      7\u001b[0m quantize_per_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquantize_per_tensor(x, patch_embed_proj_input_scale_0, patch_embed_proj_input_zero_point_0, torch\u001b[39m.\u001b[39mquint8);  patch_embed_proj_input_scale_0 \u001b[39m=\u001b[39m patch_embed_proj_input_zero_point_0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m patch_embed_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embed\u001b[39m.\u001b[39;49mproj(quantize_per_tensor);  quantize_per_tensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(patch_embed_proj, \u001b[39m2\u001b[39m);  patch_embed_proj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     10\u001b[0m transpose \u001b[39m=\u001b[39m flatten\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m);  flatten \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    460\u001b[0m     _reversed_padding_repeated_twice \u001b[39m=\u001b[39m _reverse_repeat_padding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding)\n\u001b[0;32m    461\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[0;32m    462\u001b[0m                   mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode)\n\u001b[1;32m--> 463\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mquantized\u001b[39m.\u001b[39;49mconv2d(\n\u001b[0;32m    464\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_packed_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_point)\n",
      "File \u001b[1;32mc:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: getCudnnDataTypeFromScalarType() not supported for QUInt8"
     ]
    }
   ],
   "source": [
    "#validate:\n",
    "import torch.nn as nn\n",
    "model_quantized.to('cpu')\n",
    "criterion = nn.CrossEntropyLoss().to('cpu')\n",
    "validate(1,val_loader,model_quantized,criterion,'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (patch_embed): Module(\n",
      "    (proj): QuantizedConv2d(3, 384, kernel_size=(16, 16), stride=(16, 16), scale=0.015282480977475643, zero_point=128)\n",
      "  )\n",
      "  (Encoders): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): QuantizedLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): QuantizedLinear(in_features=384, out_features=3, scale=0.0061056362465023994, zero_point=228, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    patch_embed_proj_input_scale_0 = self.patch_embed_proj_input_scale_0\n",
      "    patch_embed_proj_input_zero_point_0 = self.patch_embed_proj_input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(x, patch_embed_proj_input_scale_0, patch_embed_proj_input_zero_point_0, torch.quint8);  patch_embed_proj_input_scale_0 = patch_embed_proj_input_zero_point_0 = None\n",
      "    patch_embed_proj = self.patch_embed.proj(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    flatten = torch.flatten(patch_embed_proj, 2);  patch_embed_proj = None\n",
      "    transpose = flatten.transpose(1, 2);  flatten = None\n",
      "    patch_embed_cls_token = self.patch_embed.cls_token\n",
      "    getattr_1 = x.shape;  x = None\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    expand = patch_embed_cls_token.expand(getitem, -1, -1);  patch_embed_cls_token = getitem = None\n",
      "    patch_embed_scale_2 = self.patch_embed_scale_2\n",
      "    patch_embed_zero_point_2 = self.patch_embed_zero_point_2\n",
      "    quantize_per_tensor_4 = torch.quantize_per_tensor(expand, patch_embed_scale_2, patch_embed_zero_point_2, torch.quint8);  expand = patch_embed_scale_2 = patch_embed_zero_point_2 = None\n",
      "    cat = torch.cat((quantize_per_tensor_4, transpose), dim = 1);  quantize_per_tensor_4 = transpose = None\n",
      "    patch_embed_pos_embedding = self.patch_embed.pos_embedding\n",
      "    patch_embed_scale_4 = self.patch_embed_scale_4\n",
      "    patch_embed_zero_point_4 = self.patch_embed_zero_point_4\n",
      "    quantize_per_tensor_6 = torch.quantize_per_tensor(patch_embed_pos_embedding, patch_embed_scale_4, patch_embed_zero_point_4, torch.quint8);  patch_embed_pos_embedding = patch_embed_scale_4 = patch_embed_zero_point_4 = None\n",
      "    patch_embed_scale_5 = self.patch_embed_scale_5\n",
      "    patch_embed_zero_point_5 = self.patch_embed_zero_point_5\n",
      "    add_1 = torch.ops.quantized.add(cat, quantize_per_tensor_6, patch_embed_scale_5, patch_embed_zero_point_5);  cat = quantize_per_tensor_6 = patch_embed_scale_5 = patch_embed_zero_point_5 = None\n",
      "    dequantize_7 = add_1.dequantize();  add_1 = None\n",
      "    encoders = self.Encoders(dequantize_7);  dequantize_7 = None\n",
      "    encoders_scale_0 = self.Encoders_scale_0\n",
      "    encoders_zero_point_0 = self.Encoders_zero_point_0\n",
      "    quantize_per_tensor_8 = torch.quantize_per_tensor(encoders, encoders_scale_0, encoders_zero_point_0, torch.quint8);  encoders = encoders_scale_0 = encoders_zero_point_0 = None\n",
      "    norm = self.norm(quantize_per_tensor_8);  quantize_per_tensor_8 = None\n",
      "    dequantize_9 = norm.dequantize();  norm = None\n",
      "    getitem_4 = dequantize_9[(slice(None, None, None), 0)];  dequantize_9 = None\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    quantize_per_tensor_10 = torch.quantize_per_tensor(getitem_4, _scale_0, _zero_point_0, torch.quint8);  getitem_4 = _scale_0 = _zero_point_0 = None\n",
      "    head = self.head(quantize_per_tensor_10);  quantize_per_tensor_10 = None\n",
      "    dequantize_11 = head.dequantize();  head = None\n",
      "    return dequantize_11\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_quantized.print_readable()\n",
    "print(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "#val\n",
    "a=10.232\n",
    "print(str(a)[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "278897701cc6d4f1b9746803770f5e818cccf5e556f859a8255d5456d962739d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
