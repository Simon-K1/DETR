{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "#Trasnforemr相关组件====================================\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "class PatchEmbed(nn.Module):#默认为vit—base\n",
    "    # patch_size=16,\n",
    "    # embed_dim=768,\n",
    "    # depth=12,\n",
    "    # num_heads=12,\n",
    "    # mlp_ratio=4,\n",
    "\n",
    "    def __init__(self,In_Channels:int=1,Out_Channels:int=768,img_size:int=224,patch_size:int=16):\n",
    "        super(PatchEmbed,self).__init__()\n",
    "        self.proj=nn.Conv2d(in_channels=In_Channels,out_channels=Out_Channels, kernel_size=patch_size,stride=patch_size,)\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0],\n",
    "                          img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "    def forward(self,x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        print(\"Out shale:\",x.shape)\n",
    "        return x\n",
    "def Test_PatchEmbed():\n",
    "    model=PatchEmbed(In_Channels=1,Out_Channels=384)\n",
    "    output=model(torch.rand(1,1,224,224))\n",
    "    print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "Out shale: torch.Size([2, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "#构建Embedding 层\n",
    "In_Channels=3\n",
    "Batch_Size=2\n",
    "Out_Channels=768\n",
    "Picture_Size=224\n",
    "x=torch.rand(Batch_Size,In_Channels,Picture_Size,Picture_Size)\n",
    "Embedding_Layer=PatchEmbed(In_Channels=In_Channels,Out_Channels=Out_Channels)\n",
    "Embedding_Out=Embedding_Layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "#开始构建Encoder\n",
    "#[B,H,W]\n",
    "B=Embedding_Out.shape[0]\n",
    "H=Embedding_Out.shape[1]\n",
    "W=Embedding_Out.shape[2]\n",
    "Num_Heads=8\n",
    "\n",
    "Single_Encoder = nn.TransformerEncoderLayer(d_model=W, nhead=Num_Heads,batch_first=True,norm_first=True)#在Vit中的norm需要位于atten和FF之前\n",
    "Encoders= nn.TransformerEncoder(Single_Encoder,num_layers=12,norm=None)#构建多个连在一起的Encoder\n",
    "                                                                      #nomrm 就是在末尾加一个normlization\n",
    "\n",
    "Encoder_Out=Encoders(Embedding_Out)\n",
    "print(Encoder_Out.shape)\n",
    "                                            \n",
    "\n",
    "# print(x.shape)\n",
    "# print(Single_Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 196, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "NormLayer_After_Encoder=nn.LayerNorm(768,)\n",
    "Norm_Out=NormLayer_After_Encoder(Encoder_Out)\n",
    "print(Norm_Out.shape)\n",
    "print(NormLayer_After_Encoder.weight.shape)\n",
    "Norm_Out=Norm_Out[:,0]\n",
    "print(Norm_Out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 196, 1])\n"
     ]
    }
   ],
   "source": [
    "#开始构建Mlp head,Fq-vit直接放一个linear就没了，，，，\n",
    "Num_Class=1\n",
    "Mlp_Head=nn.Linear(W,Num_Class)\n",
    "Class_Out=Mlp_Head(Encoder_Out)\n",
    "print(Class_Out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "495274a28bdb076e4e5d468612a890f31d5e619c1dd6c8cb530d5f7b822d194c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
