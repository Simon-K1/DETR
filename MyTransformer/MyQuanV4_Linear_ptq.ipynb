{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "  (sub): Submodule(\n",
       "    (linear): Linear(in_features=2304, out_features=2304, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用pytorch做训练后量化\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.models.resnet import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torch.quantization import MinMaxObserver\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "_ = torch.manual_seed(191009)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader,device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image=image.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            print('Val[',cnt,\"]  [top1]:\",acc1,\"[top5]:\",acc5)\n",
    "    \n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = resnet18(pretrained=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(\"cpu\")\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    if isinstance(model, torch.jit.RecursiveScriptModule):\n",
    "        torch.jit.save(model, \"temp.p\")\n",
    "    else:\n",
    "        torch.jit.save(torch.jit.script(model), \"temp.p\")\n",
    "    print(\"Size (MB):\", os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove(\"temp.p\")\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        data_path+'/train',transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        data_path+'/val',  transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "data_path = 'E:/Transformer/DataSets/imagenet/Mini_Train'\n",
    "saved_model_dir = 'Export/Ptq'\n",
    "float_model_file = 'pretrained_float.pth'\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 1\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "example_inputs = (next(iter(data_loader))[0])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class Submodule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(768*3, 768*3)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj=nn.Conv2d(in_channels=3,out_channels=768, kernel_size=16,stride=16,)\n",
    "        self.linear = torch.nn.Linear(768, 768*3)\n",
    "        self.sub = Submodule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.proj(x)\n",
    "        x=torch.flatten(x,2).transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        x = self.sub(x) + x\n",
    "        return x\n",
    "float_model=M()\n",
    "float_model.eval()\n",
    "float_model.to('cuda')\n",
    "# deepcopy the model since we need to keep the original model around\n",
    "import copy\n",
    "model_to_quantize = copy.deepcopy(float_model)\n",
    "model_to_quantize.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x]\n",
      "    %activation_post_process_0 : [#users=1] = call_module[target=activation_post_process_0](args = (%x,), kwargs = {})\n",
      "    %proj : [#users=1] = call_module[target=proj](args = (%activation_post_process_0,), kwargs = {})\n",
      "    %activation_post_process_1 : [#users=1] = call_module[target=activation_post_process_1](args = (%proj,), kwargs = {})\n",
      "    %flatten : [#users=1] = call_function[target=torch.flatten](args = (%activation_post_process_1, 2), kwargs = {})\n",
      "    %activation_post_process_2 : [#users=1] = call_module[target=activation_post_process_2](args = (%flatten,), kwargs = {})\n",
      "    %transpose : [#users=1] = call_method[target=transpose](args = (%activation_post_process_2, 1, 2), kwargs = {})\n",
      "    %activation_post_process_3 : [#users=1] = call_module[target=activation_post_process_3](args = (%transpose,), kwargs = {})\n",
      "    %linear : [#users=1] = call_module[target=linear](args = (%activation_post_process_3,), kwargs = {})\n",
      "    %activation_post_process_4 : [#users=2] = call_module[target=activation_post_process_4](args = (%linear,), kwargs = {})\n",
      "    %sub_linear : [#users=1] = call_module[target=sub.linear](args = (%activation_post_process_4,), kwargs = {})\n",
      "    %activation_post_process_5 : [#users=1] = call_module[target=activation_post_process_5](args = (%sub_linear,), kwargs = {})\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_5, %activation_post_process_4), kwargs = {})\n",
      "    %activation_post_process_6 : [#users=1] = call_module[target=activation_post_process_6](args = (%add,), kwargs = {})\n",
      "    return activation_post_process_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# qconfig_mapping = QConfigMapping.set_global(default_qconfig)\n",
    "# qconfig_opt=None\n",
    "# qconfig_mapping = (QConfigMapping()\n",
    "#     .set_global(qconfig_opt)  # qconfig_opt is an optional qconfig, either a valid qconfig or None\n",
    "#     .set_object_type(torch.nn.Conv2d, qconfig_opt) # can be a callable...\n",
    "#     .set_object_type(\"torch.nn.functional.add\", qconfig_opt) # ...or a string of the class name\n",
    "#     .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig_opt) # matched in order, first match takes precedence\n",
    "#     .set_module_name(\"foo.bar\", qconfig_opt)\n",
    "#     .set_module_name_object_type_order()\n",
    "# )\n",
    "#     # priority (in increasing order): global, object_type, module_name_regex, module_name\n",
    "#     # qconfig == None means fusion and quantization should be skipped for anything\n",
    "#     # matching the rule (unless a higher priority match is found)\n",
    "from torch.ao.quantization.backend_config import DTypeConfig,BackendPatternConfig,ObservationType,BackendConfig\n",
    "weighted_int8_dtype_config = DTypeConfig(\n",
    "  input_dtype=torch.quint8,\n",
    "  output_dtype=torch.quint8,\n",
    "  weight_dtype=torch.qint8,\n",
    "  bias_dtype=torch.float)\n",
    "\n",
    "linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\n",
    "   .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n",
    "   .add_dtype_config(weighted_int8_dtype_config) \\\n",
    "\n",
    "linear_backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qlinear_cfg=torch.quantization.QConfig(\n",
    "   activation=MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "   weight=MinMaxObserver.with_args(dtype=torch.qint8))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)#.set_object_type(torch.nn.Linear, qlinear_cfg)\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)#,backend_config=linear_backend_config)\n",
    "print(prepared_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibrate times end 150\n",
      "===========calibrate end===========\n",
      "GraphModule(\n",
      "  (proj): QuantizedConv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), scale=0.06182935833930969, zero_point=63)\n",
      "  (linear): QuantizedLinear(in_features=768, out_features=2304, scale=0.04096301272511482, zero_point=70, qscheme=torch.per_channel_affine)\n",
      "  (sub): Module(\n",
      "    (linear): QuantizedLinear(in_features=2304, out_features=2304, scale=0.025224722921848297, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    proj_input_scale_0 = self.proj_input_scale_0\n",
      "    proj_input_zero_point_0 = self.proj_input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(x, proj_input_scale_0, proj_input_zero_point_0, torch.quint8);  x = proj_input_scale_0 = proj_input_zero_point_0 = None\n",
      "    proj = self.proj(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    flatten = torch.flatten(proj, 2);  proj = None\n",
      "    transpose = flatten.transpose(1, 2);  flatten = None\n",
      "    linear = self.linear(transpose);  transpose = None\n",
      "    sub_linear = self.sub.linear(linear)\n",
      "    _scale_2 = self._scale_2\n",
      "    _zero_point_2 = self._zero_point_2\n",
      "    add_1 = torch.ops.quantized.add(sub_linear, linear, _scale_2, _zero_point_2);  sub_linear = linear = _scale_2 = _zero_point_2 = None\n",
      "    dequantize_6 = add_1.dequantize();  add_1 = None\n",
      "    return dequantize_6\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "Size of model before quantization\n",
      "Size (MB): 30.699191\n",
      "Size of model after quantization\n",
      "Size (MB): 7.789621\n"
     ]
    }
   ],
   "source": [
    "def calibrate(model, data_loader,device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image=image.to(device)\n",
    "            target=target.to(device)\n",
    "            model(image)\n",
    "            \n",
    "            i+=1       \n",
    "    print(\"calibrate times end\",len(data_loader)) \n",
    "calibrate(prepared_model, data_loader_test,'cuda')  # run calibration on sample data\n",
    "                #很奇怪，这里得用cpu校准，用gpu校准下一步就过不去了，，，，，，，，，，\n",
    "print(\"===========calibrate end===========\")\n",
    "prepared_model.to('cpu')#这里得改成cpu，很奇怪\n",
    "quantized_model = convert_fx(prepared_model)\n",
    "print(quantized_model)\n",
    "\n",
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(float_model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model)\n",
    "# print(quantized_model.parameters)\n",
    "\n",
    "# test_input=torch.rand(1,3,224,224).to('cuda')\n",
    "# quantized_model.to('cuda')\n",
    "# out=quantized_model(test_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0314,  0.0223,  0.0167,  ..., -0.0237,  0.0266,  0.0059],\n",
      "        [ 0.0282, -0.0056,  0.0240,  ..., -0.0350, -0.0062, -0.0209],\n",
      "        [-0.0181,  0.0280,  0.0057,  ...,  0.0062, -0.0119,  0.0144],\n",
      "        ...,\n",
      "        [ 0.0175, -0.0195,  0.0201,  ...,  0.0243,  0.0000, -0.0122],\n",
      "        [-0.0189, -0.0359, -0.0020,  ..., -0.0116,  0.0212, -0.0345],\n",
      "        [-0.0172, -0.0147,  0.0113,  ..., -0.0223, -0.0212, -0.0280]],\n",
      "       size=(2304, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n",
      "       dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0)\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model.linear.weight())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "278897701cc6d4f1b9746803770f5e818cccf5e556f859a8255d5456d962739d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
