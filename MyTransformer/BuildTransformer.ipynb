{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建完整的一个transformer\n",
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from utils import load_weights_from_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(Input:torch.tensor=None,model:torch.nn=None,export_path:str='exported_onnx.onnx'):\n",
    "    if export_path is None:\n",
    "        export_path='exported_onnx.onnx'\n",
    "    torch.onnx.export(model,               # model being run\n",
    "                Input,                         # model input \n",
    "                export_path,   # where to save the model (can be a file or file-like object)                  \n",
    "                opset_version=11,           # the ONNX version to export the model to                  \n",
    "                input_names = ['input'],   # the model's input names\n",
    "                output_names = ['output']  # the model's output names\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picture=torch.rand(1,3,224,224)\n",
    "In_Channels=3\n",
    "Embed_Dim=384\n",
    "Picture_Size=224\n",
    "Patch_Size=16\n",
    "Num_Class=3\n",
    "Num_Heads=6\n",
    "Encoder_Layers=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PatchEmbed layer\n",
    "#只有一个卷积\n",
    "class proj(nn.Module):#默认为vit—base\n",
    "    # patch_size=16,\n",
    "    # embed_dim=768,\n",
    "    # depth=12,\n",
    "    # num_heads=12,\n",
    "    # mlp_ratio=4,\n",
    "\n",
    "    def __init__(self,In_Channels:int=1,Out_Channels:int=768,img_size:int=224,patch_size:int=16):\n",
    "        super(proj,self).__init__()\n",
    "        self.proj=nn.Conv2d(in_channels=In_Channels,out_channels=Out_Channels, kernel_size=patch_size,stride=patch_size,)\n",
    "        # img_size = to_2tuple(img_size)\n",
    "        # patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size // patch_size,\n",
    "                          img_size // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        x=torch.flatten(x,2).transpose(1, 2)\n",
    "        # print(\"Out shale:\",x.shape)\n",
    "        return x\n",
    "Layer0=proj(In_Channels,Embed_Dim,Picture_Size,patch_size=Patch_Size)\n",
    "export_onnx(Picture,Layer0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PatchEmbed layer\n",
    "#添加位置编码\n",
    "class proj(nn.Module):#默认为vit—base\n",
    "    # patch_size=16,\n",
    "    # embed_dim=768,\n",
    "    # depth=12,\n",
    "    # num_heads=12,\n",
    "    # mlp_ratio=4,\n",
    "\n",
    "    def __init__(self,In_Channels:int=1,Out_Channels:int=768,img_size:int=224,patch_size:int=16):\n",
    "        super(proj,self).__init__()\n",
    "        self.proj=nn.Conv2d(in_channels=In_Channels,out_channels=Out_Channels, kernel_size=patch_size,stride=patch_size,)\n",
    "        # img_size = to_2tuple(img_size)\n",
    "        # patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size // patch_size,\n",
    "                          img_size // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, Out_Channels))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, Out_Channels))\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        Input_Shape=x.shape\n",
    "        x = self.proj(x)\n",
    "        x=torch.flatten(x,2).transpose(1, 2)\n",
    "        cls_token=self.cls_token.expand(Input_Shape[0],-1,-1)\n",
    "        x=torch.cat((cls_token,x),dim=1)\n",
    "        x+=self.pos_embedding\n",
    "\n",
    "        return x\n",
    "Layer0=proj(In_Channels,Embed_Dim,Picture_Size,patch_size=Patch_Size)\n",
    "export_onnx(Picture,Layer0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建Vit\n",
    "class Vit(nn.Module):\n",
    "    def __init__(self,In_Channels:int=3,Out_Channels:int=768,Picture_Size:int=224,Patch_Size:int=16,Num_Class:int=1,Num_Heads:int=8,Encoder_Layers:int=12):\n",
    "        super(Vit,self).__init__()\n",
    "        self.In_Channels=In_Channels\n",
    "        # self.Batch_Size=2\n",
    "        self.Embed_Dim=Out_Channels\n",
    "        self.Picture_Size=Picture_Size\n",
    "        # self.x=torch.rand(Batch_Size,In_Channels,Picture_Size,Picture_Size)\n",
    "        self.patch_embed=proj(In_Channels=self.In_Channels,Out_Channels=self.Embed_Dim)\n",
    "        # self.Embedding_Out=Embedding_Layer(x)\n",
    "\n",
    "        self.Num_Heads=Num_Heads\n",
    "        self.Encoder_layers=Encoder_Layers\n",
    "        #开始构建Encoder\n",
    "        #[B,H,W]\n",
    "        # self.cls_token = nn.Parameter(torch.zeros(1, 1, self.Embed_Dim))\n",
    "        # num_patches = self.patch_embed.num_patches\n",
    "        # self.pos_embed = nn.Parameter(\n",
    "        #     torch.zeros(1, num_patches + 1, self.Embed_Dim))\n",
    "        \n",
    "        \n",
    "\n",
    "        self.Single_Encoder = nn.TransformerEncoderLayer(d_model=self.Embed_Dim, nhead=self.Num_Heads,batch_first=True,norm_first=True)#在Vit中的norm需要位于atten和FF之前\n",
    "        self.Encoders= nn.TransformerEncoder(self.Single_Encoder,num_layers=self.Encoder_layers,norm=None)#构建多个连在一起的Encoder\n",
    "                                                                            #nomrm 就是在末尾加一个normlization\n",
    "\n",
    "        self.norm=nn.LayerNorm(self.Embed_Dim,)\n",
    "        \n",
    "        self.Num_Class=Num_Class\n",
    "        self.head=nn.Linear(self.Embed_Dim,self.Num_Class)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "\n",
    "        self.emb_dropout=0.1\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        # print(Class_Out.shape)\n",
    "                \n",
    "                                                    \n",
    "    def forward(self,x):\n",
    "        x=self.patch_embed(x)\n",
    "        \n",
    "        B=x.shape[0]\n",
    "        H=x.shape[1]\n",
    "        W=x.shape[2]\n",
    "        Encoder_Out=self.Encoders(x)\n",
    "\n",
    "        # cls_tokens = self.cls_token.expand(\n",
    "        #     B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        # # print(Encoder_Out.shape)\n",
    "        # Encoder_Out = torch.cat((cls_tokens, Encoder_Out), dim=1)\n",
    "        # Encoder_Out = Encoder_Out + self.pos_embed\n",
    "\n",
    "        Norm_Out=self.norm(Encoder_Out)\n",
    "        Norm_Out=Norm_Out[:,0]\n",
    "        Class_Out=self.head(Norm_Out)\n",
    "        return Class_Out\n",
    "Layer0=Vit(In_Channels,Embed_Dim,Picture_Size,Patch_Size=Patch_Size)\n",
    "export_onnx(Picture,Layer0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "278897701cc6d4f1b9746803770f5e818cccf5e556f859a8255d5456d962739d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
