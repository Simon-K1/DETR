{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pytorch做训练后量化\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.models.resnet import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torch.quantization import MinMaxObserver\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "_ = torch.manual_seed(191009)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader,device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image=image.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            print('Val[',cnt,\"]  [top1]:\",acc1,\"[top5]:\",acc5)\n",
    "    \n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = resnet18(pretrained=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(\"cpu\")\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    if isinstance(model, torch.jit.RecursiveScriptModule):\n",
    "        torch.jit.save(model, \"temp.p\")\n",
    "    else:\n",
    "        torch.jit.save(torch.jit.script(model), \"temp.p\")\n",
    "    print(\"Size (MB):\", os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove(\"temp.p\")\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        data_path+'/train',transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        data_path+'/val',  transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "data_path = 'E:/Transformer/DataSets/imagenet/Mini_Train'\n",
    "saved_model_dir = 'Export/Ptq'\n",
    "float_model_file = 'pretrained_float.pth'\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 1\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "example_inputs = (next(iter(data_loader))[0])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from Mymodels import Vit\n",
    "In_Channels=3\n",
    "Embed_Dim=384\n",
    "Picture_Size=224\n",
    "Patch_Size=16\n",
    "Num_Class=3\n",
    "Num_Heads=6\n",
    "Encoder_Layers=6\n",
    "float_model=Vit(In_Channels=In_Channels,Out_Channels=Embed_Dim,Picture_Size=Picture_Size,Patch_Size=Patch_Size\n",
    ",Num_Class=Num_Class,Num_Heads=Num_Heads,Encoder_Layers=Encoder_Layers)\n",
    "pretrained=True\n",
    "if pretrained:\n",
    "    state_dict = torch.load(r'E:\\Transformer\\Transformer_Main\\MyTransformer\\Export\\float\\FloatVit_93.3333333333333398.0.pth')\n",
    "    float_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "float_model.eval()\n",
    "float_model.to('cuda')\n",
    "# deepcopy the model since we need to keep the original model around\n",
    "import copy\n",
    "model_to_quantize = copy.deepcopy(float_model)\n",
    "model_to_quantize.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (patch_embed): Module(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (Encoders): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  (head): Linear(in_features=384, out_features=3, bias=True)\n",
      "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    patch_embed_proj = self.patch_embed.proj(x)\n",
      "    flatten = torch.flatten(patch_embed_proj, 2);  patch_embed_proj = None\n",
      "    transpose = flatten.transpose(1, 2);  flatten = None\n",
      "    patch_embed_cls_token = self.patch_embed.cls_token\n",
      "    getattr_1 = x.shape;  x = None\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    expand = patch_embed_cls_token.expand(getitem, -1, -1);  patch_embed_cls_token = getitem = None\n",
      "    cat = torch.cat((expand, transpose), dim = 1);  expand = transpose = None\n",
      "    patch_embed_pos_embedding = self.patch_embed.pos_embedding\n",
      "    add = cat + patch_embed_pos_embedding;  cat = patch_embed_pos_embedding = None\n",
      "    getattr_2 = add.shape\n",
      "    getitem_1 = getattr_2[0];  getattr_2 = None\n",
      "    getattr_3 = add.shape\n",
      "    getitem_2 = getattr_3[1];  getattr_3 = None\n",
      "    getattr_4 = add.shape\n",
      "    getitem_3 = getattr_4[2];  getattr_4 = None\n",
      "    encoders = self.Encoders(add);  add = None\n",
      "    norm = self.norm(encoders);  encoders = None\n",
      "    getitem_4 = norm[(slice(None, None, None), 0)];  norm = None\n",
      "    activation_post_process_0 = self.activation_post_process_0(getitem_4);  getitem_4 = None\n",
      "    head = self.head(activation_post_process_0);  activation_post_process_0 = None\n",
      "    activation_post_process_1 = self.activation_post_process_1(head);  head = None\n",
      "    return activation_post_process_1\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25073\\.conda\\envs\\YoloV5\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# qconfig_mapping = QConfigMapping.set_global(default_qconfig)\n",
    "# qconfig_opt=None\n",
    "# qconfig_mapping = (QConfigMapping()\n",
    "#     .set_global(qconfig_opt)  # qconfig_opt is an optional qconfig, either a valid qconfig or None\n",
    "#     .set_object_type(torch.nn.Conv2d, qconfig_opt) # can be a callable...\n",
    "#     .set_object_type(\"torch.nn.functional.add\", qconfig_opt) # ...or a string of the class name\n",
    "#     .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig_opt) # matched in order, first match takes precedence\n",
    "#     .set_module_name(\"foo.bar\", qconfig_opt)\n",
    "#     .set_module_name_object_type_order()\n",
    "# )\n",
    "#     # priority (in increasing order): global, object_type, module_name_regex, module_name\n",
    "#     # qconfig == None means fusion and quantization should be skipped for anything\n",
    "#     # matching the rule (unless a higher priority match is found)\n",
    "from torch.ao.quantization.backend_config import DTypeConfig,BackendPatternConfig,ObservationType,BackendConfig\n",
    "weighted_int8_dtype_config = DTypeConfig(\n",
    "  input_dtype=torch.quint8,\n",
    "  output_dtype=torch.quint8,\n",
    "  weight_dtype=torch.qint8,\n",
    "  bias_dtype=torch.float)\n",
    "\n",
    "linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\n",
    "   .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n",
    "   .add_dtype_config(weighted_int8_dtype_config) \\\n",
    "\n",
    "linear_backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qlinear_cfg=torch.quantization.QConfig(\n",
    "   activation=MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "   weight=MinMaxObserver.with_args(dtype=torch.qint8))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)#.set_object_type(torch.nn.Linear, qlinear_cfg)\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs,backend_config=linear_backend_config)\n",
    "# print(prepared_model)\n",
    "\n",
    "print(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibrate times 0\n",
      "calibrate times 1\n",
      "calibrate times 2\n",
      "calibrate times 3\n",
      "calibrate times 4\n",
      "calibrate times 5\n",
      "calibrate times 6\n",
      "calibrate times 7\n",
      "calibrate times 8\n",
      "calibrate times 9\n",
      "calibrate times 10\n",
      "calibrate times 11\n",
      "calibrate times 12\n",
      "calibrate times 13\n",
      "calibrate times 14\n",
      "calibrate times 15\n",
      "calibrate times 16\n",
      "calibrate times 17\n",
      "calibrate times 18\n",
      "calibrate times 19\n",
      "calibrate times 20\n",
      "calibrate times 21\n",
      "calibrate times 22\n",
      "calibrate times 23\n",
      "calibrate times 24\n",
      "calibrate times 25\n",
      "calibrate times 26\n",
      "calibrate times 27\n",
      "calibrate times 28\n",
      "calibrate times 29\n",
      "calibrate times 30\n",
      "calibrate times 31\n",
      "calibrate times 32\n",
      "calibrate times 33\n",
      "calibrate times 34\n",
      "calibrate times 35\n",
      "calibrate times 36\n",
      "calibrate times 37\n",
      "calibrate times 38\n",
      "calibrate times 39\n",
      "calibrate times 40\n",
      "calibrate times 41\n",
      "calibrate times 42\n",
      "calibrate times 43\n",
      "calibrate times 44\n",
      "calibrate times 45\n",
      "calibrate times 46\n",
      "calibrate times 47\n",
      "calibrate times 48\n",
      "calibrate times 49\n",
      "calibrate times 50\n",
      "calibrate times 51\n",
      "calibrate times 52\n",
      "calibrate times 53\n",
      "calibrate times 54\n",
      "calibrate times 55\n",
      "calibrate times 56\n",
      "calibrate times 57\n",
      "calibrate times 58\n",
      "calibrate times 59\n",
      "calibrate times 60\n",
      "calibrate times 61\n",
      "calibrate times 62\n",
      "calibrate times 63\n",
      "calibrate times 64\n",
      "calibrate times 65\n",
      "calibrate times 66\n",
      "calibrate times 67\n",
      "calibrate times 68\n",
      "calibrate times 69\n",
      "calibrate times 70\n",
      "calibrate times 71\n",
      "calibrate times 72\n",
      "calibrate times 73\n",
      "calibrate times 74\n",
      "calibrate times 75\n",
      "calibrate times 76\n",
      "calibrate times 77\n",
      "calibrate times 78\n",
      "calibrate times 79\n",
      "calibrate times 80\n",
      "calibrate times 81\n",
      "calibrate times 82\n",
      "calibrate times 83\n",
      "calibrate times 84\n",
      "calibrate times 85\n",
      "calibrate times 86\n",
      "calibrate times 87\n",
      "calibrate times 88\n",
      "calibrate times 89\n",
      "calibrate times 90\n",
      "calibrate times 91\n",
      "calibrate times 92\n",
      "calibrate times 93\n",
      "calibrate times 94\n",
      "calibrate times 95\n",
      "calibrate times 96\n",
      "calibrate times 97\n",
      "calibrate times 98\n",
      "calibrate times 99\n",
      "calibrate times 100\n",
      "calibrate times 101\n",
      "calibrate times 102\n",
      "calibrate times 103\n",
      "calibrate times 104\n",
      "calibrate times 105\n",
      "calibrate times 106\n",
      "calibrate times 107\n",
      "calibrate times 108\n",
      "calibrate times 109\n",
      "calibrate times 110\n",
      "calibrate times 111\n",
      "calibrate times 112\n",
      "calibrate times 113\n",
      "calibrate times 114\n",
      "calibrate times 115\n",
      "calibrate times 116\n",
      "calibrate times 117\n",
      "calibrate times 118\n",
      "calibrate times 119\n",
      "calibrate times 120\n",
      "calibrate times 121\n",
      "calibrate times 122\n",
      "calibrate times 123\n",
      "calibrate times 124\n",
      "calibrate times 125\n",
      "calibrate times 126\n",
      "calibrate times 127\n",
      "calibrate times 128\n",
      "calibrate times 129\n",
      "calibrate times 130\n",
      "calibrate times 131\n",
      "calibrate times 132\n",
      "calibrate times 133\n",
      "calibrate times 134\n",
      "calibrate times 135\n",
      "calibrate times 136\n",
      "calibrate times 137\n",
      "calibrate times 138\n",
      "calibrate times 139\n",
      "calibrate times 140\n",
      "calibrate times 141\n",
      "calibrate times 142\n",
      "calibrate times 143\n",
      "calibrate times 144\n",
      "calibrate times 145\n",
      "calibrate times 146\n",
      "calibrate times 147\n",
      "calibrate times 148\n",
      "calibrate times 149\n",
      "===========calibrate end===========\n",
      "GraphModule(\n",
      "  (patch_embed): Module(\n",
      "    (proj): QuantizedConv2d(3, 384, kernel_size=(16, 16), stride=(16, 16), scale=0.15201020240783691, zero_point=63)\n",
      "  )\n",
      "  (Encoders): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): QuantizedLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): QuantizedLinear(in_features=384, out_features=3, scale=0.09224576503038406, zero_point=56, qscheme=torch.per_channel_affine)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    patch_embed_proj_input_scale_0 = self.patch_embed_proj_input_scale_0\n",
      "    patch_embed_proj_input_zero_point_0 = self.patch_embed_proj_input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(x, patch_embed_proj_input_scale_0, patch_embed_proj_input_zero_point_0, torch.quint8);  patch_embed_proj_input_scale_0 = patch_embed_proj_input_zero_point_0 = None\n",
      "    patch_embed_proj = self.patch_embed.proj(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    flatten = torch.flatten(patch_embed_proj, 2);  patch_embed_proj = None\n",
      "    transpose = flatten.transpose(1, 2);  flatten = None\n",
      "    patch_embed_cls_token = self.patch_embed.cls_token\n",
      "    getattr_1 = x.shape;  x = None\n",
      "    getitem = getattr_1[0];  getattr_1 = None\n",
      "    expand = patch_embed_cls_token.expand(getitem, -1, -1);  patch_embed_cls_token = getitem = None\n",
      "    patch_embed_scale_2 = self.patch_embed_scale_2\n",
      "    patch_embed_zero_point_2 = self.patch_embed_zero_point_2\n",
      "    quantize_per_tensor_4 = torch.quantize_per_tensor(expand, patch_embed_scale_2, patch_embed_zero_point_2, torch.quint8);  expand = patch_embed_scale_2 = patch_embed_zero_point_2 = None\n",
      "    cat = torch.cat((quantize_per_tensor_4, transpose), dim = 1);  quantize_per_tensor_4 = transpose = None\n",
      "    patch_embed_pos_embedding = self.patch_embed.pos_embedding\n",
      "    patch_embed_scale_4 = self.patch_embed_scale_4\n",
      "    patch_embed_zero_point_4 = self.patch_embed_zero_point_4\n",
      "    quantize_per_tensor_6 = torch.quantize_per_tensor(patch_embed_pos_embedding, patch_embed_scale_4, patch_embed_zero_point_4, torch.quint8);  patch_embed_pos_embedding = patch_embed_scale_4 = patch_embed_zero_point_4 = None\n",
      "    patch_embed_scale_5 = self.patch_embed_scale_5\n",
      "    patch_embed_zero_point_5 = self.patch_embed_zero_point_5\n",
      "    add_1 = torch.ops.quantized.add(cat, quantize_per_tensor_6, patch_embed_scale_5, patch_embed_zero_point_5);  cat = quantize_per_tensor_6 = patch_embed_scale_5 = patch_embed_zero_point_5 = None\n",
      "    dequantize_7 = add_1.dequantize();  add_1 = None\n",
      "    encoders = self.Encoders(dequantize_7);  dequantize_7 = None\n",
      "    encoders_scale_0 = self.Encoders_scale_0\n",
      "    encoders_zero_point_0 = self.Encoders_zero_point_0\n",
      "    quantize_per_tensor_8 = torch.quantize_per_tensor(encoders, encoders_scale_0, encoders_zero_point_0, torch.quint8);  encoders = encoders_scale_0 = encoders_zero_point_0 = None\n",
      "    norm = self.norm(quantize_per_tensor_8);  quantize_per_tensor_8 = None\n",
      "    dequantize_9 = norm.dequantize();  norm = None\n",
      "    getitem_4 = dequantize_9[(slice(None, None, None), 0)];  dequantize_9 = None\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    quantize_per_tensor_10 = torch.quantize_per_tensor(getitem_4, _scale_0, _zero_point_0, torch.quint8);  getitem_4 = _scale_0 = _zero_point_0 = None\n",
      "    head = self.head(quantize_per_tensor_10);  quantize_per_tensor_10 = None\n",
      "    dequantize_11 = head.dequantize();  head = None\n",
      "    return dequantize_11\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "Size of model before quantization\n",
      "Size (MB): 62.276435\n",
      "Size of model after quantization\n",
      "Size (MB): 52.733867\n"
     ]
    }
   ],
   "source": [
    "def calibrate(model, data_loader,device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image=image.to(device)\n",
    "            target=target.to(device)\n",
    "            model(image)\n",
    "            print(\"calibrate times\",i)\n",
    "            i+=1        \n",
    "calibrate(prepared_model, data_loader_test,'cuda')  # run calibration on sample data\n",
    "                #很奇怪，这里得用cpu校准，用gpu校准下一步就过不去了，，，，，，，，，，\n",
    "print(\"===========calibrate end===========\")\n",
    "prepared_model.to('cpu')#这里得改成cpu，很奇怪\n",
    "quantized_model = convert_fx(prepared_model)\n",
    "print(quantized_model)\n",
    "\n",
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(float_model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model)\n",
    "# print(quantized_model.parameters)\n",
    "\n",
    "# test_input=torch.rand(1,3,224,224).to('cuda')\n",
    "# quantized_model.to('cuda')\n",
    "# out=quantized_model(test_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val[ 1 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 2 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 3 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 4 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 5 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 6 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 7 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 8 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 9 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 10 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 11 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 12 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 13 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 14 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 15 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 16 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 17 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 18 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 19 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 20 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 21 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 22 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 23 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 24 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 25 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 26 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 27 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 28 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 29 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 30 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 31 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 32 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 33 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 34 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 35 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 36 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 37 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 38 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 39 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 40 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 41 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 42 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 43 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 44 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 45 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 46 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 47 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 48 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 49 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 50 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 51 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 52 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 53 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 54 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 55 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 56 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 57 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 58 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 59 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 60 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 61 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 62 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 63 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 64 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 65 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 66 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 67 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 68 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 69 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 70 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 71 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 72 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 73 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 74 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 75 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 76 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 77 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 78 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 79 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 80 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 81 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 82 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 83 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 84 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 85 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 86 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 87 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 88 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 89 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 90 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 91 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 92 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 93 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 94 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 95 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 96 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 97 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 98 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 99 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 100 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 101 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 102 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 103 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 104 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 105 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 106 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 107 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 108 ]  [top1]: tensor([0.]) [top5]: tensor([0.])\n",
      "Val[ 109 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 110 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 111 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 112 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 113 ]  [top1]: tensor([0.]) [top5]: tensor([0.])\n",
      "Val[ 114 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 115 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 116 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 117 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 118 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 119 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 120 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 121 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 122 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 123 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 124 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 125 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 126 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 127 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 128 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 129 ]  [top1]: tensor([0.]) [top5]: tensor([100.])\n",
      "Val[ 130 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 131 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 132 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 133 ]  [top1]: tensor([0.]) [top5]: tensor([0.])\n",
      "Val[ 134 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 135 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 136 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 137 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 138 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 139 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 140 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 141 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 142 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 143 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 144 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 145 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 146 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 147 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 148 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 149 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "Val[ 150 ]  [top1]: tensor([100.]) [top5]: tensor([100.])\n",
      "FX graph mode quantized model Evaluation accuracy on test dataset: 92.67, 98.00\n"
     ]
    }
   ],
   "source": [
    "top1, top2 = evaluate(quantized_model, criterion, data_loader_test,'cpu')#必须用cpu，用cuda会卡死\n",
    "print(\"FX graph mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top2.avg))\n",
    "torch.save(quantized_model.state_dict(), \"Export/PtqVit_\"+str(top1.avg)+\".pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看量化了个啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "print(quantized_model.patch_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "print(quantized_model.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看卷积的权重是个啥东东\n",
    "卷积被量化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "print(quantized_model.patch_embed.proj.weight)\n",
    "print(quantized_model.patch_embed.proj)\n",
    "print(quantized_model.patch_embed.proj.weight().dtype)\n",
    "# print(quantized_model.patch_embed.proj.weight())\n",
    "print(\"=================================================\")\n",
    "print(quantized_model.patch_embed.proj.weight().dequantize())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看Linear是个什么东东\n",
    "事实证明linear没有被量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model.Encoders.layers[0].linear1.weight.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出Onnx看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def export_onnx(x:torch.tensor=None,model:torch.nn=None,export_path:str='exported_onnx.onnx'):\n",
    "\n",
    "    if export_path is None:\n",
    "        export_path='exported_onnx.onnx'\n",
    "    torch.onnx.export(model,               # model being run\n",
    "                x,                         # model input \n",
    "                export_path,   # where to save the model (can be a file or file-like object)                  \n",
    "                opset_version=11,           # the ONNX version to export the model to                  \n",
    "                input_names = ['input'],   # the model's input names\n",
    "                output_names = ['output']  # the model's output names\n",
    "                )\n",
    "export_onnx(torch.rand(1,3,224,224),quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "#阶段二\n",
    "from torch.fx import symbolic_trace\n",
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(quantized_model)\n",
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "278897701cc6d4f1b9746803770f5e818cccf5e556f859a8255d5456d962739d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
