怪不得精度一直提不上去，原来是没有加class token和位置编码  
  
  
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 82, in forward
    Embedding_Out=self.Embedding_Layer(x)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
(YoloV5) PS E:\Transformer\Transformer_Main> cd e:/Transformer/Transformer_Main
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

e:\Transformer\Transformer_Main\Mytransformer.py:124: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  return Image.BICUBIC
C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torchvision\transforms\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
train loss:  1 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
torch.Size([1, 196, 768])
Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 203, in train
    output = model(data)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 87, in forward
    print(Encoder_Out.shape)
KeyboardInterrupt
(YoloV5) PS E:\Transformer\Transformer_Main> cd e:/Transformer/Transformer_Main
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

e:\Transformer\Transformer_Main\Mytransformer.py:124: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  return Image.BICUBIC
C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torchvision\transforms\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.7539torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0024torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0009torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0002torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0003torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0001torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0001torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0001torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0001torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
train loss:  0 %[->.................................................]0.0000torch.Size([1, 3, 224, 224])
Out shale: torch.Size([1, 196, 768])
Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 297, in <module>
    train(100,train_loader, model,criterion, device)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 203, in train
    output = model(data)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 86, in forward
    Encoder_Out=self.Encoders(Embedding_Out)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\transformer.py", line 280, in forward
    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\transformer.py", line 535, in forward
    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt
(YoloV5) PS E:\Transformer\Transformer_Main> cd e:/Transformer/Transformer_Main
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

e:\Transformer\Transformer_Main\Mytransformer.py:124: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  return Image.BICUBIC
  warnings.warn(
train loss: 10 %[*****->............................................]0.0000Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 297, in <module>
    train(100,train_loader, model,criterion, device)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 207, in train
    running_loss += loss.item()
KeyboardInterrupt
(YoloV5) PS E:\Transformer\Transformer_Main> cd e:/Transformer/Transformer_Main
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

e:\Transformer\Transformer_Main\Mytransformer.py:124: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  return Image.BICUBIC
C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torchvision\transforms\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
train loss: 100%[**************************************************->]0.0000Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 347, in <module>
    train(100,train_loader,val_loader, model,criterion, device)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 263, in train   
    validate(4,val_loader,model,criterion,device)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 202, in validate
    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))#output输出1000个值
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 173, in accuracy
    _, pred = output.topk(maxk, 1, True, True)
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 319, in <module>
    model=Vit_Transformer(In_Channels,Out_Channels,Picture_Size,Num_Class,Num_Heads,Encoder_Layers)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 70, in __init__
    self.Encoders= nn.TransformerEncoder(self.Single_Encoder,num_layers=self.Encoder_layers,norm=None)#构建多个连在一起的Encoder
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\transformer.py", line 189, in __init__
    self.layers = _get_clones(encoder_layer, num_layers)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\transformer.py", line 682, in _get_clones
    return ModuleList([copy.deepcopy(module) for i in range(N)])
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\modules\transformer.py", line 682, in <listcomp>
    return ModuleList([copy.deepcopy(module) for i in range(N)])
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 296, in _reconstruct
    value = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 296, in _reconstruct
    value = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 296, in _reconstruct
    value = deepcopy(value, memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\copy.py", line 153, in deepcopy
    y = copier(memo)
  File "C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torch\nn\parameter.py", line 55, in __deepcopy__
    result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
KeyboardInterrupt
(YoloV5) PS E:\Transformer\Transformer_Main> cd e:/Transformer/Transformer_Main
(YoloV5) PS E:\Transformer\Transformer_Main> & C:/Users/25073/.conda/envs/YoloV5/python.exe e:/Transformer/Transformer_Main/Mytransformer.py
fuck====================================

e:\Transformer\Transformer_Main\Mytransformer.py:124: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  return Image.BICUBIC
C:\Users\25073\.conda\envs\YoloV5\lib\site-packages\torchvision\transforms\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
train loss: 100%[**************************************************->]0.0000Test: [0/75]        Time 2.501 (2.501)      Loss 40.9416 (40.9416)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.049 (0.540)      Loss 41.1240 (40.8267)  Prec@1 0.000 (0.000)    Prec@5 100.000 (90.000)
Test: [8/75]    Time 0.047 (0.321)      Loss 40.8862 (40.9339)  Prec@1 0.000 (0.000)    Prec@5 100.000 (94.444)
Test: [12/75]   Time 0.047 (0.237)      Loss 41.0202 (40.9268)  Prec@1 0.000 (0.000)    Prec@5 100.000 (96.154)
Test: [16/75]   Time 0.047 (0.192)      Loss 41.0379 (40.9368)  Prec@1 0.000 (0.000)    Prec@5 100.000 (97.059)
Test: [20/75]   Time 0.047 (0.165)      Loss 40.0887 (40.9209)  Prec@1 0.000 (0.000)    Prec@5 50.000 (95.238)
Test: [24/75]   Time 0.047 (0.146)      Loss 40.9415 (40.9387)  Prec@1 0.000 (0.000)    Prec@5 100.000 (96.000)
Test: [28/75]   Time 0.047 (0.132)      Loss 42.8040 (41.7687)  Prec@1 0.000 (0.000)    Prec@5 50.000 (84.483)
Test: [32/75]   Time 0.047 (0.122)      Loss 41.6028 (42.0319)  Prec@1 0.000 (0.000)    Prec@5 0.000 (75.758)
Test: [36/75]   Time 0.047 (0.114)      Loss 39.5442 (42.2525)  Prec@1 0.000 (0.000)    Prec@5 100.000 (71.622)
Test: [40/75]   Time 0.047 (0.107)      Loss 45.3491 (42.6121)  Prec@1 0.000 (0.000)    Prec@5 0.000 (64.634)
Test: [44/75]   Time 0.047 (0.102)      Loss 47.1106 (42.7027)  Prec@1 0.000 (0.000)    Prec@5 0.000 (62.222)
Test: [48/75]   Time 0.047 (0.097)      Loss 48.3148 (43.0089)  Prec@1 0.000 (0.000)    Prec@5 0.000 (57.143)
Test: [52/75]   Time 0.047 (0.094)      Loss 0.0000 (40.6637)   Prec@1 100.000 (5.660)  Prec@5 100.000 (58.491)
Test: [56/75]   Time 0.047 (0.090)      Loss 0.0000 (37.8101)   Prec@1 100.000 (12.281) Prec@5 100.000 (61.404)
Test: [60/75]   Time 0.047 (0.087)      Loss 0.0000 (35.3307)   Prec@1 100.000 (18.033) Prec@5 100.000 (63.934)
Test: [64/75]   Time 0.047 (0.085)      Loss 0.0000 (33.1565)   Prec@1 100.000 (23.077) Prec@5 100.000 (66.154)
Test: [68/75]   Time 0.046 (0.083)      Loss 0.0000 (31.2344)   Prec@1 100.000 (27.536) Prec@5 100.000 (68.116)
Test: [72/75]   Time 0.046 (0.081)      Loss 0.0000 (29.5229)   Prec@1 100.000 (31.507) Prec@5 100.000 (69.863)
 * Prec@1 33.333 Prec@5 70.667 Time 6.281
train loss: 100%[**************************************************->]0.0000Test: [0/75]        Time 2.302 (2.302)      Loss 31.8066 (31.8066)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.046 (0.500)      Loss 19.7419 (21.2465)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.299)      Loss 19.0018 (20.9861)  Prec@1 0.000 (0.000)    Prec@5 100.000 (94.444)
Test: [12/75]   Time 0.046 (0.221)      Loss 24.4182 (19.9537)  Prec@1 0.000 (0.000)    Prec@5 50.000 (92.308)
Test: [16/75]   Time 0.047 (0.180)      Loss 17.4759 (18.8681)  Prec@1 0.000 (0.000)    Prec@5 100.000 (94.118)
Test: [20/75]   Time 0.047 (0.155)      Loss 25.0761 (18.5012)  Prec@1 0.000 (0.000)    Prec@5 100.000 (95.238)
Test: [24/75]   Time 0.047 (0.138)      Loss 19.1718 (18.3319)  Prec@1 0.000 (0.000)    Prec@5 100.000 (96.000)
Test: [28/75]   Time 0.047 (0.125)      Loss 27.8677 (19.7585)  Prec@1 50.000 (3.448)   Prec@5 50.000 (86.207)
Test: [32/75]   Time 0.047 (0.116)      Loss 31.9035 (22.0575)  Prec@1 0.000 (3.030)    Prec@5 50.000 (77.273)
Test: [36/75]   Time 0.047 (0.108)      Loss 41.1887 (23.4690)  Prec@1 0.000 (4.054)    Prec@5 0.000 (70.270)
Test: [40/75]   Time 0.047 (0.102)      Loss 33.9997 (24.2198)  Prec@1 0.000 (3.659)    Prec@5 50.000 (65.854)
Test: [44/75]   Time 0.046 (0.097)      Loss 45.2218 (25.8257)  Prec@1 0.000 (3.333)    Prec@5 0.000 (60.000)
Test: [48/75]   Time 0.047 (0.093)      Loss 25.8747 (26.7931)  Prec@1 0.000 (3.061)    Prec@5 0.000 (55.102)
Test: [52/75]   Time 0.046 (0.090)      Loss 0.0053 (25.5149)   Prec@1 100.000 (8.491)  Prec@5 100.000 (56.604)
Test: [56/75]   Time 0.047 (0.087)      Loss 0.0233 (23.7278)   Prec@1 100.000 (14.912) Prec@5 100.000 (59.649)
Test: [60/75]   Time 0.047 (0.084)      Loss 0.0112 (22.1722)   Prec@1 100.000 (20.492) Prec@5 100.000 (62.295)
Test: [64/75]   Time 0.046 (0.082)      Loss 0.0055 (20.8080)   Prec@1 100.000 (25.385) Prec@5 100.000 (64.615)
Test: [68/75]   Time 0.046 (0.080)      Loss 0.0000 (19.6018)   Prec@1 100.000 (29.710) Prec@5 100.000 (66.667)
Test: [72/75]   Time 0.047 (0.078)      Loss 0.0028 (18.5278)   Prec@1 100.000 (33.562) Prec@5 100.000 (68.493)
 * Prec@1 35.333 Prec@5 69.333 Time 6.080
train loss: 100%[**************************************************->]2.3370Test: [0/75]        Time 2.284 (2.284)      Loss 18.7742 (18.7742)  Prec@1 50.000 (50.000)  Prec@5 50.000 (50.000)
Test: [4/75]    Time 0.052 (0.498)      Loss 16.5887 (25.0366)  Prec@1 50.000 (30.000)  Prec@5 50.000 (30.000)
Test: [8/75]    Time 0.047 (0.298)      Loss 20.6693 (22.6368)  Prec@1 50.000 (38.889)  Prec@5 50.000 (38.889)
Test: [12/75]   Time 0.046 (0.221)      Loss 19.7077 (24.6061)  Prec@1 50.000 (34.615)  Prec@5 50.000 (34.615)
Test: [16/75]   Time 0.047 (0.180)      Loss 21.9507 (25.0414)  Prec@1 0.000 (29.412)   Prec@5 50.000 (35.294)
Test: [20/75]   Time 0.046 (0.154)      Loss 20.6961 (26.1121)  Prec@1 50.000 (26.190)  Prec@5 50.000 (30.952)
Test: [24/75]   Time 0.047 (0.137)      Loss 19.5955 (25.9141)  Prec@1 50.000 (26.000)  Prec@5 50.000 (32.000)
Test: [28/75]   Time 0.047 (0.125)      Loss 10.4805 (22.7026)  Prec@1 0.000 (32.759)   Prec@5 0.000 (37.931)
Test: [32/75]   Time 0.046 (0.115)      Loss 7.0976 (20.7381)   Prec@1 0.000 (33.333)   Prec@5 100.000 (42.424)
Test: [36/75]   Time 0.047 (0.108)      Loss 11.3695 (19.2096)  Prec@1 0.000 (35.135)   Prec@5 0.000 (43.243)
Test: [40/75]   Time 0.046 (0.102)      Loss 5.4042 (17.5787)   Prec@1 0.000 (37.805)   Prec@5 50.000 (46.341)
Test: [44/75]   Time 0.047 (0.097)      Loss 6.9712 (16.7961)   Prec@1 50.000 (36.667)  Prec@5 50.000 (45.556)
Test: [48/75]   Time 0.046 (0.093)      Loss 0.0001 (15.9421)   Prec@1 100.000 (37.755) Prec@5 100.000 (45.918)
Test: [52/75]   Time 0.047 (0.090)      Loss 4.7486 (14.9994)   Prec@1 0.000 (36.792)   Prec@5 100.000 (50.000)
Test: [56/75]   Time 0.047 (0.087)      Loss 4.4480 (14.1872)   Prec@1 0.000 (35.088)   Prec@5 100.000 (53.509)
Test: [60/75]   Time 0.047 (0.084)      Loss 6.8478 (13.5872)   Prec@1 0.000 (32.787)   Prec@5 100.000 (56.557)
Test: [64/75]   Time 0.047 (0.082)      Loss 2.8190 (12.9781)   Prec@1 50.000 (32.308)  Prec@5 100.000 (59.231)
Test: [68/75]   Time 0.047 (0.080)      Loss 6.5052 (12.4315)   Prec@1 0.000 (31.884)   Prec@5 100.000 (61.594)
Test: [72/75]   Time 0.046 (0.078)      Loss 5.6426 (12.0352)   Prec@1 0.000 (30.137)   Prec@5 100.000 (63.699)
 * Prec@1 29.333 Prec@5 64.667 Time 6.066
train loss: 100%[**************************************************->]3.0276Test: [0/75]        Time 2.297 (2.297)      Loss 38.0801 (38.0801)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.051 (0.501)      Loss 34.4889 (39.0792)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.299)      Loss 39.1808 (38.9338)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.222)      Loss 36.9499 (39.5793)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.046 (0.180)      Loss 40.9168 (39.8663)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.155)      Loss 38.6136 (40.0617)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.046 (0.138)      Loss 50.4010 (40.6715)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.046 (0.125)      Loss 0.0000 (35.1189)   Prec@1 100.000 (10.345) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.046 (0.116)      Loss 0.0000 (30.9498)   Prec@1 100.000 (18.182) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.047 (0.108)      Loss 0.0000 (27.6465)   Prec@1 100.000 (24.324) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.047 (0.102)      Loss 0.0007 (24.9586)   Prec@1 100.000 (31.707) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.046 (0.097)      Loss 0.0852 (22.7420)   Prec@1 100.000 (37.778) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.093)      Loss 0.0004 (20.9000)   Prec@1 100.000 (41.837) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.046 (0.090)      Loss 0.1257 (19.3807)   Prec@1 100.000 (45.283) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.087)      Loss 8.1856 (18.3469)   Prec@1 50.000 (45.614)  Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.084)      Loss 8.0011 (17.2764)   Prec@1 50.000 (48.361)  Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.082)      Loss 8.4212 (16.5878)   Prec@1 50.000 (49.231)  Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.080)      Loss 8.6322 (15.9692)   Prec@1 50.000 (50.000)  Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.078)      Loss 0.4025 (15.4176)   Prec@1 50.000 (49.315)  Prec@5 100.000 (65.753)
 * Prec@1 50.000 Prec@5 66.667 Time 6.081
train loss: 100%[**************************************************->]20.6665Test: [0/75]       Time 2.282 (2.282)      Loss 4.6648 (4.6648)    Prec@1 50.000 (50.000)  Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.496)      Loss 1.3038 (4.6272)    Prec@1 50.000 (20.000)  Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.046 (0.296)      Loss 4.0443 (4.2978)    Prec@1 0.000 (27.778)   Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.220)      Loss 2.9850 (4.9004)    Prec@1 50.000 (26.923)  Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.179)      Loss 7.0903 (5.0999)    Prec@1 50.000 (26.471)  Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.154)      Loss 3.3329 (4.6364)    Prec@1 0.000 (23.810)   Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.137)      Loss 1.3004 (4.5104)    Prec@1 0.000 (20.000)   Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.124)      Loss 0.6444 (3.9116)    Prec@1 100.000 (31.034) Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.115)      Loss 2.1541 (3.5507)    Prec@1 50.000 (34.848)  Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.108)      Loss 0.7900 (3.2315)    Prec@1 50.000 (39.189)  Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.102)      Loss 0.6120 (2.9607)    Prec@1 50.000 (41.463)  Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.046 (0.097)      Loss 0.2378 (2.7647)    Prec@1 100.000 (41.111) Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.047 (0.093)      Loss 0.0000 (2.5632)    Prec@1 100.000 (42.857) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.047 (0.089)      Loss 21.0576 (3.6710)   Prec@1 0.000 (41.509)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.086)      Loss 21.4798 (5.0380)   Prec@1 0.000 (38.596)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.047 (0.084)      Loss 19.3314 (6.2586)   Prec@1 0.000 (36.066)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.047 (0.081)      Loss 22.8202 (7.2423)   Prec@1 0.000 (33.846)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.046 (0.079)      Loss 19.3442 (8.0797)   Prec@1 0.000 (31.884)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.078)      Loss 26.3400 (8.9402)   Prec@1 0.000 (30.137)   Prec@5 0.000 (68.493)
 * Prec@1 29.333 Prec@5 66.667 Time 6.044
train loss: 100%[**************************************************->]1.2198Test: [0/75]        Time 2.351 (2.351)      Loss 15.3230 (15.3230)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.047 (0.509)      Loss 11.9610 (13.3327)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.304)      Loss 13.5522 (13.5537)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.046 (0.225)      Loss 16.1280 (13.6575)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.183)      Loss 15.6273 (13.5330)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.046 (0.157)      Loss 15.7441 (13.3502)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.139)      Loss 14.5034 (13.3430)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.127)      Loss 8.1696 (11.8646)   Prec@1 0.000 (8.621)    Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.117)      Loss 6.4600 (10.9884)   Prec@1 0.000 (10.606)   Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.047 (0.109)      Loss 7.1818 (10.4202)   Prec@1 0.000 (9.459)    Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.047 (0.103)      Loss 7.6041 (9.9102)    Prec@1 0.000 (10.976)   Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.098)      Loss 4.3763 (9.6492)    Prec@1 50.000 (11.111)  Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.094)      Loss 4.6634 (9.3087)    Prec@1 0.000 (11.224)   Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.091)      Loss 0.3457 (8.7556)    Prec@1 100.000 (14.151) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.048 (0.087)      Loss 0.0162 (8.1666)    Prec@1 100.000 (19.298) Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.085)      Loss 0.0001 (7.7038)    Prec@1 100.000 (22.131) Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 2.3673 (7.2740)    Prec@1 50.000 (26.154)  Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.080)      Loss 0.0099 (6.8842)    Prec@1 100.000 (28.986) Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.048 (0.079)      Loss 1.9280 (6.5654)    Prec@1 50.000 (30.822)  Prec@5 100.000 (65.753)
 * Prec@1 31.333 Prec@5 66.667 Time 6.145
train loss: 100%[**************************************************->]8.9176Test: [0/75]        Time 2.337 (2.337)      Loss 18.3444 (18.3444)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.047 (0.508)      Loss 18.6959 (17.4487)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.046 (0.303)      Loss 17.9098 (17.9622)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.224)      Loss 18.5742 (18.0600)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.183)      Loss 18.0197 (17.9532)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.157)      Loss 19.7570 (17.8136)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.139)      Loss 18.4629 (17.8385)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.126)      Loss 0.0000 (15.3780)   Prec@1 100.000 (13.793) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.117)      Loss 0.0000 (13.5140)   Prec@1 100.000 (24.242) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.046 (0.109)      Loss 0.0000 (12.0530)   Prec@1 100.000 (32.432) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.046 (0.103)      Loss 0.0000 (10.8771)   Prec@1 100.000 (39.024) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.098)      Loss 0.0000 (9.9103)    Prec@1 100.000 (44.444) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.046 (0.094)      Loss 0.0000 (9.1013)    Prec@1 100.000 (48.980) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.090)      Loss 6.7867 (8.8565)    Prec@1 0.000 (47.170)   Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.087)      Loss 12.2588 (9.0151)   Prec@1 0.000 (44.737)   Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.085)      Loss 9.5820 (9.0032)    Prec@1 0.000 (41.803)   Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.082)      Loss 13.3147 (9.0204)   Prec@1 0.000 (39.231)   Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.080)      Loss 13.2869 (9.1306)   Prec@1 0.000 (36.957)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.078)      Loss 9.5010 (9.2105)    Prec@1 0.000 (34.932)   Prec@5 100.000 (65.753)
 * Prec@1 34.000 Prec@5 66.667 Time 6.132
train loss: 100%[**************************************************->]5.8806Test: [0/75]        Time 2.393 (2.393)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.049 (0.520)      Loss 0.0000 (0.0360)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.046 (0.310)      Loss 0.0019 (0.0202)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.229)      Loss 0.0001 (0.0140)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.046 (0.186)      Loss 0.0140 (0.0178)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.160)      Loss 0.0002 (0.0144)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.141)      Loss 0.0000 (0.0121)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.128)      Loss 11.3185 (2.0389)   Prec@1 0.000 (86.207)   Prec@5 50.000 (91.379)
Test: [32/75]   Time 0.046 (0.119)      Loss 6.7017 (3.3435)    Prec@1 0.000 (75.758)   Prec@5 100.000 (84.848)
Test: [36/75]   Time 0.046 (0.111)      Loss 11.5920 (4.2040)   Prec@1 0.000 (67.568)   Prec@5 50.000 (82.432)
Test: [40/75]   Time 0.047 (0.105)      Loss 7.4971 (4.6671)    Prec@1 0.000 (60.976)   Prec@5 100.000 (82.927)
Test: [44/75]   Time 0.047 (0.099)      Loss 16.8026 (5.3069)   Prec@1 0.000 (55.556)   Prec@5 0.000 (80.000)
Test: [48/75]   Time 0.046 (0.095)      Loss 7.0912 (5.8791)    Prec@1 0.000 (51.020)   Prec@5 100.000 (77.551)
Test: [52/75]   Time 0.047 (0.091)      Loss 3.7439 (5.8470)    Prec@1 0.000 (47.170)   Prec@5 100.000 (79.245)
Test: [56/75]   Time 0.047 (0.088)      Loss 4.6570 (5.9522)    Prec@1 0.000 (43.860)   Prec@5 100.000 (80.702)
Test: [60/75]   Time 0.047 (0.086)      Loss 4.9255 (5.8948)    Prec@1 0.000 (40.984)   Prec@5 100.000 (81.967)
Test: [64/75]   Time 0.047 (0.083)      Loss 7.8575 (5.8817)    Prec@1 0.000 (38.462)   Prec@5 50.000 (82.308)
Test: [68/75]   Time 0.047 (0.081)      Loss 9.2344 (6.0437)    Prec@1 0.000 (36.232)   Prec@5 50.000 (81.159)
Test: [72/75]   Time 0.046 (0.079)      Loss 2.2403 (5.9909)    Prec@1 0.000 (34.247)   Prec@5 100.000 (82.192)
 * Prec@1 33.333 Prec@5 82.667 Time 6.184
train loss: 100%[**************************************************->]0.8463Test: [0/75]        Time 2.357 (2.357)      Loss 12.3298 (12.3298)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.052 (0.513)      Loss 11.2227 (12.1336)  Prec@1 0.000 (0.000)    Prec@5 0.000 (10.000)
Test: [8/75]    Time 0.047 (0.306)      Loss 14.1749 (12.5616)  Prec@1 0.000 (0.000)    Prec@5 50.000 (16.667)
Test: [12/75]   Time 0.047 (0.226)      Loss 13.4767 (12.9085)  Prec@1 0.000 (0.000)    Prec@5 0.000 (11.538)
Test: [16/75]   Time 0.047 (0.184)      Loss 16.6357 (13.3382)  Prec@1 0.000 (0.000)    Prec@5 50.000 (14.706)
Test: [20/75]   Time 0.047 (0.158)      Loss 14.4112 (13.2603)  Prec@1 0.000 (0.000)    Prec@5 0.000 (11.905)
Test: [24/75]   Time 0.047 (0.140)      Loss 13.1798 (13.1308)  Prec@1 0.000 (0.000)    Prec@5 0.000 (10.000)
Test: [28/75]   Time 0.047 (0.127)      Loss 3.9128 (12.4324)   Prec@1 50.000 (5.172)   Prec@5 100.000 (20.690)
Test: [32/75]   Time 0.047 (0.118)      Loss 0.0000 (11.5838)   Prec@1 100.000 (10.606) Prec@5 100.000 (28.788)
Test: [36/75]   Time 0.046 (0.110)      Loss 5.2238 (10.4798)   Prec@1 50.000 (18.919)  Prec@5 100.000 (36.486)
Test: [40/75]   Time 0.047 (0.104)      Loss 0.0000 (9.5379)    Prec@1 100.000 (25.610) Prec@5 100.000 (42.683)
Test: [44/75]   Time 0.047 (0.099)      Loss 11.6880 (9.2200)   Prec@1 0.000 (26.667)   Prec@5 100.000 (47.778)
Test: [48/75]   Time 0.047 (0.095)      Loss 0.0000 (9.1102)    Prec@1 100.000 (28.571) Prec@5 100.000 (51.020)
Test: [52/75]   Time 0.047 (0.091)      Loss 0.0000 (8.4226)    Prec@1 100.000 (33.962) Prec@5 100.000 (54.717)
Test: [56/75]   Time 0.046 (0.088)      Loss 0.0000 (7.8331)    Prec@1 100.000 (38.596) Prec@5 100.000 (57.895)
Test: [60/75]   Time 0.047 (0.085)      Loss 0.0000 (7.3199)    Prec@1 100.000 (42.623) Prec@5 100.000 (60.656)
Test: [64/75]   Time 0.047 (0.083)      Loss 0.8948 (6.8832)    Prec@1 50.000 (45.385)  Prec@5 100.000 (63.077)
Test: [68/75]   Time 0.047 (0.081)      Loss 5.9086 (6.6061)    Prec@1 0.000 (46.377)   Prec@5 100.000 (65.217)
Test: [72/75]   Time 0.046 (0.079)      Loss 0.0010 (6.2443)    Prec@1 100.000 (49.315) Prec@5 100.000 (67.123)
 * Prec@1 50.667 Prec@5 68.000 Time 6.160
train loss: 100%[**************************************************->]3.3144Test: [0/75]        Time 2.461 (2.461)      Loss 12.6527 (12.6527)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.533)      Loss 9.7999 (11.3688)   Prec@1 0.000 (0.000)    Prec@5 50.000 (70.000)
Test: [8/75]    Time 0.047 (0.317)      Loss 9.9342 (11.3063)   Prec@1 0.000 (0.000)    Prec@5 50.000 (66.667)
Test: [12/75]   Time 0.046 (0.234)      Loss 11.6440 (11.7804)  Prec@1 0.000 (0.000)    Prec@5 50.000 (65.385)
Test: [16/75]   Time 0.047 (0.190)      Loss 13.5389 (12.0654)  Prec@1 0.000 (0.000)    Prec@5 100.000 (67.647)
Test: [20/75]   Time 0.047 (0.163)      Loss 15.0557 (12.0987)  Prec@1 0.000 (0.000)    Prec@5 100.000 (69.048)
Test: [24/75]   Time 0.047 (0.144)      Loss 10.6262 (11.7934)  Prec@1 0.000 (0.000)    Prec@5 50.000 (64.000)
Test: [28/75]   Time 0.047 (0.131)      Loss 4.3895 (10.9709)   Prec@1 50.000 (5.172)   Prec@5 100.000 (65.517)
Test: [32/75]   Time 0.046 (0.121)      Loss 0.0000 (9.9750)    Prec@1 100.000 (13.636) Prec@5 100.000 (68.182)
Test: [36/75]   Time 0.047 (0.113)      Loss 6.9437 (9.0843)    Prec@1 50.000 (21.622)  Prec@5 50.000 (70.270)
Test: [40/75]   Time 0.047 (0.106)      Loss 0.0000 (8.2032)    Prec@1 100.000 (29.268) Prec@5 100.000 (73.171)
Test: [44/75]   Time 0.046 (0.101)      Loss 5.2515 (7.6133)    Prec@1 50.000 (33.333)  Prec@5 100.000 (75.556)
Test: [48/75]   Time 0.046 (0.096)      Loss 0.0000 (7.2333)    Prec@1 100.000 (36.735) Prec@5 100.000 (76.531)
Test: [52/75]   Time 0.047 (0.093)      Loss 0.0001 (6.8800)    Prec@1 100.000 (38.679) Prec@5 100.000 (77.358)
Test: [56/75]   Time 0.048 (0.090)      Loss 0.0032 (6.5339)    Prec@1 100.000 (42.105) Prec@5 100.000 (78.070)
Test: [60/75]   Time 0.046 (0.087)      Loss 0.1094 (6.3186)    Prec@1 100.000 (44.262) Prec@5 100.000 (78.689)
Test: [64/75]   Time 0.047 (0.084)      Loss 4.9383 (6.0057)    Prec@1 50.000 (46.923)  Prec@5 50.000 (79.231)
Test: [68/75]   Time 0.047 (0.082)      Loss 17.5151 (6.1718)   Prec@1 0.000 (45.652)   Prec@5 0.000 (78.261)
Test: [72/75]   Time 0.047 (0.080)      Loss 3.8247 (5.9054)    Prec@1 50.000 (47.260)  Prec@5 100.000 (79.452)
 * Prec@1 48.667 Prec@5 80.000 Time 6.270
train loss: 100%[**************************************************->]7.2852Test: [0/75]        Time 2.407 (2.407)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.522)      Loss 0.0001 (0.3276)    Prec@1 100.000 (90.000) Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.311)      Loss 0.0028 (0.1827)    Prec@1 100.000 (94.444) Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.230)      Loss 0.1122 (0.1441)    Prec@1 100.000 (96.154) Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.187)      Loss 0.0713 (0.1199)    Prec@1 100.000 (97.059) Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.046 (0.160)      Loss 0.0006 (0.0975)    Prec@1 100.000 (97.619) Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.046 (0.142)      Loss 0.0369 (0.0852)    Prec@1 100.000 (98.000) Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.129)      Loss 13.2535 (2.0494)   Prec@1 0.000 (84.483)   Prec@5 50.000 (89.655)
Test: [32/75]   Time 0.048 (0.119)      Loss 6.9510 (3.2055)    Prec@1 0.000 (74.242)   Prec@5 100.000 (83.333)
Test: [36/75]   Time 0.047 (0.111)      Loss 11.2950 (3.7453)   Prec@1 0.000 (66.216)   Prec@5 50.000 (83.784)
Test: [40/75]   Time 0.047 (0.105)      Loss 6.4037 (4.0930)    Prec@1 0.000 (59.756)   Prec@5 100.000 (84.146)
Test: [44/75]   Time 0.046 (0.100)      Loss 14.9341 (4.7129)   Prec@1 0.000 (54.444)   Prec@5 0.000 (80.000)
Test: [48/75]   Time 0.047 (0.095)      Loss 4.5409 (5.0511)    Prec@1 0.000 (50.000)   Prec@5 100.000 (78.571)
Test: [52/75]   Time 0.047 (0.092)      Loss 2.5992 (5.1984)    Prec@1 50.000 (48.113)  Prec@5 100.000 (79.245)
Test: [56/75]   Time 0.047 (0.089)      Loss 7.8515 (5.2088)    Prec@1 0.000 (48.246)   Prec@5 100.000 (78.947)
Test: [60/75]   Time 0.047 (0.086)      Loss 4.3102 (5.1939)    Prec@1 50.000 (48.361)  Prec@5 100.000 (78.689)
Test: [64/75]   Time 0.047 (0.084)      Loss 9.1820 (5.0206)    Prec@1 0.000 (50.000)   Prec@5 50.000 (79.231)
Test: [68/75]   Time 0.047 (0.081)      Loss 15.5496 (5.2848)   Prec@1 0.000 (48.551)   Prec@5 0.000 (77.536)
Test: [72/75]   Time 0.047 (0.080)      Loss 4.8569 (5.3946)    Prec@1 0.000 (45.890)   Prec@5 100.000 (77.397)
 * Prec@1 46.000 Prec@5 78.000 Time 6.254
train loss: 100%[**************************************************->]7.9860Test: [0/75]        Time 2.347 (2.347)      Loss 7.4373 (7.4373)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.046 (0.510)      Loss 7.0780 (7.0457)    Prec@1 0.000 (0.000)    Prec@5 100.000 (80.000)
Test: [8/75]    Time 0.047 (0.304)      Loss 6.5738 (7.1605)    Prec@1 0.000 (0.000)    Prec@5 50.000 (77.778)
Test: [12/75]   Time 0.046 (0.225)      Loss 6.6757 (7.2765)    Prec@1 0.000 (0.000)    Prec@5 50.000 (76.923)
Test: [16/75]   Time 0.046 (0.183)      Loss 6.3911 (7.2881)    Prec@1 0.000 (0.000)    Prec@5 50.000 (76.471)
Test: [20/75]   Time 0.047 (0.157)      Loss 8.9628 (7.3014)    Prec@1 0.000 (0.000)    Prec@5 100.000 (78.571)
Test: [24/75]   Time 0.046 (0.139)      Loss 7.8214 (7.1224)    Prec@1 0.000 (0.000)    Prec@5 50.000 (78.000)
Test: [28/75]   Time 0.047 (0.127)      Loss 1.2563 (6.3595)    Prec@1 50.000 (8.621)   Prec@5 100.000 (81.034)
Test: [32/75]   Time 0.047 (0.117)      Loss 0.0005 (5.6899)    Prec@1 100.000 (16.667) Prec@5 100.000 (83.333)
Test: [36/75]   Time 0.047 (0.109)      Loss 0.7838 (5.0960)    Prec@1 50.000 (24.324)  Prec@5 100.000 (85.135)
Test: [40/75]   Time 0.047 (0.103)      Loss 0.0002 (4.5990)    Prec@1 100.000 (31.707) Prec@5 100.000 (86.585)
Test: [44/75]   Time 0.046 (0.098)      Loss 0.9099 (4.2155)    Prec@1 50.000 (36.667)  Prec@5 100.000 (87.778)
Test: [48/75]   Time 0.047 (0.094)      Loss 0.0000 (3.8943)    Prec@1 100.000 (40.816) Prec@5 100.000 (88.776)
Test: [52/75]   Time 0.047 (0.090)      Loss 0.0102 (3.8516)    Prec@1 100.000 (42.453) Prec@5 100.000 (86.792)
Test: [56/75]   Time 0.047 (0.087)      Loss 4.5775 (3.8832)    Prec@1 50.000 (43.860)  Prec@5 50.000 (85.088)
Test: [60/75]   Time 0.046 (0.085)      Loss 2.5397 (3.9042)    Prec@1 50.000 (44.262)  Prec@5 50.000 (83.607)
Test: [64/75]   Time 0.047 (0.082)      Loss 6.3961 (3.7629)    Prec@1 0.000 (46.154)   Prec@5 50.000 (83.846)
Test: [68/75]   Time 0.046 (0.080)      Loss 14.5804 (3.9527)   Prec@1 0.000 (44.928)   Prec@5 0.000 (81.884)
Test: [72/75]   Time 0.047 (0.079)      Loss 3.7911 (4.0456)    Prec@1 50.000 (44.521)  Prec@5 50.000 (80.137)
 * Prec@1 45.333 Prec@5 80.000 Time 6.128
train loss: 100%[**************************************************->]0.0158Test: [0/75]        Time 2.339 (2.339)      Loss 9.5069 (9.5069)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.047 (0.507)      Loss 8.3511 (9.7679)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.303)      Loss 10.1243 (9.3848)   Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.224)      Loss 10.0481 (9.3951)   Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.182)      Loss 10.3688 (9.2377)   Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.156)      Loss 6.6865 (9.1540)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.139)      Loss 10.8369 (9.3781)   Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.046 (0.126)      Loss 8.6191 (9.3492)    Prec@1 0.000 (1.724)    Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.046 (0.117)      Loss 4.7345 (8.9697)    Prec@1 0.000 (1.515)    Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.046 (0.109)      Loss 6.3820 (8.4148)    Prec@1 0.000 (2.703)    Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.046 (0.103)      Loss 0.6993 (7.9685)    Prec@1 50.000 (4.878)   Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.046 (0.098)      Loss 6.6923 (7.6962)    Prec@1 0.000 (7.778)    Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.046 (0.094)      Loss 0.6654 (7.4503)    Prec@1 50.000 (9.184)   Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.090)      Loss 0.0000 (6.9445)    Prec@1 100.000 (14.151) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.087)      Loss 0.0006 (6.4626)    Prec@1 100.000 (20.175) Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.046 (0.085)      Loss 0.0000 (6.0532)    Prec@1 100.000 (24.590) Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.046 (0.082)      Loss 0.0029 (5.6808)    Prec@1 100.000 (29.231) Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.046 (0.080)      Loss 0.4217 (5.3576)    Prec@1 50.000 (32.609)  Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.078)      Loss 0.0011 (5.0651)    Prec@1 100.000 (36.301) Prec@5 100.000 (65.753)
 * Prec@1 38.000 Prec@5 66.667 Time 6.114
train loss: 100%[**************************************************->]15.5259Test: [0/75]       Time 2.431 (2.431)      Loss 0.5284 (0.5284)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.052 (0.528)      Loss 0.8924 (0.5923)    Prec@1 50.000 (80.000)  Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.314)      Loss 0.2533 (0.6740)    Prec@1 100.000 (61.111) Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.232)      Loss 0.8204 (0.7017)    Prec@1 50.000 (57.692)  Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.189)      Loss 0.6149 (0.6765)    Prec@1 50.000 (58.824)  Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.162)      Loss 1.4189 (0.7225)    Prec@1 0.000 (52.381)   Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.143)      Loss 0.3816 (0.6538)    Prec@1 100.000 (60.000) Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.130)      Loss 1.0871 (0.7237)    Prec@1 0.000 (55.172)   Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.120)      Loss 1.0616 (0.7446)    Prec@1 50.000 (53.030)  Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.112)      Loss 0.5663 (0.7490)    Prec@1 50.000 (52.703)  Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.106)      Loss 0.2889 (0.7416)    Prec@1 100.000 (53.659) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.047 (0.100)      Loss 1.2171 (0.7655)    Prec@1 50.000 (53.333)  Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.048 (0.096)      Loss 0.3279 (0.7410)    Prec@1 100.000 (55.102) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.047 (0.092)      Loss 12.5595 (1.5088)   Prec@1 0.000 (51.887)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.046 (0.089)      Loss 14.4123 (2.3941)   Prec@1 0.000 (48.246)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.047 (0.086)      Loss 14.3104 (3.2255)   Prec@1 0.000 (45.082)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.047 (0.084)      Loss 15.4237 (3.8371)   Prec@1 0.000 (42.308)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.047 (0.082)      Loss 17.5803 (4.4824)   Prec@1 0.000 (39.855)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.080)      Loss 13.2723 (5.0450)   Prec@1 0.000 (37.671)   Prec@5 0.000 (68.493)
 * Prec@1 36.667 Prec@5 66.667 Time 6.230
train loss: 100%[**************************************************->]10.9549Test: [0/75]       Time 2.482 (2.482)      Loss 2.5951 (2.5951)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.537)      Loss 3.2333 (2.4881)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.046 (0.319)      Loss 1.9935 (2.6741)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.235)      Loss 2.8632 (2.7561)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.191)      Loss 3.2281 (2.7841)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.046 (0.163)      Loss 3.6125 (2.8816)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.145)      Loss 4.8282 (2.8759)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.131)      Loss 0.0651 (2.4968)    Prec@1 100.000 (13.793) Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.121)      Loss 0.1074 (2.2017)    Prec@1 100.000 (24.242) Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.113)      Loss 0.0296 (1.9704)    Prec@1 100.000 (32.432) Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.107)      Loss 0.0256 (1.7829)    Prec@1 100.000 (39.024) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.047 (0.101)      Loss 0.0564 (1.6321)    Prec@1 100.000 (44.444) Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.047 (0.097)      Loss 0.0367 (1.5017)    Prec@1 100.000 (48.980) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.047 (0.093)      Loss 6.7524 (1.8239)    Prec@1 0.000 (47.170)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.090)      Loss 8.4932 (2.2794)    Prec@1 0.000 (43.860)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.047 (0.087)      Loss 8.8761 (2.7153)    Prec@1 0.000 (40.984)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.047 (0.085)      Loss 8.6811 (3.0411)    Prec@1 0.000 (38.462)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.046 (0.082)      Loss 10.3565 (3.3569)   Prec@1 0.000 (36.232)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.080)      Loss 6.7649 (3.6531)    Prec@1 0.000 (34.247)   Prec@5 0.000 (68.493)
 * Prec@1 33.333 Prec@5 66.667 Time 6.283
train loss: 100%[**************************************************->]5.6478Test: [0/75]        Time 2.389 (2.389)      Loss 10.1159 (10.1159)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.046 (0.518)      Loss 10.6865 (10.2263)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.309)      Loss 10.0018 (10.2838)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.228)      Loss 10.2250 (10.3105)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.048 (0.186)      Loss 10.9256 (10.3486)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.159)      Loss 10.8344 (10.3870)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.046 (0.141)      Loss 11.1369 (10.3836)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.128)      Loss 0.0286 (8.9573)    Prec@1 100.000 (13.793) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.118)      Loss 0.0405 (7.8753)    Prec@1 100.000 (24.242) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.048 (0.111)      Loss 0.0198 (7.0260)    Prec@1 100.000 (32.432) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.047 (0.104)      Loss 0.0027 (6.3416)    Prec@1 100.000 (39.024) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.099)      Loss 0.0353 (5.7799)    Prec@1 100.000 (44.444) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.046 (0.095)      Loss 0.0051 (5.3093)    Prec@1 100.000 (48.980) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.091)      Loss 2.8666 (5.0853)    Prec@1 0.000 (47.170)   Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.088)      Loss 3.6393 (4.9797)    Prec@1 0.000 (43.860)   Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.086)      Loss 3.8303 (4.9164)    Prec@1 0.000 (40.984)   Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 3.7408 (4.7949)    Prec@1 0.000 (39.231)   Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.081)      Loss 4.7984 (4.7145)    Prec@1 0.000 (36.957)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.046 (0.079)      Loss 2.5406 (4.6951)    Prec@1 0.000 (34.932)   Prec@5 100.000 (65.753)
 * Prec@1 34.667 Prec@5 66.667 Time 6.197
train loss: 100%[**************************************************->]8.9907Test: [0/75]        Time 2.422 (2.422)      Loss 0.5815 (0.5815)    Prec@1 50.000 (50.000)  Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.524)      Loss 0.7593 (0.6237)    Prec@1 50.000 (60.000)  Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.312)      Loss 1.0181 (0.7004)    Prec@1 0.000 (50.000)   Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.230)      Loss 0.6914 (0.7153)    Prec@1 50.000 (50.000)  Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.187)      Loss 0.8662 (0.7135)    Prec@1 0.000 (44.118)   Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.046 (0.160)      Loss 0.8526 (0.7069)    Prec@1 0.000 (47.619)   Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.142)      Loss 0.9507 (0.6976)    Prec@1 0.000 (48.000)   Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.129)      Loss 0.7292 (0.7150)    Prec@1 0.000 (44.828)   Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.119)      Loss 0.9601 (0.7291)    Prec@1 50.000 (43.939)  Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.111)      Loss 0.4493 (0.7193)    Prec@1 100.000 (45.946) Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.105)      Loss 0.4099 (0.7044)    Prec@1 100.000 (48.780) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.046 (0.100)      Loss 0.7957 (0.6966)    Prec@1 50.000 (50.000)  Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.046 (0.096)      Loss 0.5979 (0.6860)    Prec@1 50.000 (51.020)  Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.047 (0.092)      Loss 7.0355 (1.0903)    Prec@1 0.000 (48.113)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.089)      Loss 8.0995 (1.5837)    Prec@1 0.000 (44.737)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.047 (0.086)      Loss 8.6143 (2.0502)    Prec@1 0.000 (41.803)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.046 (0.084)      Loss 8.7879 (2.3854)    Prec@1 0.000 (39.231)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.046 (0.081)      Loss 9.5114 (2.7381)    Prec@1 0.000 (36.957)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.080)      Loss 8.0034 (3.0693)    Prec@1 0.000 (34.932)   Prec@5 0.000 (68.493)
 * Prec@1 34.000 Prec@5 66.667 Time 6.213
train loss: 100%[**************************************************->]2.9068Test: [0/75]        Time 2.480 (2.480)      Loss 8.5350 (8.5350)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.051 (0.537)      Loss 8.9521 (8.6951)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.320)      Loss 8.8403 (8.7848)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.236)      Loss 8.8179 (8.8406)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.191)      Loss 8.8830 (8.8469)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.046 (0.164)      Loss 9.0509 (8.8302)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.145)      Loss 9.0420 (8.8263)    Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.132)      Loss 0.1086 (7.6231)    Prec@1 100.000 (13.793) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.121)      Loss 0.0783 (6.7196)    Prec@1 100.000 (24.242) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.047 (0.113)      Loss 0.1154 (5.9994)    Prec@1 100.000 (32.432) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.047 (0.107)      Loss 0.0078 (5.4206)    Prec@1 100.000 (39.024) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.101)      Loss 0.0767 (4.9463)    Prec@1 100.000 (44.444) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.097)      Loss 0.0132 (4.5501)    Prec@1 100.000 (48.980) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.093)      Loss 1.0771 (4.3015)    Prec@1 50.000 (48.113)  Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.046 (0.090)      Loss 1.5783 (4.1340)    Prec@1 0.000 (46.491)   Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.046 (0.087)      Loss 1.4577 (4.0061)    Prec@1 0.000 (45.082)   Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.085)      Loss 2.8776 (3.8331)    Prec@1 0.000 (45.385)   Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.046 (0.082)      Loss 3.8502 (3.7456)    Prec@1 0.000 (43.478)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.081)      Loss 1.8998 (3.6720)    Prec@1 0.000 (41.096)   Prec@5 100.000 (65.753)
 * Prec@1 42.000 Prec@5 66.667 Time 6.288
train loss: 100%[**************************************************->]18.0449Test: [0/75]       Time 2.426 (2.426)      Loss 5.6509 (5.6509)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.048 (0.526)      Loss 5.6369 (5.5645)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.313)      Loss 5.4072 (5.7919)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.231)      Loss 6.1571 (5.9228)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.188)      Loss 5.5571 (5.9243)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.161)      Loss 6.4484 (5.9240)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.143)      Loss 5.6223 (5.9394)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.129)      Loss 0.0048 (5.1208)    Prec@1 100.000 (13.793) Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.046 (0.119)      Loss 0.0041 (4.5010)    Prec@1 100.000 (24.242) Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.047 (0.112)      Loss 0.0087 (4.0148)    Prec@1 100.000 (32.432) Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.105)      Loss 0.0007 (3.6235)    Prec@1 100.000 (39.024) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.047 (0.100)      Loss 0.0050 (3.3019)    Prec@1 100.000 (44.444) Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.047 (0.096)      Loss 0.0011 (3.0329)    Prec@1 100.000 (48.980) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.046 (0.092)      Loss 15.8473 (3.7503)   Prec@1 0.000 (47.170)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.089)      Loss 16.3103 (4.6532)   Prec@1 0.000 (43.860)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.047 (0.086)      Loss 15.5360 (5.4817)   Prec@1 0.000 (40.984)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.046 (0.084)      Loss 19.2900 (6.0624)   Prec@1 0.000 (38.462)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.047 (0.082)      Loss 20.4119 (6.7235)   Prec@1 0.000 (36.232)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.080)      Loss 18.1712 (7.3305)   Prec@1 0.000 (34.247)   Prec@5 0.000 (68.493)
 * Prec@1 33.333 Prec@5 66.667 Time 6.219
train loss: 100%[**************************************************->]3.6917Test: [0/75]        Time 2.435 (2.435)      Loss 0.4183 (0.4183)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.528)      Loss 0.6951 (0.5234)    Prec@1 50.000 (70.000)  Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.314)      Loss 0.5939 (0.6430)    Prec@1 100.000 (61.111) Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.232)      Loss 0.8600 (0.6783)    Prec@1 50.000 (57.692)  Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.188)      Loss 0.7294 (0.6976)    Prec@1 50.000 (58.824)  Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.161)      Loss 0.9069 (0.6999)    Prec@1 50.000 (59.524)  Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.046 (0.143)      Loss 0.8742 (0.7229)    Prec@1 50.000 (56.000)  Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.130)      Loss 1.6761 (0.8350)    Prec@1 0.000 (53.448)   Prec@5 50.000 (96.552)
Test: [32/75]   Time 0.046 (0.120)      Loss 0.6676 (0.9451)    Prec@1 50.000 (51.515)  Prec@5 100.000 (93.939)
Test: [36/75]   Time 0.047 (0.112)      Loss 2.8252 (1.0336)    Prec@1 50.000 (51.351)  Prec@5 50.000 (90.541)
Test: [40/75]   Time 0.046 (0.105)      Loss 0.2218 (1.0228)    Prec@1 100.000 (52.439) Prec@5 100.000 (90.244)
Test: [44/75]   Time 0.046 (0.100)      Loss 1.6716 (1.1410)    Prec@1 0.000 (48.889)   Prec@5 100.000 (87.778)
Test: [48/75]   Time 0.046 (0.096)      Loss 0.4807 (1.2012)    Prec@1 100.000 (48.980) Prec@5 100.000 (85.714)
Test: [52/75]   Time 0.046 (0.092)      Loss 2.6142 (1.2783)    Prec@1 0.000 (46.226)   Prec@5 50.000 (83.019)
Test: [56/75]   Time 0.046 (0.089)      Loss 1.2863 (1.3639)    Prec@1 0.000 (44.737)   Prec@5 100.000 (81.579)
Test: [60/75]   Time 0.047 (0.086)      Loss 1.6672 (1.4653)    Prec@1 50.000 (44.262)  Prec@5 50.000 (79.508)
Test: [64/75]   Time 0.047 (0.084)      Loss 4.2988 (1.4686)    Prec@1 0.000 (44.615)   Prec@5 0.000 (79.231)
Test: [68/75]   Time 0.046 (0.082)      Loss 5.4278 (1.5509)    Prec@1 0.000 (42.754)   Prec@5 0.000 (78.261)
Test: [72/75]   Time 0.046 (0.080)      Loss 3.5568 (1.6506)    Prec@1 0.000 (41.781)   Prec@5 0.000 (76.027)
 * Prec@1 42.000 Prec@5 76.000 Time 6.228
train loss: 100%[**************************************************->]10.9945Test: [0/75]       Time 2.407 (2.407)      Loss 31.8385 (31.8385)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.051 (0.523)      Loss 31.4479 (30.9528)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.311)      Loss 30.0323 (31.2371)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.230)      Loss 33.9936 (31.8168)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.187)      Loss 30.8065 (31.8113)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.046 (0.160)      Loss 33.9848 (31.9074)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.142)      Loss 30.1455 (31.9653)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.129)      Loss 0.0000 (27.5563)   Prec@1 100.000 (13.793) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.119)      Loss 0.0000 (24.2161)   Prec@1 100.000 (24.242) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.046 (0.111)      Loss 0.0037 (21.5983)   Prec@1 100.000 (32.432) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.046 (0.105)      Loss 0.0000 (19.4911)   Prec@1 100.000 (39.024) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.100)      Loss 0.0001 (17.7611)   Prec@1 100.000 (44.444) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.095)      Loss 0.0000 (16.3113)   Prec@1 100.000 (48.980) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.092)      Loss 12.2831 (15.7408)  Prec@1 0.000 (47.170)   Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.089)      Loss 6.6062 (15.3127)   Prec@1 0.000 (44.737)   Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.086)      Loss 7.3835 (15.0147)   Prec@1 50.000 (42.623)  Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 16.2748 (14.5621)  Prec@1 0.000 (40.769)   Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.048 (0.081)      Loss 18.7266 (14.3750)  Prec@1 0.000 (38.406)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.046 (0.079)      Loss 16.4539 (14.3034)  Prec@1 0.000 (36.301)   Prec@5 100.000 (65.753)
 * Prec@1 36.667 Prec@5 66.667 Time 6.207
train loss: 100%[**************************************************->]2.4122Test: [0/75]        Time 2.391 (2.391)      Loss 0.9520 (0.9520)    Prec@1 50.000 (50.000)  Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.052 (0.520)      Loss 1.8845 (1.6139)    Prec@1 0.000 (20.000)   Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.046 (0.310)      Loss 2.4236 (1.8458)    Prec@1 0.000 (22.222)   Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.229)      Loss 2.7393 (1.7858)    Prec@1 0.000 (19.231)   Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.046 (0.186)      Loss 2.9344 (1.9690)    Prec@1 0.000 (14.706)   Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.160)      Loss 2.6571 (1.9638)    Prec@1 0.000 (14.286)   Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.046 (0.142)      Loss 3.7363 (1.9897)    Prec@1 0.000 (16.000)   Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.128)      Loss 3.3217 (2.0542)    Prec@1 0.000 (18.966)   Prec@5 0.000 (94.828)
Test: [32/75]   Time 0.047 (0.119)      Loss 0.2151 (2.0611)    Prec@1 100.000 (24.242) Prec@5 100.000 (90.909)
Test: [36/75]   Time 0.046 (0.111)      Loss 6.4240 (2.1295)    Prec@1 50.000 (28.378)  Prec@5 50.000 (89.189)
Test: [40/75]   Time 0.047 (0.105)      Loss 0.1240 (1.9778)    Prec@1 100.000 (32.927) Prec@5 100.000 (87.805)
Test: [44/75]   Time 0.046 (0.099)      Loss 3.1415 (2.2001)    Prec@1 0.000 (33.333)   Prec@5 50.000 (84.444)
Test: [48/75]   Time 0.047 (0.095)      Loss 0.1306 (2.2845)    Prec@1 100.000 (33.673) Prec@5 100.000 (82.653)
Test: [52/75]   Time 0.047 (0.092)      Loss 0.5313 (2.2037)    Prec@1 50.000 (34.906)  Prec@5 100.000 (82.075)
Test: [56/75]   Time 0.047 (0.088)      Loss 0.0176 (2.2076)    Prec@1 100.000 (36.842) Prec@5 100.000 (81.579)
Test: [60/75]   Time 0.046 (0.086)      Loss 0.7941 (2.1984)    Prec@1 50.000 (37.705)  Prec@5 100.000 (80.328)
Test: [64/75]   Time 0.046 (0.083)      Loss 3.0441 (2.1180)    Prec@1 0.000 (39.231)   Prec@5 50.000 (80.769)
Test: [68/75]   Time 0.046 (0.081)      Loss 5.3703 (2.1381)    Prec@1 0.000 (40.580)   Prec@5 0.000 (79.710)
Test: [72/75]   Time 0.047 (0.079)      Loss 3.2055 (2.1891)    Prec@1 0.000 (40.411)   Prec@5 50.000 (78.082)
 * Prec@1 41.333 Prec@5 78.000 Time 6.193
train loss: 100%[**************************************************->]35.7671Test: [0/75]       Time 2.631 (2.631)      Loss 14.8033 (14.8033)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.046 (0.565)      Loss 12.8768 (12.3222)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.335)      Loss 9.9742 (12.2886)   Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.246)      Loss 16.1093 (13.0046)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.199)      Loss 10.5019 (12.9253)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.170)      Loss 15.9697 (12.9954)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.151)      Loss 10.9632 (13.0750)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.136)      Loss 0.0000 (11.2716)   Prec@1 100.000 (13.793) Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.125)      Loss 0.0000 (9.9053)    Prec@1 100.000 (24.242) Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.117)      Loss 0.0003 (8.8345)    Prec@1 100.000 (32.432) Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.110)      Loss 0.0000 (7.9726)    Prec@1 100.000 (39.024) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.047 (0.104)      Loss 0.0001 (7.2645)    Prec@1 100.000 (44.444) Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.047 (0.100)      Loss 0.0000 (6.6715)    Prec@1 100.000 (48.980) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.046 (0.096)      Loss 41.2542 (8.4699)   Prec@1 0.000 (47.170)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.092)      Loss 36.8567 (10.6831)  Prec@1 0.000 (43.860)   Prec@5 0.000 (87.719)
Test: [60/75]   Time 0.048 (0.089)      Loss 34.3464 (12.5841)  Prec@1 0.000 (40.984)   Prec@5 0.000 (81.967)
Test: [64/75]   Time 0.048 (0.087)      Loss 48.2883 (13.9518)  Prec@1 0.000 (38.462)   Prec@5 0.000 (76.923)
Test: [68/75]   Time 0.046 (0.084)      Loss 49.9841 (15.3988)  Prec@1 0.000 (36.232)   Prec@5 0.000 (72.464)
Test: [72/75]   Time 0.047 (0.082)      Loss 47.5255 (16.9435)  Prec@1 0.000 (34.247)   Prec@5 0.000 (68.493)
 * Prec@1 33.333 Prec@5 66.667 Time 6.475
train loss: 100%[**************************************************->]6.2740Test: [0/75]        Time 2.445 (2.445)      Loss 0.0001 (0.0001)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.529)      Loss 0.0003 (0.0003)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.315)      Loss 0.0091 (0.0014)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.232)      Loss 0.0004 (0.0010)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.189)      Loss 0.0252 (0.0026)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.162)      Loss 0.0003 (0.0022)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.046 (0.143)      Loss 0.0069 (0.0021)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.130)      Loss 11.9838 (1.4978)   Prec@1 0.000 (86.207)   Prec@5 50.000 (94.828)
Test: [32/75]   Time 0.047 (0.120)      Loss 9.1349 (2.7076)    Prec@1 0.000 (75.758)   Prec@5 100.000 (90.909)
Test: [36/75]   Time 0.047 (0.112)      Loss 12.5562 (3.6217)   Prec@1 0.000 (67.568)   Prec@5 50.000 (89.189)
Test: [40/75]   Time 0.047 (0.106)      Loss 7.9791 (4.2200)    Prec@1 0.000 (60.976)   Prec@5 100.000 (87.805)
Test: [44/75]   Time 0.046 (0.100)      Loss 12.6003 (5.0935)   Prec@1 0.000 (55.556)   Prec@5 50.000 (84.444)
Test: [48/75]   Time 0.047 (0.096)      Loss 9.9257 (5.7569)    Prec@1 0.000 (51.020)   Prec@5 100.000 (82.653)
Test: [52/75]   Time 0.047 (0.092)      Loss 9.5927 (6.0788)    Prec@1 0.000 (47.170)   Prec@5 100.000 (83.019)
Test: [56/75]   Time 0.047 (0.089)      Loss 6.1663 (6.2874)    Prec@1 0.000 (44.737)   Prec@5 100.000 (82.456)
Test: [60/75]   Time 0.047 (0.086)      Loss 5.8852 (6.4332)    Prec@1 0.000 (42.623)   Prec@5 100.000 (81.148)
Test: [64/75]   Time 0.047 (0.084)      Loss 12.4744 (6.4107)   Prec@1 0.000 (40.769)   Prec@5 50.000 (81.538)
Test: [68/75]   Time 0.047 (0.082)      Loss 15.7037 (6.5552)   Prec@1 0.000 (39.130)   Prec@5 0.000 (80.435)
Test: [72/75]   Time 0.048 (0.080)      Loss 13.8332 (6.8266)   Prec@1 0.000 (37.671)   Prec@5 50.000 (78.767)
 * Prec@1 38.000 Prec@5 78.000 Time 6.295
train loss: 100%[**************************************************->]10.6708Test: [0/75]       Time 2.419 (2.419)      Loss 34.1592 (34.1592)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.047 (0.522)      Loss 32.5670 (32.2159)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.311)      Loss 32.4864 (32.0810)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.046 (0.230)      Loss 36.0961 (32.7051)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.187)      Loss 32.7539 (32.8126)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.160)      Loss 35.6265 (32.8464)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.142)      Loss 31.1159 (32.9306)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.046 (0.129)      Loss 0.0000 (28.3886)   Prec@1 100.000 (13.793) Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.047 (0.119)      Loss 0.0000 (24.9490)   Prec@1 100.000 (24.242) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.047 (0.111)      Loss 0.2801 (22.2697)   Prec@1 100.000 (31.081) Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.048 (0.105)      Loss 0.0000 (20.0970)   Prec@1 100.000 (37.805) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.100)      Loss 0.0076 (18.4345)   Prec@1 100.000 (40.000) Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.096)      Loss 0.0000 (17.0127)   Prec@1 100.000 (42.857) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.092)      Loss 0.0003 (16.0913)   Prec@1 100.000 (44.340) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.048 (0.089)      Loss 2.8411 (15.5549)   Prec@1 50.000 (44.737)  Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.086)      Loss 5.6715 (15.1320)   Prec@1 50.000 (44.262)  Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.048 (0.084)      Loss 12.9845 (14.5130)  Prec@1 0.000 (44.615)   Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.048 (0.082)      Loss 19.3671 (14.1582)  Prec@1 0.000 (44.928)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.048 (0.080)      Loss 16.2845 (14.1222)  Prec@1 0.000 (43.836)   Prec@5 100.000 (65.753)
 * Prec@1 44.000 Prec@5 66.667 Time 6.268
train loss: 100%[**************************************************->]0.1251Test: [0/75]        Time 2.384 (2.384)      Loss 22.3689 (22.3689)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.049 (0.516)      Loss 26.7309 (25.7809)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.046 (0.308)      Loss 29.6477 (26.2147)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.046 (0.227)      Loss 22.7219 (25.2300)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.047 (0.185)      Loss 29.9723 (25.4016)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.047 (0.159)      Loss 22.3165 (25.3129)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.047 (0.141)      Loss 29.2440 (25.1687)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.128)      Loss 4.8209 (22.3015)   Prec@1 0.000 (6.897)    Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.046 (0.118)      Loss 0.5785 (20.3324)   Prec@1 50.000 (9.091)   Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.046 (0.110)      Loss 8.6432 (18.7445)   Prec@1 50.000 (13.514)  Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.046 (0.104)      Loss 0.4885 (17.2734)   Prec@1 50.000 (17.073)  Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.099)      Loss 8.7650 (16.7609)   Prec@1 50.000 (17.778)  Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.046 (0.095)      Loss 0.4308 (16.1293)   Prec@1 50.000 (18.367)  Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.091)      Loss 0.0000 (15.0299)   Prec@1 100.000 (22.642) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.046 (0.088)      Loss 0.0000 (14.0582)   Prec@1 100.000 (26.316) Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.046 (0.085)      Loss 0.0001 (13.1567)   Prec@1 100.000 (30.328) Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 0.0013 (12.3471)   Prec@1 100.000 (34.615) Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.081)      Loss 0.2548 (11.6499)   Prec@1 100.000 (37.681) Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.079)      Loss 0.0093 (11.0558)   Prec@1 100.000 (39.726) Prec@5 100.000 (65.753)
 * Prec@1 41.333 Prec@5 66.667 Time 6.173
train loss: 100%[**************************************************->]13.0345Test: [0/75]       Time 2.401 (2.401)      Loss 11.7723 (11.7723)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.052 (0.522)      Loss 10.1639 (10.2972)  Prec@1 0.000 (0.000)    Prec@5 100.000 (90.000)
Test: [8/75]    Time 0.046 (0.311)      Loss 9.5064 (9.9903)    Prec@1 0.000 (0.000)    Prec@5 50.000 (77.778)
Test: [12/75]   Time 0.047 (0.230)      Loss 12.3732 (10.3913)  Prec@1 0.000 (0.000)    Prec@5 100.000 (84.615)
Test: [16/75]   Time 0.046 (0.187)      Loss 9.1162 (10.3113)   Prec@1 0.000 (0.000)    Prec@5 50.000 (82.353)
Test: [20/75]   Time 0.047 (0.160)      Loss 11.6408 (10.2511)  Prec@1 0.000 (0.000)    Prec@5 100.000 (83.333)
Test: [24/75]   Time 0.047 (0.142)      Loss 8.6504 (10.1978)   Prec@1 0.000 (0.000)    Prec@5 50.000 (84.000)
Test: [28/75]   Time 0.047 (0.129)      Loss 0.0001 (8.7913)    Prec@1 100.000 (13.793) Prec@5 100.000 (86.207)
Test: [32/75]   Time 0.047 (0.119)      Loss 0.0000 (7.7257)    Prec@1 100.000 (24.242) Prec@5 100.000 (87.879)
Test: [36/75]   Time 0.047 (0.111)      Loss 0.0007 (6.8914)    Prec@1 100.000 (32.432) Prec@5 100.000 (89.189)
Test: [40/75]   Time 0.046 (0.105)      Loss 0.0000 (6.2295)    Prec@1 100.000 (37.805) Prec@5 100.000 (90.244)
Test: [44/75]   Time 0.047 (0.100)      Loss 0.0011 (5.6916)    Prec@1 100.000 (42.222) Prec@5 100.000 (91.111)
Test: [48/75]   Time 0.046 (0.095)      Loss 0.0000 (5.2328)    Prec@1 100.000 (46.939) Prec@5 100.000 (91.837)
Test: [52/75]   Time 0.046 (0.092)      Loss 0.3859 (5.1679)    Prec@1 50.000 (47.170)  Prec@5 100.000 (90.566)
Test: [56/75]   Time 0.047 (0.089)      Loss 6.3537 (5.3599)    Prec@1 0.000 (46.491)   Prec@5 50.000 (88.596)
Test: [60/75]   Time 0.047 (0.086)      Loss 6.9443 (5.7030)    Prec@1 50.000 (45.082)  Prec@5 50.000 (85.246)
Test: [64/75]   Time 0.046 (0.083)      Loss 14.0365 (5.7767)   Prec@1 0.000 (44.615)   Prec@5 0.000 (83.846)
Test: [68/75]   Time 0.047 (0.081)      Loss 18.8513 (5.9234)   Prec@1 0.000 (43.478)   Prec@5 0.000 (82.609)
Test: [72/75]   Time 0.047 (0.079)      Loss 14.8002 (6.3169)   Prec@1 0.000 (41.781)   Prec@5 0.000 (79.452)
 * Prec@1 42.000 Prec@5 78.667 Time 6.214
train loss: 100%[**************************************************->]14.8027Test: [0/75]       Time 2.357 (2.357)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.051 (0.513)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.306)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.226)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.184)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.158)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.046 (0.140)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.127)      Loss 14.4600 (2.0178)   Prec@1 0.000 (86.207)   Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.047 (0.117)      Loss 14.8036 (3.6122)   Prec@1 0.000 (75.758)   Prec@5 100.000 (96.970)
Test: [36/75]   Time 0.046 (0.110)      Loss 15.0739 (4.8069)   Prec@1 0.000 (67.568)   Prec@5 100.000 (95.946)
Test: [40/75]   Time 0.047 (0.104)      Loss 14.2539 (5.7422)   Prec@1 0.000 (60.976)   Prec@5 100.000 (95.122)
Test: [44/75]   Time 0.047 (0.099)      Loss 14.9503 (6.5775)   Prec@1 0.000 (55.556)   Prec@5 100.000 (92.222)
Test: [48/75]   Time 0.047 (0.094)      Loss 14.1270 (7.2670)   Prec@1 0.000 (51.020)   Prec@5 100.000 (89.796)
Test: [52/75]   Time 0.046 (0.091)      Loss 11.0378 (7.8205)   Prec@1 0.000 (47.170)   Prec@5 100.000 (88.679)
Test: [56/75]   Time 0.047 (0.088)      Loss 13.7011 (8.2878)   Prec@1 0.000 (43.860)   Prec@5 50.000 (86.842)
Test: [60/75]   Time 0.047 (0.085)      Loss 12.4030 (8.7955)   Prec@1 0.000 (40.984)   Prec@5 100.000 (83.607)
Test: [64/75]   Time 0.047 (0.083)      Loss 20.0417 (9.0290)   Prec@1 0.000 (39.231)   Prec@5 0.000 (83.077)
Test: [68/75]   Time 0.047 (0.081)      Loss 22.7387 (9.3513)   Prec@1 0.000 (36.957)   Prec@5 0.000 (81.884)
Test: [72/75]   Time 0.046 (0.079)      Loss 20.5089 (9.8687)   Prec@1 0.000 (34.932)   Prec@5 0.000 (78.767)
 * Prec@1 34.000 Prec@5 78.000 Time 6.166
train loss: 100%[**************************************************->]20.1089Test: [0/75]       Time 2.465 (2.465)      Loss 17.1209 (17.1209)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.048 (0.533)      Loss 17.5967 (17.2118)  Prec@1 0.000 (0.000)    Prec@5 100.000 (90.000)
Test: [8/75]    Time 0.047 (0.317)      Loss 17.7922 (17.3144)  Prec@1 0.000 (0.000)    Prec@5 100.000 (94.444)
Test: [12/75]   Time 0.046 (0.234)      Loss 17.5894 (17.3525)  Prec@1 0.000 (0.000)    Prec@5 100.000 (96.154)
Test: [16/75]   Time 0.046 (0.190)      Loss 17.1118 (17.2996)  Prec@1 0.000 (0.000)    Prec@5 50.000 (94.118)
Test: [20/75]   Time 0.047 (0.163)      Loss 16.9384 (17.2377)  Prec@1 0.000 (0.000)    Prec@5 100.000 (95.238)
Test: [24/75]   Time 0.047 (0.144)      Loss 17.1607 (17.2224)  Prec@1 0.000 (0.000)    Prec@5 100.000 (96.000)
Test: [28/75]   Time 0.046 (0.131)      Loss 0.0000 (14.8469)   Prec@1 100.000 (13.793) Prec@5 100.000 (96.552)
Test: [32/75]   Time 0.047 (0.120)      Loss 0.0000 (13.0472)   Prec@1 100.000 (24.242) Prec@5 100.000 (96.970)
Test: [36/75]   Time 0.047 (0.112)      Loss 0.0000 (11.6367)   Prec@1 100.000 (32.432) Prec@5 100.000 (97.297)
Test: [40/75]   Time 0.047 (0.106)      Loss 0.0000 (10.5014)   Prec@1 100.000 (39.024) Prec@5 100.000 (97.561)
Test: [44/75]   Time 0.046 (0.101)      Loss 0.0000 (9.5680)    Prec@1 100.000 (44.444) Prec@5 100.000 (97.778)
Test: [48/75]   Time 0.047 (0.096)      Loss 0.0000 (8.7869)    Prec@1 100.000 (48.980) Prec@5 100.000 (97.959)
Test: [52/75]   Time 0.047 (0.093)      Loss 16.1046 (9.1771)   Prec@1 0.000 (47.170)   Prec@5 50.000 (94.340)
Test: [56/75]   Time 0.046 (0.089)      Loss 18.8031 (9.8520)   Prec@1 0.000 (43.860)   Prec@5 50.000 (90.351)
Test: [60/75]   Time 0.046 (0.087)      Loss 15.6455 (10.4864)  Prec@1 0.000 (40.984)   Prec@5 50.000 (86.066)
Test: [64/75]   Time 0.047 (0.084)      Loss 24.0800 (10.9021)  Prec@1 0.000 (38.462)   Prec@5 0.000 (83.846)
Test: [68/75]   Time 0.046 (0.082)      Loss 26.8765 (11.4135)  Prec@1 0.000 (36.232)   Prec@5 0.000 (81.159)
Test: [72/75]   Time 0.047 (0.080)      Loss 24.0107 (12.0651)  Prec@1 0.000 (34.247)   Prec@5 0.000 (78.082)
 * Prec@1 33.333 Prec@5 77.333 Time 6.248
train loss: 100%[**************************************************->]0.0003Test: [0/75]        Time 2.372 (2.372)      Loss 7.2343 (7.2343)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.046 (0.513)      Loss 10.8173 (10.6600)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.046 (0.306)      Loss 10.5940 (10.5296)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.226)      Loss 5.4178 (9.7543)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.184)      Loss 12.1992 (9.7184)   Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.158)      Loss 8.4027 (9.7689)    Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.140)      Loss 10.8871 (9.7103)   Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.127)      Loss 17.9669 (10.8344)  Prec@1 0.000 (0.000)    Prec@5 0.000 (86.207)
Test: [32/75]   Time 0.047 (0.118)      Loss 16.6221 (11.9163)  Prec@1 0.000 (0.000)    Prec@5 0.000 (75.758)
Test: [36/75]   Time 0.046 (0.110)      Loss 19.7688 (12.5831)  Prec@1 0.000 (0.000)    Prec@5 0.000 (67.568)
Test: [40/75]   Time 0.046 (0.104)      Loss 15.8728 (13.0946)  Prec@1 0.000 (0.000)    Prec@5 0.000 (60.976)
Test: [44/75]   Time 0.046 (0.099)      Loss 18.6569 (13.7356)  Prec@1 0.000 (0.000)    Prec@5 0.000 (55.556)
Test: [48/75]   Time 0.047 (0.094)      Loss 15.2932 (14.2367)  Prec@1 0.000 (0.000)    Prec@5 0.000 (51.020)
Test: [52/75]   Time 0.046 (0.091)      Loss 0.0000 (13.5226)   Prec@1 100.000 (5.660)  Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.088)      Loss 0.0000 (12.5736)   Prec@1 100.000 (12.281) Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.085)      Loss 0.0000 (11.7491)   Prec@1 100.000 (18.033) Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 0.0000 (11.0261)   Prec@1 100.000 (23.077) Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.081)      Loss 0.0002 (10.3869)   Prec@1 100.000 (27.536) Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.079)      Loss 0.0001 (9.8179)    Prec@1 100.000 (31.507) Prec@5 100.000 (65.753)
 * Prec@1 33.333 Prec@5 66.667 Time 6.171
train loss: 100%[**************************************************->]20.8937Test: [0/75]       Time 2.421 (2.421)      Loss 14.6274 (14.6274)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.523)      Loss 15.7025 (15.1754)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.311)      Loss 15.9794 (15.2717)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.047 (0.230)      Loss 14.8917 (15.2650)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.187)      Loss 14.9764 (15.1863)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.160)      Loss 14.9496 (15.1547)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.142)      Loss 15.1252 (15.1257)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.046 (0.129)      Loss 0.0000 (13.0394)   Prec@1 100.000 (13.793) Prec@5 100.000 (100.000)
Test: [32/75]   Time 0.046 (0.119)      Loss 0.0000 (11.4589)   Prec@1 100.000 (24.242) Prec@5 100.000 (100.000)
Test: [36/75]   Time 0.046 (0.111)      Loss 0.0000 (10.2201)   Prec@1 100.000 (32.432) Prec@5 100.000 (100.000)
Test: [40/75]   Time 0.047 (0.105)      Loss 0.0000 (9.2230)    Prec@1 100.000 (39.024) Prec@5 100.000 (100.000)
Test: [44/75]   Time 0.047 (0.100)      Loss 0.0000 (8.4032)    Prec@1 100.000 (44.444) Prec@5 100.000 (100.000)
Test: [48/75]   Time 0.046 (0.095)      Loss 0.0000 (7.7172)    Prec@1 100.000 (48.980) Prec@5 100.000 (100.000)
Test: [52/75]   Time 0.047 (0.092)      Loss 15.9072 (8.1889)   Prec@1 0.000 (47.170)   Prec@5 0.000 (94.340)
Test: [56/75]   Time 0.047 (0.089)      Loss 20.8287 (8.9437)   Prec@1 0.000 (43.860)   Prec@5 0.000 (89.474)
Test: [60/75]   Time 0.047 (0.086)      Loss 17.6868 (9.6718)   Prec@1 0.000 (40.984)   Prec@5 50.000 (85.246)
Test: [64/75]   Time 0.047 (0.083)      Loss 23.9051 (10.1617)  Prec@1 0.000 (38.462)   Prec@5 0.000 (80.769)
Test: [68/75]   Time 0.047 (0.081)      Loss 26.0837 (10.7457)  Prec@1 0.000 (36.232)   Prec@5 0.000 (76.812)
Test: [72/75]   Time 0.047 (0.079)      Loss 23.7261 (11.4083)  Prec@1 0.000 (34.247)   Prec@5 0.000 (72.603)
 * Prec@1 33.333 Prec@5 71.333 Time 6.195
train loss: 100%[**************************************************->]12.2778Test: [0/75]       Time 2.390 (2.390)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.047 (0.518)      Loss 0.0000 (0.0001)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [8/75]    Time 0.047 (0.308)      Loss 0.0000 (0.0001)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [12/75]   Time 0.046 (0.228)      Loss 0.0000 (0.0001)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [16/75]   Time 0.047 (0.185)      Loss 0.0001 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [20/75]   Time 0.047 (0.159)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [24/75]   Time 0.047 (0.141)      Loss 0.0000 (0.0000)    Prec@1 100.000 (100.000)        Prec@5 100.000 (100.000)
Test: [28/75]   Time 0.047 (0.128)      Loss 11.2846 (1.5818)   Prec@1 0.000 (86.207)   Prec@5 100.000 (98.276)
Test: [32/75]   Time 0.047 (0.118)      Loss 11.8019 (2.8182)   Prec@1 0.000 (75.758)   Prec@5 100.000 (98.485)
Test: [36/75]   Time 0.046 (0.111)      Loss 12.0706 (3.7491)   Prec@1 0.000 (67.568)   Prec@5 50.000 (95.946)
Test: [40/75]   Time 0.047 (0.104)      Loss 11.1928 (4.4717)   Prec@1 0.000 (60.976)   Prec@5 100.000 (95.122)
Test: [44/75]   Time 0.047 (0.099)      Loss 11.4075 (5.1002)   Prec@1 0.000 (55.556)   Prec@5 100.000 (92.222)
Test: [48/75]   Time 0.047 (0.095)      Loss 11.1565 (5.6126)   Prec@1 0.000 (51.020)   Prec@5 100.000 (89.796)
Test: [52/75]   Time 0.047 (0.091)      Loss 7.4975 (5.9845)    Prec@1 0.000 (47.170)   Prec@5 100.000 (88.679)
Test: [56/75]   Time 0.046 (0.088)      Loss 12.2641 (6.3294)   Prec@1 0.000 (43.860)   Prec@5 50.000 (86.842)
Test: [60/75]   Time 0.047 (0.085)      Loss 9.5657 (6.7254)    Prec@1 0.000 (40.984)   Prec@5 50.000 (82.787)
Test: [64/75]   Time 0.047 (0.083)      Loss 15.1283 (6.9336)   Prec@1 0.000 (39.231)   Prec@5 0.000 (80.769)
Test: [68/75]   Time 0.047 (0.081)      Loss 17.3131 (7.2358)   Prec@1 0.000 (36.957)   Prec@5 0.000 (79.710)
Test: [72/75]   Time 0.046 (0.079)      Loss 15.2657 (7.6099)   Prec@1 0.000 (34.932)   Prec@5 0.000 (77.397)
 * Prec@1 34.000 Prec@5 76.667 Time 6.188
train loss: 100%[**************************************************->]4.1048Test: [0/75]        Time 2.395 (2.395)      Loss 11.1556 (11.1556)  Prec@1 0.000 (0.000)    Prec@5 100.000 (100.000)
Test: [4/75]    Time 0.046 (0.519)      Loss 11.3133 (10.9158)  Prec@1 0.000 (0.000)    Prec@5 50.000 (60.000)
Test: [8/75]    Time 0.046 (0.309)      Loss 11.6325 (11.0183)  Prec@1 0.000 (0.000)    Prec@5 50.000 (61.111)
Test: [12/75]   Time 0.047 (0.228)      Loss 11.9253 (11.2103)  Prec@1 0.000 (0.000)    Prec@5 100.000 (65.385)
Test: [16/75]   Time 0.047 (0.186)      Loss 9.7808 (11.0847)   Prec@1 0.000 (0.000)    Prec@5 50.000 (64.706)
Test: [20/75]   Time 0.046 (0.159)      Loss 10.7839 (11.0021)  Prec@1 0.000 (0.000)    Prec@5 50.000 (61.905)
Test: [24/75]   Time 0.046 (0.141)      Loss 10.1844 (10.9919)  Prec@1 0.000 (0.000)    Prec@5 50.000 (62.000)
Test: [28/75]   Time 0.047 (0.128)      Loss 0.0000 (9.4761)    Prec@1 100.000 (13.793) Prec@5 100.000 (67.241)
Test: [32/75]   Time 0.047 (0.118)      Loss 0.0000 (8.3275)    Prec@1 100.000 (24.242) Prec@5 100.000 (71.212)
Test: [36/75]   Time 0.047 (0.111)      Loss 0.0003 (7.4306)    Prec@1 100.000 (32.432) Prec@5 100.000 (74.324)
Test: [40/75]   Time 0.046 (0.104)      Loss 0.0000 (6.7059)    Prec@1 100.000 (39.024) Prec@5 100.000 (76.829)
Test: [44/75]   Time 0.047 (0.099)      Loss 0.0001 (6.1109)    Prec@1 100.000 (44.444) Prec@5 100.000 (78.889)
Test: [48/75]   Time 0.047 (0.095)      Loss 0.0000 (5.6257)    Prec@1 100.000 (47.959) Prec@5 100.000 (80.612)
Test: [52/75]   Time 0.046 (0.091)      Loss 0.4785 (5.4064)    Prec@1 50.000 (48.113)  Prec@5 100.000 (82.075)
Test: [56/75]   Time 0.047 (0.088)      Loss 5.8962 (5.4219)    Prec@1 0.000 (47.368)   Prec@5 100.000 (81.579)
Test: [60/75]   Time 0.046 (0.086)      Loss 2.1938 (5.4777)    Prec@1 50.000 (45.902)  Prec@5 100.000 (81.148)
Test: [64/75]   Time 0.046 (0.083)      Loss 11.8050 (5.4670)   Prec@1 0.000 (45.385)   Prec@5 50.000 (80.769)
Test: [68/75]   Time 0.047 (0.081)      Loss 12.2103 (5.4973)   Prec@1 0.000 (44.203)   Prec@5 0.000 (79.710)
Test: [72/75]   Time 0.047 (0.079)      Loss 9.7197 (5.6437)    Prec@1 0.000 (42.466)   Prec@5 50.000 (78.082)
 * Prec@1 42.667 Prec@5 78.000 Time 6.190
train loss: 100%[**************************************************->]0.3432Test: [0/75]        Time 2.400 (2.400)      Loss 14.8288 (14.8288)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [4/75]    Time 0.046 (0.520)      Loss 16.3443 (16.4302)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [8/75]    Time 0.047 (0.310)      Loss 17.4306 (16.5081)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [12/75]   Time 0.047 (0.229)      Loss 15.7652 (16.2308)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [16/75]   Time 0.046 (0.186)      Loss 17.9953 (16.2581)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [20/75]   Time 0.046 (0.160)      Loss 15.0577 (16.2910)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [24/75]   Time 0.046 (0.141)      Loss 17.3845 (16.2395)  Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)
Test: [28/75]   Time 0.047 (0.128)      Loss 0.4572 (14.3078)   Prec@1 50.000 (5.172)   Prec@5 100.000 (13.793)
Test: [32/75]   Time 0.046 (0.119)      Loss 0.1509 (12.7385)   Prec@1 100.000 (10.606) Prec@5 100.000 (24.242)
Test: [36/75]   Time 0.046 (0.111)      Loss 1.9280 (11.5468)   Prec@1 50.000 (17.568)  Prec@5 100.000 (32.432)
Test: [40/75]   Time 0.047 (0.105)      Loss 0.1700 (10.5649)   Prec@1 100.000 (23.171) Prec@5 100.000 (39.024)
Test: [44/75]   Time 0.047 (0.099)      Loss 1.1529 (9.9109)    Prec@1 50.000 (25.556)  Prec@5 100.000 (44.444)
Test: [48/75]   Time 0.047 (0.095)      Loss 0.1590 (9.3904)    Prec@1 100.000 (26.531) Prec@5 100.000 (48.980)
Test: [52/75]   Time 0.047 (0.091)      Loss 0.0000 (8.7158)    Prec@1 100.000 (31.132) Prec@5 100.000 (52.830)
Test: [56/75]   Time 0.047 (0.088)      Loss 0.0197 (8.1553)    Prec@1 100.000 (34.211) Prec@5 100.000 (56.140)
Test: [60/75]   Time 0.047 (0.086)      Loss 0.0008 (7.6349)    Prec@1 100.000 (37.705) Prec@5 100.000 (59.016)
Test: [64/75]   Time 0.047 (0.083)      Loss 1.5604 (7.1909)    Prec@1 50.000 (40.769)  Prec@5 100.000 (61.538)
Test: [68/75]   Time 0.047 (0.081)      Loss 1.4889 (6.8195)    Prec@1 0.000 (42.029)   Prec@5 100.000 (63.768)
Test: [72/75]   Time 0.047 (0.079)      Loss 0.6424 (6.4872)    Prec@1 50.000 (42.466)  Prec@5 100.000 (65.753)
 * Prec@1 43.333 Prec@5 66.667 Time 6.196
train loss: 14 %[*******->..........................................]16.5653Traceback (most recent call last):
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 347, in <module>
    train(100,train_loader,val_loader, model,criterion, device)
  File "e:\Transformer\Transformer_Main\Mytransformer.py", line 255, in train
    running_loss += loss.item()
KeyboardInterrupt
(YoloV5) PS E:\Transformer\Transformer_Main> 