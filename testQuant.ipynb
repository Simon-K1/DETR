{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_B(S1, S2, S3):  # 第一组  β即N_REAL  +++++求shitf++++\n",
    "    M = (S1 * S2) / S3\n",
    "    M.squeeze()\n",
    "    M=M.cpu()#先将tensor放到cpu上\n",
    "    M = M.numpy()\n",
    "    \n",
    "    daxiao = S2.shape[0]  # 第一层权重的shape[0]是32 shape[0]表示行数 是一维大小位32的列向量\n",
    "    SCALE = np.zeros(daxiao, dtype=np.uint32, order='C')  # 相当于32个输出通道 每个对应一组shift\n",
    "    N_REAL = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    for i, ii in enumerate(M):  # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "\n",
    "        while not (ii >= 0.5 and ii <= 1.0):  # 左移到（0.5，1） 左移一次相当于*2\n",
    "            ii *= 2\n",
    "        pass\n",
    "        mmmm = ii * (2 ** 32)  # 乘2^32\n",
    "\n",
    "        SCALE[i] = mmmm.astype(np.int32)\n",
    "\n",
    "    for i, ii in enumerate(M):\n",
    "        N_REAL[i] = round(math.log(SCALE[i] / ii, 2)) - 32   # fpga加了1这里要减1,  β值也是m维 相当于存的mmmm\n",
    "        if int(N_REAL[i])<0:\n",
    "            print(\"error\\n\")\n",
    "        if N_REAL[i] !=0:\n",
    "            N_REAL[i]=N_REAL[i]-1\n",
    "\n",
    "    return N_REAL\n",
    "\n",
    "\n",
    "def gen_M_N(S1, S2, S3):  # 第二组求M'即M  ++++++返回scale和shift+++++\n",
    "    daxiao = S2.shape[0]\n",
    "    M = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    N_REAL = gen_B(S1, S2, S3)\n",
    "    M = np.zeros(S2.shape[0])\n",
    "    \n",
    "    for i, ii in enumerate(M):\n",
    "        Scale=(S1*S2[i]/S3).cpu()\n",
    "        # Scale=Scale.numpy\n",
    "        M[i] = (torch.round(Scale * (2 ** (32 + N_REAL[i] + 1)))).numpy()  # s1s2/s3 *2^(32+β+1)\n",
    "    M = M.astype(np.uint32)\n",
    "    # exit()\n",
    "    return M, N_REAL\n",
    "\n",
    "\n",
    "# r_b=s1*s2*q_b\n",
    "# r_b是量化前的bias,q_b是量化后的bias\n",
    "def gen_int_bias(s1, s2, bias_float):  # 求bias/s1s2即bb\n",
    "    aa = bias_float / s1\n",
    "    # print(bias_float)\n",
    "    # exit()\n",
    "    bb = torch.div(aa, s2)  # 对应元素做除法\n",
    "\n",
    "    # for i, m in enumerate(bb):\n",
    "    #     bb[i] = round(m.item())\n",
    "    # bias = bb.int()\n",
    "    return bb\n",
    "\n",
    "\n",
    "def gen_M(s1, s2, s3):\n",
    "    aa = s1 * s2\n",
    "    M = aa / s3\n",
    "    return M\n",
    "\n",
    "\n",
    "def new_bias(z1, q2, bias):  # 求最终的bias=bias/s1s2-q2z1\n",
    "    q2 = q2.type(torch.float64)\n",
    "    bias1 = z1 * q2\n",
    "    shape = bias1.shape\n",
    "    n_bias = np.zeros(shape[0], dtype=np.float64, order='C')\n",
    "    for m in range(shape[0]):  # bias1的维度是M C K K 将C K K 做累加 变成M维\n",
    "        n_bias[m] = bias1[m, :, :, :].sum()  # 从第一组开始一直有m组，m是输出通道数\n",
    "        # print()\n",
    "        n_bias[m] = (bias[m] - n_bias[m])  # 做减法\n",
    "    # print(n_bias) 第一层n_bias是一维32个\n",
    "    # exit()\n",
    "    daxiao = shape[0]  # 第一层是32\n",
    "    SCALE = np.zeros(daxiao, dtype=np.float64, order='C')\n",
    "    # N_REAL = np.zeros(daxiao, dtype=np.float32, order='C')\n",
    "    N_REAL = []\n",
    "    for i, ii in enumerate(n_bias):  # i和ii就是从n_bias中取值\n",
    "        index = 0\n",
    "\n",
    "        while not (abs(ii) >= (2 ** 23) and abs(ii) <= (2 ** 24)):\n",
    "            if index >= 16:  # fpga里面最多移动16位,所有成到16就停止了,这样精度也够了\n",
    "                break\n",
    "            else:\n",
    "                ii *= 2\n",
    "                index = index + 1\n",
    "\n",
    "        N_REAL.append(index)\n",
    "        SCALE[i] = round(ii)\n",
    "    out_bias = []\n",
    "    for index in range(shape[0]):\n",
    "        data_integer_old = ('{:024b}'.format(int(SCALE[index]) & 0xffffff))  # {:024b} 24位2二进制不足补0；& 0xffffff按位与\n",
    "        n = N_REAL[index]\n",
    "        symbol = '0'\n",
    "        if n_bias[index] < 0:  # 符号位\n",
    "            symbol = '1'#这里也不是多此一举，因为如果你给一个uint24，MSB=1，那么你在FPGA那边如何分辨这是正数还是负数，所以还是需要1bit的符号位来帮助fpga做判断的\n",
    "        elif n_bias[index] > 0:\n",
    "            symbol = '0'\n",
    "        data_integer = data_integer_old[8:]\n",
    "        data_decimal = '{:07b}'.format(int(n))\n",
    "        out_bias1 = symbol + str(data_decimal) + str(data_integer_old)  # 1bit+7bit+24bit\n",
    "        a = int(out_bias1, 2)  # 转成int型 out_bias1为二进制 ；a是十进制\n",
    "        out_bias.append(a)  # 一个一个写入out_bias\n",
    "    # print(out_bias)\n",
    "    # exit()\n",
    "    return out_bias\n",
    "\n",
    "\n",
    "def get_add_bias(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_add_SCALE(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_add_NREAL(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_weight(new_weight, shape, weight, inchannel):  # 八入八出操作 输入通道inchannel不固定 输出通道outchannel为8\n",
    "    j = 0\n",
    "\n",
    "    shift_num = 0\n",
    "    for index in range(inchannel):\n",
    "        if (inchannel == (1 << index)):  # index不停左移一位直到与inchannel相等 来判断移了几位   2的shift_num次方=inchannel\n",
    "            shift_num = index\n",
    "            break\n",
    "    # if shape[0] == 32 and shape[1] == 64 and shape[2] == 1:\n",
    "    #     # print(\"shape:\",new_weight.shape)\n",
    "    #     with open('weight_1x1_8i8o(2).txt', 'a')as f:\n",
    "    #         f.write('-' * 40 + '\\n')\n",
    "    #         f.write(str(shape) + '\\n')\n",
    "    for i in range(shape[2]):  # mckk  K\n",
    "        for ii in range(shape[3]):  # K\n",
    "            for kernel_times in range(shape[0] >> 3):  # >>右移三位 因为输出通道outchannel默认为8 卷积核个数\n",
    "                for channel_in_times in range(shape[1] >> shift_num):  # 右移shift_num 输入通道数\n",
    "                    for iii in range(8):  # shape[0] >> 3 右移了3次即2^3 要补8次\n",
    "                        for iiii in range(\n",
    "                                inchannel):  # shape[1] >> shift_num 右移了shift_num即2^shift_num 要补inchannel次(2^shift_num =inchannel)\n",
    "                            # print('++++++++++++++++++')\n",
    "                            weight[j] = new_weight[kernel_times * 8 + iii][channel_in_times * inchannel + iiii][i][ii]\n",
    "                            # if shape[0] == 32 and shape[1] == 64 and shape[2] == 1:\n",
    "                            # with open('weight_1x1_8i8o(2).txt', 'a')as f:\n",
    "                            #     f.write(str(kernel_times * 8 + iii) + ',')\n",
    "                            #     f.write(str(channel_in_times * inchannel + iiii) + ',')\n",
    "                            #     f.write(str(i) + ',')\n",
    "                            #     f.write(str(ii) + '\\n')\n",
    "                            #\n",
    "                            j += 1\n",
    "    # exit()\n",
    "    return weight\n",
    "\n",
    "\n",
    "def add_weight_channel(new_weig, weig, shape):  # 补0 把weig存入new_weig；new_weig是全0，\n",
    "    for kernel_num in range(shape[0]):\n",
    "        for channel_in_num in range(shape[1]):\n",
    "            for row in range(shape[2]):\n",
    "                for col in range(shape[3]):\n",
    "                    new_weig[kernel_num][channel_in_num][row][col] = weig[kernel_num][channel_in_num][row][col]\n",
    "    return new_weig\n",
    "\n",
    "\n",
    "def tensorr(x):\n",
    "    tensor_py = torch.from_numpy(np.load(x))  # 创建tensor\n",
    "    return tensor_py\n",
    "\n",
    "\n",
    "def get_weight2(new_weight, shape, weight):  # 四维权重写成一维\n",
    "    j = 0\n",
    "    for kernel_times in range(shape[0]):\n",
    "        for channel_in_times in range(shape[1]):\n",
    "            for i in range(shape[2]):\n",
    "                for ii in range(shape[3]):\n",
    "                    # print('++++++++++++++++++')\n",
    "                    weight[j] = new_weight[kernel_times][channel_in_times][i][ii]\n",
    "                    j += 1\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25073\\AppData\\Local\\Temp\\ipykernel_12988\\3006563256.py:13: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ii *= 2\n"
     ]
    }
   ],
   "source": [
    "S1=torch.rand(1)*0.499+0.001\n",
    "S2=torch.rand(768)*0.499+0.001\n",
    "S3=torch.rand(1)*0.499+0.001\n",
    "bias=torch.rand(1)*0.499+0.001\n",
    "SCALE, N_REAL = gen_M_N(S1, S2, S3)\n",
    "SCALE\n",
    "N_REAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('1010010',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bias_Tmp='1111'\n",
    "BiasAdd=int(Bias_Tmp, 2) - (1 << len(Bias_Tmp))\n",
    "BiasAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1010\n",
      "6\n",
      "-0b101\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "a=bin(10)\n",
    "print(a)\n",
    "print(len(a))\n",
    "print('-0b'+a[2:5])\n",
    "print(int('-0b'+a[2:5],2))\n",
    "# bin(-5)\n",
    "# for i in range(len(a)):\n",
    "#     print(i,\"  \",a[i])\n",
    "# print(a[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0b1111111\n",
      "1111111\n"
     ]
    }
   ],
   "source": [
    "a=bin(-127)\n",
    "print(a)\n",
    "print(a[3:].zfill(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 222, 222])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.rand(1, 8, 224, 224)\n",
    "w = torch.rand(768, 8, 3, 3)\n",
    "stride = (1, 1)\n",
    "\n",
    "output = F.conv2d(x, w, stride=stride)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=int(102)\n",
    "bb=torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5007e-01, 2.2411e-03, 2.0470e-01, 5.2534e-03, 8.0235e-01, 5.2113e-01,\n",
       "        1.6234e-01, 7.2542e-01, 1.4723e-01, 5.8487e-02, 3.2948e-04, 2.1061e-01,\n",
       "        5.0463e-02, 4.2238e-02, 3.2456e-01, 1.0675e-02, 1.2187e-01, 1.3574e-01,\n",
       "        3.3166e-01, 3.6028e-02, 2.1759e-01, 1.7268e-01, 1.4460e-01, 3.8965e-01,\n",
       "        5.8133e-01, 1.1793e-01, 2.4376e-01, 3.6589e-01, 2.2602e-02, 1.4659e-01,\n",
       "        3.5235e-01, 2.8605e-01, 5.2180e-01, 8.3473e-02, 3.1087e-01, 5.4658e-04,\n",
       "        1.1127e-01, 1.1466e-02, 4.5347e-01, 1.5913e-01, 7.9129e-02, 2.1646e-01,\n",
       "        1.9854e-02, 5.8259e-01, 2.3040e-01, 2.0833e-02, 1.2947e-01, 5.8352e-01,\n",
       "        1.1117e-01, 5.3672e-01, 2.3853e-01, 6.7361e-02, 7.0189e-01, 4.8092e-01,\n",
       "        5.0899e-01, 2.8234e-02, 1.5773e-01, 7.4047e-02, 2.4848e-01, 3.8188e-01,\n",
       "        4.2239e-01, 1.8467e-02, 5.7272e-02, 3.8750e-01, 8.0112e-02, 5.1387e-01,\n",
       "        1.8160e-01, 1.7189e-01, 5.6067e-01, 1.1905e-01, 4.3684e-01, 6.1986e-02,\n",
       "        2.3197e-01, 3.6373e-01, 6.9991e-01, 5.5023e-01, 1.8354e-01, 5.1366e-02,\n",
       "        4.3522e-01, 2.0100e-01, 9.4076e-03, 7.4761e-02, 4.8473e-02, 6.2607e-02,\n",
       "        2.1890e-01, 7.4243e-02, 1.2383e-01, 3.3540e-01, 1.6655e-01, 6.8948e-03,\n",
       "        1.3887e-01, 4.6798e-01, 1.9080e-02, 1.9759e-01, 7.2493e-02, 2.8863e-03,\n",
       "        2.5338e-01, 2.0433e-02, 6.5629e-02, 4.0576e-01, 3.3908e-02, 8.0002e-03,\n",
       "        4.1221e-02, 9.9792e-04, 5.1697e-01, 9.7696e-02, 6.1538e-01, 3.4261e-01,\n",
       "        7.1281e-03, 8.7523e-02, 1.6338e-01, 3.7008e-01, 4.8340e-01, 1.4387e-03,\n",
       "        3.7858e-01, 5.7882e-02, 9.4460e-02, 8.3003e-01, 4.4682e-03, 2.3723e-01,\n",
       "        1.0489e-01, 6.3103e-02, 2.2163e-01, 5.4237e-03, 4.1119e-01, 2.1318e-01,\n",
       "        2.5222e-01, 2.4876e-01, 1.4830e-01, 4.5673e-02, 5.0293e-01, 6.7000e-01,\n",
       "        3.1765e-01, 8.0708e-01, 7.0382e-01, 2.3736e-01, 2.8333e-01, 5.4603e-01,\n",
       "        5.8726e-01, 3.6336e-01, 1.9324e-01, 7.5286e-02, 5.1744e-01, 7.2371e-02,\n",
       "        4.2239e-01, 2.2575e-01, 1.0337e-02, 2.4391e-02, 3.2766e-01, 5.5592e-01,\n",
       "        3.9422e-01, 2.8029e-01, 3.8409e-01, 1.4974e-01, 2.4536e-01, 1.4373e-01,\n",
       "        8.0244e-02, 3.3036e-02, 1.5458e-01, 8.9706e-02, 3.6000e-02, 1.8692e-01,\n",
       "        3.5538e-01, 2.1732e-01, 3.2203e-02, 2.1853e-02, 3.1391e-01, 2.7163e-01,\n",
       "        6.7852e-01, 5.0273e-02, 1.2934e-01, 2.6566e-01, 8.7725e-02, 3.0845e-01,\n",
       "        4.5961e-01, 2.5302e-01, 3.6279e-01, 6.3868e-01, 2.3189e-01, 2.2456e-01,\n",
       "        1.3356e-02, 5.2695e-01, 1.0721e-02, 4.2980e-01, 1.3861e-01, 1.5268e-01,\n",
       "        4.4152e-01, 1.3498e-01, 2.1799e-01, 1.2478e-01, 4.6218e-01, 3.3377e-02,\n",
       "        4.2263e-01, 6.4758e-01, 1.5082e-01, 3.1977e-01, 3.8548e-01, 9.3596e-02,\n",
       "        6.0847e-02, 9.1432e-02, 4.7109e-01, 1.9994e-01, 5.5122e-01, 1.2318e-01,\n",
       "        4.0952e-01, 6.7640e-02, 1.0167e-01, 4.3676e-01, 7.4827e-01, 1.2779e-01,\n",
       "        2.7611e-01, 2.1603e-01, 8.7753e-02, 3.0940e-01, 7.0877e-01, 3.2021e-01,\n",
       "        7.2716e-02, 3.3669e-01, 3.7105e-02, 9.2683e-02, 1.5440e-02, 4.6516e-02,\n",
       "        1.6493e-02, 3.8413e-01, 3.1805e-01, 5.0204e-02, 1.1929e-02, 2.9439e-03,\n",
       "        3.0360e-01, 2.4275e-01, 3.5219e-02, 9.6975e-01, 4.0620e-01, 3.4653e-02,\n",
       "        2.1201e-01, 1.0882e-01, 6.9655e-02, 4.2946e-01, 1.3975e-01, 5.3953e-01,\n",
       "        4.8469e-01, 1.0071e-01, 8.0557e-01, 1.9706e-01, 6.5127e-01, 7.3567e-02,\n",
       "        3.3421e-01, 4.5229e-01, 2.6353e-01, 5.2640e-01, 4.3259e-01, 7.3551e-01,\n",
       "        5.1944e-02, 2.2226e-01, 2.5128e-01, 2.4532e-01, 3.2166e-01, 4.6074e-02,\n",
       "        6.9613e-02, 4.0859e-01, 1.8458e-01, 4.2507e-01, 1.7006e-02, 1.0209e-01,\n",
       "        4.7774e-02, 4.8541e-01, 5.0827e-03, 3.2151e-02, 6.1863e-01, 9.9924e-02,\n",
       "        2.3601e-01, 7.7862e-01, 7.3701e-01, 2.8332e-01, 5.0522e-03, 2.3721e-01,\n",
       "        1.8929e-02, 2.5096e-01, 2.8577e-01, 3.0846e-01, 4.2441e-02, 2.4252e-02,\n",
       "        8.4951e-03, 9.0187e-03, 1.0702e-01, 3.6129e-01, 4.5225e-02, 6.5059e-02,\n",
       "        5.0491e-01, 3.5748e-02, 2.6323e-01, 3.6121e-01, 3.0279e-01, 3.6068e-01,\n",
       "        5.8363e-01, 8.5959e-01, 1.9227e-01, 1.7227e-01, 2.8427e-01, 1.0797e-01,\n",
       "        1.5109e-02, 2.4030e-02, 6.0585e-01, 7.2622e-01, 3.4078e-01, 1.0620e-01,\n",
       "        3.0841e-02, 4.7955e-01, 4.9612e-02, 4.1827e-01, 2.2739e-01, 1.9353e-01,\n",
       "        5.8298e-01, 1.0333e-02, 3.6137e-01, 3.1267e-01, 4.9379e-01, 1.5944e-02,\n",
       "        6.1860e-01, 8.5076e-02, 3.8417e-02, 5.0191e-02, 7.4359e-01, 1.4077e-01,\n",
       "        3.7786e-01, 2.3600e-01, 3.0805e-02, 8.0940e-02, 8.3191e-02, 4.9584e-01,\n",
       "        5.3524e-03, 3.0613e-02, 1.5527e-02, 8.7075e-01, 6.9710e-01, 7.3564e-01,\n",
       "        3.9965e-01, 4.6793e-01, 3.2368e-03, 1.2750e-02, 3.3953e-01, 2.1083e-01,\n",
       "        4.2234e-01, 5.2729e-01, 2.5190e-01, 1.9034e-01, 1.7494e-01, 4.3533e-01,\n",
       "        5.2246e-02, 2.4273e-01, 2.6141e-01, 6.4721e-02, 6.7885e-01, 2.8779e-02,\n",
       "        5.7033e-02, 2.7687e-01, 1.5225e-02, 3.6720e-01, 6.1001e-01, 3.6725e-01,\n",
       "        3.4061e-01, 8.5717e-03, 1.2981e-01, 3.5723e-02, 4.1077e-02, 7.5610e-01,\n",
       "        2.4790e-01, 6.8826e-01, 1.4251e-01, 9.8246e-03, 1.1443e-02, 7.3681e-02,\n",
       "        3.3852e-01, 5.7528e-02, 2.8957e-01, 2.4300e-01, 2.6190e-01, 1.6538e-01,\n",
       "        4.7191e-01, 6.8749e-02, 8.0063e-01, 7.7518e-01, 5.9326e-01, 8.7056e-01,\n",
       "        1.2499e-01, 1.8631e-01, 1.6883e-01, 3.8072e-02, 4.8878e-02, 6.8445e-03,\n",
       "        2.9302e-01, 7.4422e-01, 3.6523e-03, 3.9178e-01, 4.2383e-01, 5.3604e-01,\n",
       "        2.7239e-01, 7.5202e-02, 2.1293e-03, 1.6652e-01, 1.4497e-01, 8.2156e-01,\n",
       "        5.7282e-01, 2.7368e-03, 3.9185e-01, 3.2980e-01, 5.2711e-01, 1.0103e-01,\n",
       "        2.4142e-01, 1.5564e-02, 3.7957e-02, 3.3730e-01, 4.6389e-01, 1.4128e-01,\n",
       "        4.1984e-01, 2.6418e-01, 3.4888e-01, 1.9997e-01, 1.9825e-02, 2.0603e-01,\n",
       "        5.7613e-02, 2.3193e-01, 1.3147e-01, 6.0833e-01, 5.6636e-01, 1.6211e-02,\n",
       "        3.0053e-01, 4.1503e-03, 3.1028e-01, 2.4836e-01, 2.6582e-01, 4.5905e-01,\n",
       "        5.3631e-01, 8.9065e-01, 5.5984e-02, 4.1479e-02, 1.4237e-01, 4.8714e-01,\n",
       "        1.4740e-02, 2.0302e-01, 5.4513e-01, 2.4188e-02, 3.0985e-02, 3.1191e-02,\n",
       "        4.8798e-02, 2.9409e-01, 1.0025e-01, 7.6411e-02, 3.2477e-01, 2.3645e-01,\n",
       "        2.9165e-02, 1.9143e-01, 5.2841e-01, 5.4719e-01, 1.0347e-02, 7.1921e-02,\n",
       "        9.6005e-03, 1.6875e-01, 1.9119e-01, 2.7887e-03, 5.2851e-01, 4.8507e-01,\n",
       "        4.2652e-03, 4.1458e-01, 4.1453e-01, 3.4866e-01, 3.1100e-01, 1.4016e-01,\n",
       "        4.8819e-02, 3.9086e-01, 3.4159e-01, 2.8522e-01, 2.1593e-01, 2.2479e-01,\n",
       "        5.8834e-02, 1.4893e-02, 9.3404e-02, 1.2778e-02, 1.8471e-01, 2.9168e-01,\n",
       "        4.2592e-01, 7.9967e-02, 7.7059e-01, 6.2330e-01, 2.1013e-01, 3.1237e-01,\n",
       "        2.8548e-01, 2.5272e-01, 5.8928e-01, 5.1880e-02, 1.0800e-01, 1.4298e-01,\n",
       "        5.1616e-01, 2.8079e-02, 2.1229e-02, 4.9958e-04, 2.0791e-01, 7.4011e-01,\n",
       "        5.3896e-01, 7.0689e-01, 2.2715e-01, 1.0016e-01, 4.4516e-02, 1.6420e-01,\n",
       "        1.8411e-02, 5.7085e-01, 3.4957e-01, 8.4479e-01, 1.8556e-02, 1.0213e-01,\n",
       "        4.6688e-01, 1.1981e-01, 4.4384e-03, 6.9155e-01, 2.5252e-01, 7.6378e-01,\n",
       "        2.9046e-01, 3.1093e-02, 3.3934e-01, 1.1031e-01, 3.7849e-01, 1.9988e-01,\n",
       "        1.3794e-01, 3.1658e-01, 3.1455e-03, 2.8389e-01, 9.3731e-02, 5.3853e-02,\n",
       "        1.4794e-01, 5.3651e-02, 8.1016e-01, 5.7130e-02, 1.4402e-01, 2.1699e-01,\n",
       "        4.8006e-01, 2.7640e-01, 2.7832e-01, 7.5302e-02, 3.6610e-01, 7.2045e-01,\n",
       "        4.9872e-03, 6.2376e-02, 4.2710e-02, 6.1474e-01, 4.2955e-01, 3.8346e-01,\n",
       "        5.1733e-01, 5.6419e-02, 1.6251e-01, 2.2456e-03, 2.6172e-01, 1.1940e-01,\n",
       "        1.5147e-01, 2.1237e-02, 3.3921e-01, 1.5730e-03, 5.7085e-01, 6.5444e-01,\n",
       "        2.5599e-01, 9.8856e-02, 4.1113e-01, 7.2011e-01, 6.7877e-04, 4.7600e-01,\n",
       "        8.4732e-02, 1.4692e-01, 3.2854e-01, 8.7200e-01, 3.0000e-01, 8.2984e-02,\n",
       "        4.2861e-01, 2.5970e-01, 1.0190e-01, 3.7619e-01, 1.6304e-01, 1.2493e-01,\n",
       "        4.1223e-01, 9.6124e-02, 1.6510e-01, 1.7298e-02, 7.1049e-02, 6.8954e-03,\n",
       "        1.9689e-01, 1.3701e-01, 2.8537e-01, 3.6589e-01, 2.4386e-01, 3.9701e-01,\n",
       "        4.0222e-02, 3.9738e-01, 5.0223e-03, 4.8130e-02, 6.5207e-01, 3.0475e-01,\n",
       "        4.4496e-01, 6.9273e-01, 1.0358e-01, 1.3718e-01, 2.0242e-01, 5.7235e-01,\n",
       "        2.9149e-01, 3.7766e-01, 6.4563e-02, 1.3865e-01, 2.1821e-01, 8.6096e-01,\n",
       "        2.2961e-01, 7.6714e-01, 1.2152e-01, 1.5426e-01, 1.4538e-03, 1.2724e-01,\n",
       "        4.3739e-02, 5.9118e-02, 9.3473e-03, 6.4580e-01, 5.7878e-02, 3.0313e-01,\n",
       "        8.7075e-02, 4.6477e-02, 5.1930e-02, 4.7460e-01, 4.1896e-02, 2.8247e-02,\n",
       "        3.6786e-01, 4.5627e-01, 9.7459e-02, 4.8167e-01, 4.1302e-01, 1.6205e-02,\n",
       "        3.9047e-02, 3.0547e-04, 3.3587e-01, 4.9226e-01, 2.6205e-01, 2.5814e-01,\n",
       "        6.6904e-02, 6.1020e-01, 3.7302e-01, 5.5471e-01, 1.7925e-01, 2.4535e-01,\n",
       "        6.8044e-02, 1.1268e-02, 2.2256e-01, 3.1314e-01, 2.4790e-01, 4.3044e-01,\n",
       "        1.2092e-01, 3.7928e-02, 8.1227e-01, 1.9009e-01, 1.9773e-01, 5.5124e-02,\n",
       "        3.2041e-01, 7.6419e-02, 2.3965e-01, 3.3958e-01, 6.2632e-02, 1.7392e-01,\n",
       "        9.2438e-02, 5.5750e-02, 2.7113e-02, 8.0766e-02, 2.3432e-01, 5.0999e-01,\n",
       "        3.6168e-02, 2.7718e-01, 8.1112e-01, 4.3964e-01, 5.4159e-01, 4.7631e-02,\n",
       "        3.7880e-02, 9.6119e-02, 5.8191e-01, 3.7980e-03, 4.8496e-01, 3.8456e-01,\n",
       "        6.3242e-02, 2.2845e-01, 1.4947e-01, 3.1259e-02, 2.0240e-01, 5.2315e-01,\n",
       "        1.1784e-01, 2.1295e-01, 1.8980e-01, 1.1010e-01, 7.9409e-02, 1.4307e-01,\n",
       "        1.1567e-01, 1.0107e-01, 4.9425e-01, 3.6429e-03, 7.7662e-02, 1.6459e-01,\n",
       "        2.6259e-01, 8.9792e-02, 4.2718e-02, 4.0227e-01, 2.1745e-01, 2.0123e-01,\n",
       "        4.3782e-01, 3.2998e-01, 4.4320e-01, 5.0309e-03, 8.8310e-02, 3.0614e-01,\n",
       "        4.2342e-01, 5.6502e-01, 4.1763e-01, 2.9027e-01, 4.3271e-02, 2.0829e-02,\n",
       "        3.6819e-02, 3.3395e-01, 1.7131e-01, 8.4062e-01, 7.8114e-01, 1.2837e-01,\n",
       "        3.5635e-02, 8.4617e-01, 1.3900e-02, 3.8649e-01, 3.1542e-01, 6.7511e-03,\n",
       "        1.6741e-01, 2.5311e-01, 3.5984e-01, 3.4904e-02, 7.4940e-02, 3.7490e-02,\n",
       "        1.7004e-01, 2.5728e-01, 9.2952e-03, 4.1661e-01, 1.2832e-02, 2.7057e-01,\n",
       "        9.3882e-01, 7.8729e-01, 4.4994e-01, 3.9209e-01, 3.4310e-04, 5.1107e-03,\n",
       "        5.0163e-02, 1.8526e-01, 2.7770e-01, 4.0793e-02, 2.7632e-02, 1.8660e-01,\n",
       "        2.6758e-01, 9.2181e-01, 7.8407e-01, 2.4548e-02, 2.0224e-02, 3.1916e-02,\n",
       "        3.7373e-01, 2.1913e-01, 5.4784e-01, 4.9377e-02, 3.0652e-01, 6.3432e-02,\n",
       "        1.2382e-01, 7.3534e-02, 6.5223e-01, 7.6708e-01, 1.5167e-01, 1.8212e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(1,768,14,14)\n",
    "b=torch.rand(768)\n",
    "torch.mul(a[0,:,1,1],b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3435973888"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "A=torch.tensor(0.8)\n",
    "A=A.numpy()\n",
    "A=A*(2**32)\n",
    "A.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [85], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m M\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(M):  \u001b[38;5;66;03m# enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     ii\u001b[38;5;241m=\u001b[39m\u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "M=torch.rand(10)\n",
    "for i in enumerate(M):  # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "    ii=M[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa='00001001100000111010100011010110'\n",
    "aa\n",
    "len(aa[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111111111111111111111111111111111111111110000000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_binary(number, num_bits):\n",
    "    binary = bin(number & int(\"1\" * num_bits, 2))[2:]\n",
    "    return '0' * (num_bits - len(binary)) + binary\n",
    "\n",
    "number = -1024\n",
    "num_bits = 80\n",
    "\n",
    "binary_representation = convert_to_binary(number, num_bits)\n",
    "print(binary_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0b10000000000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(-1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error0] tensor([3]) -1\n",
      "[error1] tensor([3]) -1\n",
      "[right] tensor([3]) -3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def complement2Dec(binary):\n",
    "    if binary[0] == '1':\n",
    "        # 对负数进行补码处理\n",
    "        inverted_binary = ''.join('1' if bit == '0' else '0' for bit in binary)\n",
    "        decimal = -(int(inverted_binary, 2) + 1)\n",
    "    else:\n",
    "        # 对正数直接转换为十进制\n",
    "        decimal = int(binary, 2)\n",
    "    return decimal\n",
    "\n",
    "aa=-1*torch.randint(-10,20,(1,))\n",
    "\n",
    "aa_bin=bin(aa)\n",
    "if aa<0:\n",
    "    aa_bin=aa_bin[3:]\n",
    "else:\n",
    "    aa_bin=aa_bin[2:]\n",
    "bb=int(aa_bin, 2) - (1 << len(aa_bin))\n",
    "print(\"[error0]\",aa,bb)\n",
    "bb=complement2Dec(aa_bin)\n",
    "print(\"[error1]\",aa,bb)\n",
    "if aa_bin[0]=='1':\n",
    "    bb=int('-0b'+aa_bin,2)\n",
    "    print(\"[right]\",aa,bb)\n",
    "else:\n",
    "    bb=int(aa_bin,2)\n",
    "    print(\"[right]\",aa,bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "input=torch.zeros(1,2,3,3)\n",
    "weight=torch.zeros(2,2,3,3)\n",
    "bias=torch.ones(2)\n",
    "output=F.conv2d(input, weight, stride=(1,1),bias=bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0b111100001010100100000000'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(-15771904)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"111100001010100100000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fliplr(string):\n",
    "    # string = \"Hello, World!\"\n",
    "    reversed_string = string[::-1]\n",
    "    return reversed_string\n",
    "    # print(reversed_string)\n",
    "    # 运行以上代码，将输出反转后的字符串：\"!dlroW ,olleH\"。通过使用切片操作符[::-1]，可以从字符串的末尾开始，每次递减一个索引，从而实现字符串的反转。\n",
    "def compbin2hex(binary):\n",
    "    #二进制补码转十进制\n",
    "    output=0\n",
    "    if binary[0]=='1':#如果为负数\n",
    "        inverted_binary = ''.join('1' if bit == '0' else '0' for bit in binary[1:])\n",
    "        output=-(int(inverted_binary,2)+1)\n",
    "    else:\n",
    "        binary=fliplr(binary)\n",
    "        for i in range(len(binary)):\n",
    "            output=output+(ord(binary[i])-48)*pow(2,i)\n",
    "    return output\n",
    "output=compbin2hex('01010')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary=bin(-127)\n",
    "binary=binary[3:]\n",
    "print(binary)\n",
    "inverted_binary = ''.join('1' if bit == '0' else '0' for bit in binary[1:])\n",
    "dec=int(inverted_binary,2)+1\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101111111'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_binary(number, num_bits):\n",
    "    binary = bin(number & int(\"1\" * num_bits, 2))[2:]\n",
    "    return '0' * (num_bits - len(binary)) + binary\n",
    "a=convert_to_binary(-129,9)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "Bias_Tmp_All=torch.rand(768)\n",
    "q1q2=torch.rand(4,768,14,14)\n",
    "result=q1q2.permute(0,3,2,1)+Bias_Tmp_All\n",
    "result[0,0,0,]-(q1q2[0,:,0,0]+Bias_Tmp_All)\n",
    "result=result.permute(0,3,2,1)\n",
    "\n",
    "\n",
    "SCALE=torch.rand(768)\n",
    "ScaleMul=torch.mul(result.permute(0,3,2,1),SCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01111111111111111'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='00001111111111111111'\n",
    "a[-17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def Generate_Weight_Bin(Tensor,Type,Path):\n",
    "    #生成权重和图片的bin文件\n",
    "    #Tensor:输入的tensor\n",
    "    #Type：可选：Image，ConvWeight，LinearWeight，QuantData\n",
    "    #path:存储路径\n",
    "    \n",
    "    #image要求image是量化后的8bit数据\n",
    "    if Type==\"Image\":\n",
    "        assert len(Tensor.shape)==3\n",
    "        with open (Path+\"/Image.bin\",'w') as ff:\n",
    "        #Wq的维度：[OC,IC,K,K]\n",
    "            for Row in range(Tensor.shape[1]):#遍历行\n",
    "                for Col in range(Tensor.shape[2]):#遍历列\n",
    "                    for IC in range(0,Tensor.shape[0],8):#遍历通道\n",
    "                        Hex0=Tensor[IC+0,Row,Col].item()&0xff\n",
    "                        Hex1=Tensor[IC+1,Row,Col].item()&0xff\n",
    "                        Hex2=Tensor[IC+2,Row,Col].item()&0xff\n",
    "                        Hex3=Tensor[IC+3,Row,Col].item()&0xff\n",
    "                        Hex4=Tensor[IC+4,Row,Col].item()&0xff\n",
    "                        Hex5=Tensor[IC+5,Row,Col].item()&0xff\n",
    "                        Hex6=Tensor[IC+6,Row,Col].item()&0xff\n",
    "                        Hex7=Tensor[IC+7,Row,Col].item()&0xff\n",
    "                        #print('%02x%02x%02x%02x%02x%02x%02x%02x'%(Hex7,Hex6,Hex5,Hex4,Hex3,Hex2,Hex1,Hex0))\n",
    "                        ff.write('%02x%02x%02x%02x%02x%02x%02x%02x'%(Hex7,Hex6,Hex5,Hex4,Hex3,Hex2,Hex1,Hex0))\n",
    "                        ff.write(\"\\n\")\n",
    "            ff.close()\n",
    "Image=torch.randint(0,255,(8,224,224))\n",
    "len(Image.shape)\n",
    "print(\"Image\"=='Image')\n",
    "Generate_Weight_Bin(Image,'Image',r\"E:\\Transformer\\Transformer_Arithmatic\\Transformer_Main\\BinPath\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S1', 'Z1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA=[\"S1\",\"Z1\"]\n",
    "AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5900 0.8746 0.0381 0.2223 0.0573 0.7255 0.5468 0.8782 0.6533 0.1052 0.5575 0.5817 0.2564 0.7570 0.5693 0.8580 0.1442 0.5555 \n",
      "0.4053 0.2857 0.6090 0.4987 0.5069 0.6957 0.3159 0.4980 0.7419 0.4554 0.3331 0.0074 0.2225 0.6998 0.9449 0.7807 0.3040 0.8826 \n",
      "0.2615 0.3207 0.4010 0.6163 0.4286 0.5436 0.7966 0.4408 0.6621 0.4182 0.0859 0.9937 0.1960 0.0945 0.2836 0.8827 0.0148 0.0588 \n",
      "0.6993 0.9975 0.8634 0.0371 0.1230 0.0656 0.8094 0.8603 0.3696 0.2882 0.1645 0.9922 0.6486 0.7259 0.4661 0.0603 0.5087 0.1564 \n",
      "tensor([[0.5900, 0.8746, 0.0381, 0.2223, 0.0573, 0.7255, 0.5468, 0.8782, 0.6533,\n",
      "         0.1052, 0.5575, 0.5817, 0.2564, 0.7570, 0.5693, 0.8580, 0.1442, 0.5555],\n",
      "        [0.4053, 0.2857, 0.6090, 0.4987, 0.5069, 0.6957, 0.3159, 0.4980, 0.7419,\n",
      "         0.4554, 0.3331, 0.0074, 0.2225, 0.6998, 0.9449, 0.7807, 0.3040, 0.8826],\n",
      "        [0.2615, 0.3207, 0.4010, 0.6163, 0.4286, 0.5436, 0.7966, 0.4408, 0.6621,\n",
      "         0.4182, 0.0859, 0.9937, 0.1960, 0.0945, 0.2836, 0.8827, 0.0148, 0.0588],\n",
      "        [0.6993, 0.9975, 0.8634, 0.0371, 0.1230, 0.0656, 0.8094, 0.8603, 0.3696,\n",
      "         0.2882, 0.1645, 0.9922, 0.6486, 0.7259, 0.4661, 0.0603, 0.5087, 0.1564]])\n",
      "tensor([[0.5900, 0.0381, 0.0573, 0.5468, 0.6533, 0.5575, 0.2564, 0.5693, 0.1442,\n",
      "         0.8746, 0.2223, 0.7255, 0.8782, 0.1052, 0.5817, 0.7570, 0.8580, 0.5555],\n",
      "        [0.4053, 0.6090, 0.5069, 0.3159, 0.7419, 0.3331, 0.2225, 0.9449, 0.3040,\n",
      "         0.2857, 0.4987, 0.6957, 0.4980, 0.4554, 0.0074, 0.6998, 0.7807, 0.8826],\n",
      "        [0.2615, 0.4010, 0.4286, 0.7966, 0.6621, 0.0859, 0.1960, 0.2836, 0.0148,\n",
      "         0.3207, 0.6163, 0.5436, 0.4408, 0.4182, 0.9937, 0.0945, 0.8827, 0.0588],\n",
      "        [0.6993, 0.8634, 0.1230, 0.8094, 0.3696, 0.1645, 0.6486, 0.4661, 0.5087,\n",
      "         0.9975, 0.0371, 0.0656, 0.8603, 0.2882, 0.9922, 0.7259, 0.0603, 0.1564]])\n",
      "tensor([[[[0.5900, 0.0381, 0.0573],\n",
      "          [0.5468, 0.6533, 0.5575],\n",
      "          [0.2564, 0.5693, 0.1442]],\n",
      "\n",
      "         [[0.8746, 0.2223, 0.7255],\n",
      "          [0.8782, 0.1052, 0.5817],\n",
      "          [0.7570, 0.8580, 0.5555]]],\n",
      "\n",
      "\n",
      "        [[[0.4053, 0.6090, 0.5069],\n",
      "          [0.3159, 0.7419, 0.3331],\n",
      "          [0.2225, 0.9449, 0.3040]],\n",
      "\n",
      "         [[0.2857, 0.4987, 0.6957],\n",
      "          [0.4980, 0.4554, 0.0074],\n",
      "          [0.6998, 0.7807, 0.8826]]],\n",
      "\n",
      "\n",
      "        [[[0.2615, 0.4010, 0.4286],\n",
      "          [0.7966, 0.6621, 0.0859],\n",
      "          [0.1960, 0.2836, 0.0148]],\n",
      "\n",
      "         [[0.3207, 0.6163, 0.5436],\n",
      "          [0.4408, 0.4182, 0.9937],\n",
      "          [0.0945, 0.8827, 0.0588]]],\n",
      "\n",
      "\n",
      "        [[[0.6993, 0.8634, 0.1230],\n",
      "          [0.8094, 0.3696, 0.1645],\n",
      "          [0.6486, 0.4661, 0.5087]],\n",
      "\n",
      "         [[0.9975, 0.0371, 0.0656],\n",
      "          [0.8603, 0.2882, 0.9922],\n",
      "          [0.7259, 0.0603, 0.1564]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weight=torch.rand(4,2,3,3)\n",
    "weight_permute=weight.permute(0,2,3,1)\n",
    "\n",
    "\n",
    "weight_permute\n",
    "WeightMatrix=torch.zeros(4,2*3*3)\n",
    "for Oc in range(4):\n",
    "    for H in range(3):\n",
    "        for W in range(3):\n",
    "            for Ic in range(2):\n",
    "                print(\"%.4f\"%weight[Oc,Ic,H,W].item(),end=' ')\n",
    "                # print(weight_permute[Oc,H,W,Ic]==weight[Oc,Ic,H,W])\n",
    "    print()\n",
    "print(weight_permute.reshape(-1,2*3*3))\n",
    "print(weight.reshape(-1,2*3*3))\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "data={}\n",
    "data[\"data\"]=[1,2,3]\n",
    "\n",
    "sio.savemat(r'matlab\\Embedding_Conv.mat', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(weight_permute.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6203, 0.6479, 0.5679, 0.7791, 0.9912, 0.5173, 0.0303, 0.2098, 0.6674,\n",
      "         0.2378, 0.6693, 0.1406, 0.1802, 0.7911, 0.1496, 0.9827, 0.1039, 0.0095],\n",
      "        [0.9029, 0.3299, 0.7488, 0.9962, 0.8423, 0.3409, 0.0522, 0.7883, 0.2416,\n",
      "         0.1972, 0.8414, 0.5133, 0.9008, 0.3382, 0.0687, 0.4170, 0.9094, 0.6457],\n",
      "        [0.6904, 0.6208, 0.4632, 0.8345, 0.7178, 0.5761, 0.3408, 0.5039, 0.3442,\n",
      "         0.8730, 0.4708, 0.8199, 0.9649, 0.0668, 0.9774, 0.8267, 0.6873, 0.3437],\n",
      "        [0.4341, 0.3287, 0.1510, 0.1694, 0.4085, 0.4417, 0.8256, 0.2875, 0.0944,\n",
      "         0.3703, 0.1294, 0.5641, 0.5131, 0.1129, 0.1789, 0.3293, 0.6852, 0.6605]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.6203, 0.6479, 0.5679, 0.7791, 0.9912, 0.5173,\n",
       "         0.0303, 0.2098, 0.6674, 0.2378, 0.6693, 0.1406, 0.1802, 0.7911, 0.1496,\n",
       "         0.9827, 0.1039, 0.0095],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9029, 0.3299, 0.7488, 0.9962, 0.8423, 0.3409,\n",
       "         0.0522, 0.7883, 0.2416, 0.1972, 0.8414, 0.5133, 0.9008, 0.3382, 0.0687,\n",
       "         0.4170, 0.9094, 0.6457],\n",
       "        [0.0000, 0.0000, 0.0000, 0.6904, 0.6208, 0.4632, 0.8345, 0.7178, 0.5761,\n",
       "         0.3408, 0.5039, 0.3442, 0.8730, 0.4708, 0.8199, 0.9649, 0.0668, 0.9774,\n",
       "         0.8267, 0.6873, 0.3437],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4341, 0.3287, 0.1510, 0.1694, 0.4085, 0.4417,\n",
       "         0.8256, 0.2875, 0.0944, 0.3703, 0.1294, 0.5641, 0.5131, 0.1129, 0.1789,\n",
       "         0.3293, 0.6852, 0.6605]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "weight=torch.rand(4,2,3,3)\n",
    "weight_fattened=weight.permute(0,2,3,1).reshape(weight.shape[0],-1)\n",
    "weight_cat=x=torch.cat((torch.zeros(weight.shape[0],3),weight_fattened),dim=1)\n",
    "print(weight_fattened)\n",
    "weight_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "image=torch.randint(0,255,(1,8,224,224))\n",
    "def Generate_Bin(Tensor,Type,Path):\n",
    "    #生成权重和图片的bin文件\n",
    "    #Tensor:输入的tensor\n",
    "    #Type：可选：Image，ConvWeight，LinearWeight，QuantData\n",
    "    #path:存储路径\n",
    "    if Type==\"Image\":\n",
    "        Output_Reshape1=torch.permute(Tensor,[0,2,3,1])\n",
    "        Output_Reshape2=torch.reshape(Output_Reshape1,[-1])\n",
    "        Output_Reshape3 = Output_Reshape2.numpy()\n",
    "        int_output= Output_Reshape3.astype(np.uint8)#转为int，因为之前的tensor是float\n",
    "        print(int_output.dtype)\n",
    "        ff=open (Path+\"/Image.bin\",'w')\n",
    "        int_output.tofile(ff)\n",
    "        ff.close()\n",
    "    elif Type==\"ConvWeight\":\n",
    "        Output_Reshape1=torch.permute(Tensor,[0,2,3,1])\n",
    "        Output_Reshape2=torch.reshape(Output_Reshape1,[-1])\n",
    "        Output_Reshape3 = Output_Reshape2.numpy()\n",
    "        int_output= Output_Reshape3.astype(np.uint8)#转为int，因为之前的tensor是float\n",
    "        print(int_output.dtype)\n",
    "        ff=open (Path+\"ConvWeight.bin\",'w')\n",
    "        int_output.tofile(ff)\n",
    "        ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[173, 235, 130],\n",
       "        [201, 241, 134],\n",
       "        [ 84,  64, 219],\n",
       "        [  0,  23,  45],\n",
       "        [206,  32,  22],\n",
       "        [107,  36, 151],\n",
       "        [ 90, 250,  62],\n",
       "        [221, 106, 204]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "image[0,:,0,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.zeros((1,3),np.uint8)\n",
    "b=np.ones((1,3),np.uint8)\n",
    "c=[a,b]\n",
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "Tensor=[np.array([1,1,1,1]), np.array([2,2,2,2]), np.array([3,3,3,3])]\n",
    "Bias=Tensor[0].astype(np.uint32)\n",
    "Scale=Tensor[1].astype(np.uint32)\n",
    "Shift=Tensor[2].astype(np.uint16)\n",
    "ff=open (\"BinPath/ConvQuant.bin\",'a')\n",
    "Bias.tofile(ff)\n",
    "Scale.tofile(ff)\n",
    "Shift.tofile(ff)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1]),\n",
       " array([2, 2, 2, 2]),\n",
       " array([3, 3, 3, 3]),\n",
       " tensor([[0.5326, 0.7909]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "Tensor=[np.array([1,1,1,1]), np.array([2,2,2,2]), np.array([3,3,3,3]),torch.rand(1,2)]\n",
    "Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[1,2,3]\n",
    "b=np.array(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          ...,\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.]],\n",
      "\n",
      "         [[117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          ...,\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.]],\n",
      "\n",
      "         [[117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          ...,\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.],\n",
      "          [117., 117., 117.,  ..., 117., 117., 117.]]]])\n",
      "torch.Size([1, 8, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A=torch.zeros(1,3,224,224)+117\n",
    "print(A)\n",
    "B=torch.rand(1,5,224,224)\n",
    "C=torch.cat((A,B),dim=1)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Name: Linear\n",
      "Layer 2 Name: Linear\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        layer1_name = self.fc1.__class__.__name__\n",
    "        print(\"Layer 1 Name:\", layer1_name)\n",
    "        x = self.fc2(x)\n",
    "        layer2_name = self.fc2.__class__.__name__\n",
    "        print(\"Layer 2 Name:\", layer2_name)\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "net = MyNetwork()\n",
    "\n",
    "# 创建输入数据\n",
    "input_data = torch.randn(5, 10)  # 假设输入数据维度为(5, 10)\n",
    "\n",
    "# 前向传播\n",
    "output = net(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=10\n",
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
