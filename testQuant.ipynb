{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_B(S1, S2, S3):  # 第一组  β即N_REAL  +++++求shitf++++\n",
    "    M = (S1 * S2) / S3\n",
    "    M.squeeze()\n",
    "    M=M.cpu()#先将tensor放到cpu上\n",
    "    M = M.numpy()\n",
    "    \n",
    "    daxiao = S2.shape[0]  # 第一层权重的shape[0]是32 shape[0]表示行数 是一维大小位32的列向量\n",
    "    SCALE = np.zeros(daxiao, dtype=np.uint32, order='C')  # 相当于32个输出通道 每个对应一组shift\n",
    "    N_REAL = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    for i, ii in enumerate(M):  # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "\n",
    "        while not (ii >= 0.5 and ii <= 1.0):  # 左移到（0.5，1） 左移一次相当于*2\n",
    "            ii *= 2\n",
    "        pass\n",
    "        mmmm = ii * (2 ** 32)  # 乘2^32\n",
    "\n",
    "        SCALE[i] = mmmm.astype(np.int32)\n",
    "\n",
    "    for i, ii in enumerate(M):\n",
    "        N_REAL[i] = round(math.log(SCALE[i] / ii, 2)) - 32   # fpga加了1这里要减1,  β值也是m维 相当于存的mmmm\n",
    "        if int(N_REAL[i])<0:\n",
    "            print(\"error\\n\")\n",
    "        if N_REAL[i] !=0:\n",
    "            N_REAL[i]=N_REAL[i]-1\n",
    "\n",
    "    return N_REAL\n",
    "\n",
    "\n",
    "def gen_M_N(S1, S2, S3):  # 第二组求M'即M  ++++++返回scale和shift+++++\n",
    "    daxiao = S2.shape[0]\n",
    "    M = np.zeros(daxiao, dtype=np.uint32, order='C')\n",
    "    N_REAL = gen_B(S1, S2, S3)\n",
    "    M = np.zeros(S2.shape[0])\n",
    "    \n",
    "    for i, ii in enumerate(M):\n",
    "        Scale=(S1*S2[i]/S3).cpu()\n",
    "        # Scale=Scale.numpy\n",
    "        M[i] = (torch.round(Scale * (2 ** (32 + N_REAL[i] + 1)))).numpy()  # s1s2/s3 *2^(32+β+1)\n",
    "    M = M.astype(np.uint32)\n",
    "    # exit()\n",
    "    return M, N_REAL\n",
    "\n",
    "\n",
    "# r_b=s1*s2*q_b\n",
    "# r_b是量化前的bias,q_b是量化后的bias\n",
    "def gen_int_bias(s1, s2, bias_float):  # 求bias/s1s2即bb\n",
    "    aa = bias_float / s1\n",
    "    # print(bias_float)\n",
    "    # exit()\n",
    "    bb = torch.div(aa, s2)  # 对应元素做除法\n",
    "\n",
    "    # for i, m in enumerate(bb):\n",
    "    #     bb[i] = round(m.item())\n",
    "    # bias = bb.int()\n",
    "    return bb\n",
    "\n",
    "\n",
    "def gen_M(s1, s2, s3):\n",
    "    aa = s1 * s2\n",
    "    M = aa / s3\n",
    "    return M\n",
    "\n",
    "\n",
    "def new_bias(z1, q2, bias):  # 求最终的bias=bias/s1s2-q2z1\n",
    "    q2 = q2.type(torch.float64)\n",
    "    bias1 = z1 * q2\n",
    "    shape = bias1.shape\n",
    "    n_bias = np.zeros(shape[0], dtype=np.float64, order='C')\n",
    "    for m in range(shape[0]):  # bias1的维度是M C K K 将C K K 做累加 变成M维\n",
    "        n_bias[m] = bias1[m, :, :, :].sum()  # 从第一组开始一直有m组，m是输出通道数\n",
    "        # print()\n",
    "        n_bias[m] = (bias[m] - n_bias[m])  # 做减法\n",
    "    # print(n_bias) 第一层n_bias是一维32个\n",
    "    # exit()\n",
    "    daxiao = shape[0]  # 第一层是32\n",
    "    SCALE = np.zeros(daxiao, dtype=np.float64, order='C')\n",
    "    # N_REAL = np.zeros(daxiao, dtype=np.float32, order='C')\n",
    "    N_REAL = []\n",
    "    for i, ii in enumerate(n_bias):  # i和ii就是从n_bias中取值\n",
    "        index = 0\n",
    "\n",
    "        while not (abs(ii) >= (2 ** 23) and abs(ii) <= (2 ** 24)):\n",
    "            if index >= 16:  # fpga里面最多移动16位,所有成到16就停止了,这样精度也够了\n",
    "                break\n",
    "            else:\n",
    "                ii *= 2\n",
    "                index = index + 1\n",
    "\n",
    "        N_REAL.append(index)\n",
    "        SCALE[i] = round(ii)\n",
    "    out_bias = []\n",
    "    for index in range(shape[0]):\n",
    "        data_integer_old = ('{:024b}'.format(int(SCALE[index]) & 0xffffff))  # {:024b} 24位2二进制不足补0；& 0xffffff按位与\n",
    "        n = N_REAL[index]\n",
    "        symbol = '0'\n",
    "        if n_bias[index] < 0:  # 符号位\n",
    "            symbol = '1'#这里也不是多此一举，因为如果你给一个uint24，MSB=1，那么你在FPGA那边如何分辨这是正数还是负数，所以还是需要1bit的符号位来帮助fpga做判断的\n",
    "        elif n_bias[index] > 0:\n",
    "            symbol = '0'\n",
    "        data_integer = data_integer_old[8:]\n",
    "        data_decimal = '{:07b}'.format(int(n))\n",
    "        out_bias1 = symbol + str(data_decimal) + str(data_integer_old)  # 1bit+7bit+24bit\n",
    "        a = int(out_bias1, 2)  # 转成int型 out_bias1为二进制 ；a是十进制\n",
    "        out_bias.append(a)  # 一个一个写入out_bias\n",
    "    # print(out_bias)\n",
    "    # exit()\n",
    "    return out_bias\n",
    "\n",
    "\n",
    "def get_add_bias(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_add_SCALE(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_add_NREAL(new, shape, old):  # 补0\n",
    "    for kernel_num in range(shape):\n",
    "        new[kernel_num] = old[kernel_num]\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_weight(new_weight, shape, weight, inchannel):  # 八入八出操作 输入通道inchannel不固定 输出通道outchannel为8\n",
    "    j = 0\n",
    "\n",
    "    shift_num = 0\n",
    "    for index in range(inchannel):\n",
    "        if (inchannel == (1 << index)):  # index不停左移一位直到与inchannel相等 来判断移了几位   2的shift_num次方=inchannel\n",
    "            shift_num = index\n",
    "            break\n",
    "    # if shape[0] == 32 and shape[1] == 64 and shape[2] == 1:\n",
    "    #     # print(\"shape:\",new_weight.shape)\n",
    "    #     with open('weight_1x1_8i8o(2).txt', 'a')as f:\n",
    "    #         f.write('-' * 40 + '\\n')\n",
    "    #         f.write(str(shape) + '\\n')\n",
    "    for i in range(shape[2]):  # mckk  K\n",
    "        for ii in range(shape[3]):  # K\n",
    "            for kernel_times in range(shape[0] >> 3):  # >>右移三位 因为输出通道outchannel默认为8 卷积核个数\n",
    "                for channel_in_times in range(shape[1] >> shift_num):  # 右移shift_num 输入通道数\n",
    "                    for iii in range(8):  # shape[0] >> 3 右移了3次即2^3 要补8次\n",
    "                        for iiii in range(\n",
    "                                inchannel):  # shape[1] >> shift_num 右移了shift_num即2^shift_num 要补inchannel次(2^shift_num =inchannel)\n",
    "                            # print('++++++++++++++++++')\n",
    "                            weight[j] = new_weight[kernel_times * 8 + iii][channel_in_times * inchannel + iiii][i][ii]\n",
    "                            # if shape[0] == 32 and shape[1] == 64 and shape[2] == 1:\n",
    "                            # with open('weight_1x1_8i8o(2).txt', 'a')as f:\n",
    "                            #     f.write(str(kernel_times * 8 + iii) + ',')\n",
    "                            #     f.write(str(channel_in_times * inchannel + iiii) + ',')\n",
    "                            #     f.write(str(i) + ',')\n",
    "                            #     f.write(str(ii) + '\\n')\n",
    "                            #\n",
    "                            j += 1\n",
    "    # exit()\n",
    "    return weight\n",
    "\n",
    "\n",
    "def add_weight_channel(new_weig, weig, shape):  # 补0 把weig存入new_weig；new_weig是全0，\n",
    "    for kernel_num in range(shape[0]):\n",
    "        for channel_in_num in range(shape[1]):\n",
    "            for row in range(shape[2]):\n",
    "                for col in range(shape[3]):\n",
    "                    new_weig[kernel_num][channel_in_num][row][col] = weig[kernel_num][channel_in_num][row][col]\n",
    "    return new_weig\n",
    "\n",
    "\n",
    "def tensorr(x):\n",
    "    tensor_py = torch.from_numpy(np.load(x))  # 创建tensor\n",
    "    return tensor_py\n",
    "\n",
    "\n",
    "def get_weight2(new_weight, shape, weight):  # 四维权重写成一维\n",
    "    j = 0\n",
    "    for kernel_times in range(shape[0]):\n",
    "        for channel_in_times in range(shape[1]):\n",
    "            for i in range(shape[2]):\n",
    "                for ii in range(shape[3]):\n",
    "                    # print('++++++++++++++++++')\n",
    "                    weight[j] = new_weight[kernel_times][channel_in_times][i][ii]\n",
    "                    j += 1\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25073\\AppData\\Local\\Temp\\ipykernel_12988\\3006563256.py:13: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ii *= 2\n"
     ]
    }
   ],
   "source": [
    "S1=torch.rand(1)*0.499+0.001\n",
    "S2=torch.rand(768)*0.499+0.001\n",
    "S3=torch.rand(1)*0.499+0.001\n",
    "bias=torch.rand(1)*0.499+0.001\n",
    "SCALE, N_REAL = gen_M_N(S1, S2, S3)\n",
    "SCALE\n",
    "N_REAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('1010010',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bias_Tmp='1111'\n",
    "BiasAdd=int(Bias_Tmp, 2) - (1 << len(Bias_Tmp))\n",
    "BiasAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1010\n",
      "6\n",
      "-0b101\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "a=bin(10)\n",
    "print(a)\n",
    "print(len(a))\n",
    "print('-0b'+a[2:5])\n",
    "print(int('-0b'+a[2:5],2))\n",
    "# bin(-5)\n",
    "# for i in range(len(a)):\n",
    "#     print(i,\"  \",a[i])\n",
    "# print(a[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0b1111111\n",
      "1111111\n"
     ]
    }
   ],
   "source": [
    "a=bin(-127)\n",
    "print(a)\n",
    "print(a[3:].zfill(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 222, 222])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.rand(1, 8, 224, 224)\n",
    "w = torch.rand(768, 8, 3, 3)\n",
    "stride = (1, 1)\n",
    "\n",
    "output = F.conv2d(x, w, stride=stride)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=int(102)\n",
    "bb=torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5007e-01, 2.2411e-03, 2.0470e-01, 5.2534e-03, 8.0235e-01, 5.2113e-01,\n",
       "        1.6234e-01, 7.2542e-01, 1.4723e-01, 5.8487e-02, 3.2948e-04, 2.1061e-01,\n",
       "        5.0463e-02, 4.2238e-02, 3.2456e-01, 1.0675e-02, 1.2187e-01, 1.3574e-01,\n",
       "        3.3166e-01, 3.6028e-02, 2.1759e-01, 1.7268e-01, 1.4460e-01, 3.8965e-01,\n",
       "        5.8133e-01, 1.1793e-01, 2.4376e-01, 3.6589e-01, 2.2602e-02, 1.4659e-01,\n",
       "        3.5235e-01, 2.8605e-01, 5.2180e-01, 8.3473e-02, 3.1087e-01, 5.4658e-04,\n",
       "        1.1127e-01, 1.1466e-02, 4.5347e-01, 1.5913e-01, 7.9129e-02, 2.1646e-01,\n",
       "        1.9854e-02, 5.8259e-01, 2.3040e-01, 2.0833e-02, 1.2947e-01, 5.8352e-01,\n",
       "        1.1117e-01, 5.3672e-01, 2.3853e-01, 6.7361e-02, 7.0189e-01, 4.8092e-01,\n",
       "        5.0899e-01, 2.8234e-02, 1.5773e-01, 7.4047e-02, 2.4848e-01, 3.8188e-01,\n",
       "        4.2239e-01, 1.8467e-02, 5.7272e-02, 3.8750e-01, 8.0112e-02, 5.1387e-01,\n",
       "        1.8160e-01, 1.7189e-01, 5.6067e-01, 1.1905e-01, 4.3684e-01, 6.1986e-02,\n",
       "        2.3197e-01, 3.6373e-01, 6.9991e-01, 5.5023e-01, 1.8354e-01, 5.1366e-02,\n",
       "        4.3522e-01, 2.0100e-01, 9.4076e-03, 7.4761e-02, 4.8473e-02, 6.2607e-02,\n",
       "        2.1890e-01, 7.4243e-02, 1.2383e-01, 3.3540e-01, 1.6655e-01, 6.8948e-03,\n",
       "        1.3887e-01, 4.6798e-01, 1.9080e-02, 1.9759e-01, 7.2493e-02, 2.8863e-03,\n",
       "        2.5338e-01, 2.0433e-02, 6.5629e-02, 4.0576e-01, 3.3908e-02, 8.0002e-03,\n",
       "        4.1221e-02, 9.9792e-04, 5.1697e-01, 9.7696e-02, 6.1538e-01, 3.4261e-01,\n",
       "        7.1281e-03, 8.7523e-02, 1.6338e-01, 3.7008e-01, 4.8340e-01, 1.4387e-03,\n",
       "        3.7858e-01, 5.7882e-02, 9.4460e-02, 8.3003e-01, 4.4682e-03, 2.3723e-01,\n",
       "        1.0489e-01, 6.3103e-02, 2.2163e-01, 5.4237e-03, 4.1119e-01, 2.1318e-01,\n",
       "        2.5222e-01, 2.4876e-01, 1.4830e-01, 4.5673e-02, 5.0293e-01, 6.7000e-01,\n",
       "        3.1765e-01, 8.0708e-01, 7.0382e-01, 2.3736e-01, 2.8333e-01, 5.4603e-01,\n",
       "        5.8726e-01, 3.6336e-01, 1.9324e-01, 7.5286e-02, 5.1744e-01, 7.2371e-02,\n",
       "        4.2239e-01, 2.2575e-01, 1.0337e-02, 2.4391e-02, 3.2766e-01, 5.5592e-01,\n",
       "        3.9422e-01, 2.8029e-01, 3.8409e-01, 1.4974e-01, 2.4536e-01, 1.4373e-01,\n",
       "        8.0244e-02, 3.3036e-02, 1.5458e-01, 8.9706e-02, 3.6000e-02, 1.8692e-01,\n",
       "        3.5538e-01, 2.1732e-01, 3.2203e-02, 2.1853e-02, 3.1391e-01, 2.7163e-01,\n",
       "        6.7852e-01, 5.0273e-02, 1.2934e-01, 2.6566e-01, 8.7725e-02, 3.0845e-01,\n",
       "        4.5961e-01, 2.5302e-01, 3.6279e-01, 6.3868e-01, 2.3189e-01, 2.2456e-01,\n",
       "        1.3356e-02, 5.2695e-01, 1.0721e-02, 4.2980e-01, 1.3861e-01, 1.5268e-01,\n",
       "        4.4152e-01, 1.3498e-01, 2.1799e-01, 1.2478e-01, 4.6218e-01, 3.3377e-02,\n",
       "        4.2263e-01, 6.4758e-01, 1.5082e-01, 3.1977e-01, 3.8548e-01, 9.3596e-02,\n",
       "        6.0847e-02, 9.1432e-02, 4.7109e-01, 1.9994e-01, 5.5122e-01, 1.2318e-01,\n",
       "        4.0952e-01, 6.7640e-02, 1.0167e-01, 4.3676e-01, 7.4827e-01, 1.2779e-01,\n",
       "        2.7611e-01, 2.1603e-01, 8.7753e-02, 3.0940e-01, 7.0877e-01, 3.2021e-01,\n",
       "        7.2716e-02, 3.3669e-01, 3.7105e-02, 9.2683e-02, 1.5440e-02, 4.6516e-02,\n",
       "        1.6493e-02, 3.8413e-01, 3.1805e-01, 5.0204e-02, 1.1929e-02, 2.9439e-03,\n",
       "        3.0360e-01, 2.4275e-01, 3.5219e-02, 9.6975e-01, 4.0620e-01, 3.4653e-02,\n",
       "        2.1201e-01, 1.0882e-01, 6.9655e-02, 4.2946e-01, 1.3975e-01, 5.3953e-01,\n",
       "        4.8469e-01, 1.0071e-01, 8.0557e-01, 1.9706e-01, 6.5127e-01, 7.3567e-02,\n",
       "        3.3421e-01, 4.5229e-01, 2.6353e-01, 5.2640e-01, 4.3259e-01, 7.3551e-01,\n",
       "        5.1944e-02, 2.2226e-01, 2.5128e-01, 2.4532e-01, 3.2166e-01, 4.6074e-02,\n",
       "        6.9613e-02, 4.0859e-01, 1.8458e-01, 4.2507e-01, 1.7006e-02, 1.0209e-01,\n",
       "        4.7774e-02, 4.8541e-01, 5.0827e-03, 3.2151e-02, 6.1863e-01, 9.9924e-02,\n",
       "        2.3601e-01, 7.7862e-01, 7.3701e-01, 2.8332e-01, 5.0522e-03, 2.3721e-01,\n",
       "        1.8929e-02, 2.5096e-01, 2.8577e-01, 3.0846e-01, 4.2441e-02, 2.4252e-02,\n",
       "        8.4951e-03, 9.0187e-03, 1.0702e-01, 3.6129e-01, 4.5225e-02, 6.5059e-02,\n",
       "        5.0491e-01, 3.5748e-02, 2.6323e-01, 3.6121e-01, 3.0279e-01, 3.6068e-01,\n",
       "        5.8363e-01, 8.5959e-01, 1.9227e-01, 1.7227e-01, 2.8427e-01, 1.0797e-01,\n",
       "        1.5109e-02, 2.4030e-02, 6.0585e-01, 7.2622e-01, 3.4078e-01, 1.0620e-01,\n",
       "        3.0841e-02, 4.7955e-01, 4.9612e-02, 4.1827e-01, 2.2739e-01, 1.9353e-01,\n",
       "        5.8298e-01, 1.0333e-02, 3.6137e-01, 3.1267e-01, 4.9379e-01, 1.5944e-02,\n",
       "        6.1860e-01, 8.5076e-02, 3.8417e-02, 5.0191e-02, 7.4359e-01, 1.4077e-01,\n",
       "        3.7786e-01, 2.3600e-01, 3.0805e-02, 8.0940e-02, 8.3191e-02, 4.9584e-01,\n",
       "        5.3524e-03, 3.0613e-02, 1.5527e-02, 8.7075e-01, 6.9710e-01, 7.3564e-01,\n",
       "        3.9965e-01, 4.6793e-01, 3.2368e-03, 1.2750e-02, 3.3953e-01, 2.1083e-01,\n",
       "        4.2234e-01, 5.2729e-01, 2.5190e-01, 1.9034e-01, 1.7494e-01, 4.3533e-01,\n",
       "        5.2246e-02, 2.4273e-01, 2.6141e-01, 6.4721e-02, 6.7885e-01, 2.8779e-02,\n",
       "        5.7033e-02, 2.7687e-01, 1.5225e-02, 3.6720e-01, 6.1001e-01, 3.6725e-01,\n",
       "        3.4061e-01, 8.5717e-03, 1.2981e-01, 3.5723e-02, 4.1077e-02, 7.5610e-01,\n",
       "        2.4790e-01, 6.8826e-01, 1.4251e-01, 9.8246e-03, 1.1443e-02, 7.3681e-02,\n",
       "        3.3852e-01, 5.7528e-02, 2.8957e-01, 2.4300e-01, 2.6190e-01, 1.6538e-01,\n",
       "        4.7191e-01, 6.8749e-02, 8.0063e-01, 7.7518e-01, 5.9326e-01, 8.7056e-01,\n",
       "        1.2499e-01, 1.8631e-01, 1.6883e-01, 3.8072e-02, 4.8878e-02, 6.8445e-03,\n",
       "        2.9302e-01, 7.4422e-01, 3.6523e-03, 3.9178e-01, 4.2383e-01, 5.3604e-01,\n",
       "        2.7239e-01, 7.5202e-02, 2.1293e-03, 1.6652e-01, 1.4497e-01, 8.2156e-01,\n",
       "        5.7282e-01, 2.7368e-03, 3.9185e-01, 3.2980e-01, 5.2711e-01, 1.0103e-01,\n",
       "        2.4142e-01, 1.5564e-02, 3.7957e-02, 3.3730e-01, 4.6389e-01, 1.4128e-01,\n",
       "        4.1984e-01, 2.6418e-01, 3.4888e-01, 1.9997e-01, 1.9825e-02, 2.0603e-01,\n",
       "        5.7613e-02, 2.3193e-01, 1.3147e-01, 6.0833e-01, 5.6636e-01, 1.6211e-02,\n",
       "        3.0053e-01, 4.1503e-03, 3.1028e-01, 2.4836e-01, 2.6582e-01, 4.5905e-01,\n",
       "        5.3631e-01, 8.9065e-01, 5.5984e-02, 4.1479e-02, 1.4237e-01, 4.8714e-01,\n",
       "        1.4740e-02, 2.0302e-01, 5.4513e-01, 2.4188e-02, 3.0985e-02, 3.1191e-02,\n",
       "        4.8798e-02, 2.9409e-01, 1.0025e-01, 7.6411e-02, 3.2477e-01, 2.3645e-01,\n",
       "        2.9165e-02, 1.9143e-01, 5.2841e-01, 5.4719e-01, 1.0347e-02, 7.1921e-02,\n",
       "        9.6005e-03, 1.6875e-01, 1.9119e-01, 2.7887e-03, 5.2851e-01, 4.8507e-01,\n",
       "        4.2652e-03, 4.1458e-01, 4.1453e-01, 3.4866e-01, 3.1100e-01, 1.4016e-01,\n",
       "        4.8819e-02, 3.9086e-01, 3.4159e-01, 2.8522e-01, 2.1593e-01, 2.2479e-01,\n",
       "        5.8834e-02, 1.4893e-02, 9.3404e-02, 1.2778e-02, 1.8471e-01, 2.9168e-01,\n",
       "        4.2592e-01, 7.9967e-02, 7.7059e-01, 6.2330e-01, 2.1013e-01, 3.1237e-01,\n",
       "        2.8548e-01, 2.5272e-01, 5.8928e-01, 5.1880e-02, 1.0800e-01, 1.4298e-01,\n",
       "        5.1616e-01, 2.8079e-02, 2.1229e-02, 4.9958e-04, 2.0791e-01, 7.4011e-01,\n",
       "        5.3896e-01, 7.0689e-01, 2.2715e-01, 1.0016e-01, 4.4516e-02, 1.6420e-01,\n",
       "        1.8411e-02, 5.7085e-01, 3.4957e-01, 8.4479e-01, 1.8556e-02, 1.0213e-01,\n",
       "        4.6688e-01, 1.1981e-01, 4.4384e-03, 6.9155e-01, 2.5252e-01, 7.6378e-01,\n",
       "        2.9046e-01, 3.1093e-02, 3.3934e-01, 1.1031e-01, 3.7849e-01, 1.9988e-01,\n",
       "        1.3794e-01, 3.1658e-01, 3.1455e-03, 2.8389e-01, 9.3731e-02, 5.3853e-02,\n",
       "        1.4794e-01, 5.3651e-02, 8.1016e-01, 5.7130e-02, 1.4402e-01, 2.1699e-01,\n",
       "        4.8006e-01, 2.7640e-01, 2.7832e-01, 7.5302e-02, 3.6610e-01, 7.2045e-01,\n",
       "        4.9872e-03, 6.2376e-02, 4.2710e-02, 6.1474e-01, 4.2955e-01, 3.8346e-01,\n",
       "        5.1733e-01, 5.6419e-02, 1.6251e-01, 2.2456e-03, 2.6172e-01, 1.1940e-01,\n",
       "        1.5147e-01, 2.1237e-02, 3.3921e-01, 1.5730e-03, 5.7085e-01, 6.5444e-01,\n",
       "        2.5599e-01, 9.8856e-02, 4.1113e-01, 7.2011e-01, 6.7877e-04, 4.7600e-01,\n",
       "        8.4732e-02, 1.4692e-01, 3.2854e-01, 8.7200e-01, 3.0000e-01, 8.2984e-02,\n",
       "        4.2861e-01, 2.5970e-01, 1.0190e-01, 3.7619e-01, 1.6304e-01, 1.2493e-01,\n",
       "        4.1223e-01, 9.6124e-02, 1.6510e-01, 1.7298e-02, 7.1049e-02, 6.8954e-03,\n",
       "        1.9689e-01, 1.3701e-01, 2.8537e-01, 3.6589e-01, 2.4386e-01, 3.9701e-01,\n",
       "        4.0222e-02, 3.9738e-01, 5.0223e-03, 4.8130e-02, 6.5207e-01, 3.0475e-01,\n",
       "        4.4496e-01, 6.9273e-01, 1.0358e-01, 1.3718e-01, 2.0242e-01, 5.7235e-01,\n",
       "        2.9149e-01, 3.7766e-01, 6.4563e-02, 1.3865e-01, 2.1821e-01, 8.6096e-01,\n",
       "        2.2961e-01, 7.6714e-01, 1.2152e-01, 1.5426e-01, 1.4538e-03, 1.2724e-01,\n",
       "        4.3739e-02, 5.9118e-02, 9.3473e-03, 6.4580e-01, 5.7878e-02, 3.0313e-01,\n",
       "        8.7075e-02, 4.6477e-02, 5.1930e-02, 4.7460e-01, 4.1896e-02, 2.8247e-02,\n",
       "        3.6786e-01, 4.5627e-01, 9.7459e-02, 4.8167e-01, 4.1302e-01, 1.6205e-02,\n",
       "        3.9047e-02, 3.0547e-04, 3.3587e-01, 4.9226e-01, 2.6205e-01, 2.5814e-01,\n",
       "        6.6904e-02, 6.1020e-01, 3.7302e-01, 5.5471e-01, 1.7925e-01, 2.4535e-01,\n",
       "        6.8044e-02, 1.1268e-02, 2.2256e-01, 3.1314e-01, 2.4790e-01, 4.3044e-01,\n",
       "        1.2092e-01, 3.7928e-02, 8.1227e-01, 1.9009e-01, 1.9773e-01, 5.5124e-02,\n",
       "        3.2041e-01, 7.6419e-02, 2.3965e-01, 3.3958e-01, 6.2632e-02, 1.7392e-01,\n",
       "        9.2438e-02, 5.5750e-02, 2.7113e-02, 8.0766e-02, 2.3432e-01, 5.0999e-01,\n",
       "        3.6168e-02, 2.7718e-01, 8.1112e-01, 4.3964e-01, 5.4159e-01, 4.7631e-02,\n",
       "        3.7880e-02, 9.6119e-02, 5.8191e-01, 3.7980e-03, 4.8496e-01, 3.8456e-01,\n",
       "        6.3242e-02, 2.2845e-01, 1.4947e-01, 3.1259e-02, 2.0240e-01, 5.2315e-01,\n",
       "        1.1784e-01, 2.1295e-01, 1.8980e-01, 1.1010e-01, 7.9409e-02, 1.4307e-01,\n",
       "        1.1567e-01, 1.0107e-01, 4.9425e-01, 3.6429e-03, 7.7662e-02, 1.6459e-01,\n",
       "        2.6259e-01, 8.9792e-02, 4.2718e-02, 4.0227e-01, 2.1745e-01, 2.0123e-01,\n",
       "        4.3782e-01, 3.2998e-01, 4.4320e-01, 5.0309e-03, 8.8310e-02, 3.0614e-01,\n",
       "        4.2342e-01, 5.6502e-01, 4.1763e-01, 2.9027e-01, 4.3271e-02, 2.0829e-02,\n",
       "        3.6819e-02, 3.3395e-01, 1.7131e-01, 8.4062e-01, 7.8114e-01, 1.2837e-01,\n",
       "        3.5635e-02, 8.4617e-01, 1.3900e-02, 3.8649e-01, 3.1542e-01, 6.7511e-03,\n",
       "        1.6741e-01, 2.5311e-01, 3.5984e-01, 3.4904e-02, 7.4940e-02, 3.7490e-02,\n",
       "        1.7004e-01, 2.5728e-01, 9.2952e-03, 4.1661e-01, 1.2832e-02, 2.7057e-01,\n",
       "        9.3882e-01, 7.8729e-01, 4.4994e-01, 3.9209e-01, 3.4310e-04, 5.1107e-03,\n",
       "        5.0163e-02, 1.8526e-01, 2.7770e-01, 4.0793e-02, 2.7632e-02, 1.8660e-01,\n",
       "        2.6758e-01, 9.2181e-01, 7.8407e-01, 2.4548e-02, 2.0224e-02, 3.1916e-02,\n",
       "        3.7373e-01, 2.1913e-01, 5.4784e-01, 4.9377e-02, 3.0652e-01, 6.3432e-02,\n",
       "        1.2382e-01, 7.3534e-02, 6.5223e-01, 7.6708e-01, 1.5167e-01, 1.8212e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(1,768,14,14)\n",
    "b=torch.rand(768)\n",
    "torch.mul(a[0,:,1,1],b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
